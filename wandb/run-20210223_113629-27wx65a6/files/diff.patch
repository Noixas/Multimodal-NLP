diff --git a/data/meme_dataset.py b/data/meme_dataset.py
index 4df775c..fd97af3 100644
--- a/data/meme_dataset.py
+++ b/data/meme_dataset.py
@@ -313,7 +313,7 @@ class MemeDataset(data.Dataset):
         img_feat = self.data.img_feats[idx]
         img_pos_feat = self.data.img_pos_feats[idx]
 
-        gender_race_probs = self.data.gender_race_probs
+        gender_race_probs = self.data.gender_race_probs[idx]
 
         return {
             'img_feat': img_feat,
diff --git a/model/meme_uniter.py b/model/meme_uniter.py
index ffb05a0..0b39608 100644
--- a/model/meme_uniter.py
+++ b/model/meme_uniter.py
@@ -24,9 +24,12 @@ class MemeUniter(nn.Module):
     def forward(self, **kwargs):
         out = self.uniter_model(**kwargs)
         out = self.uniter_model.pooler(out)
-        print(kwargs)
         gender_race_probs = kwargs["gender_race_probs"]
+        print("out",out.shape)
+        print("gender_race_probs",gender_race_probs.shape)
+        # exit()
         out = torch.cat((out, gender_race_probs), 1) # concatenate the uniter output with gender and race probabilities
+        print("out after",out.shape)
         out = self.linear_1(out)
         out = self.activation_1(out)
         out = self.linear_2(out)
diff --git a/vis_checkpoints/events.out.tfevents.1614075555.astro.1549507.0 b/vis_checkpoints/events.out.tfevents.1614075555.astro.1549507.0
new file mode 100644
index 0000000..6b403bf
Binary files /dev/null and b/vis_checkpoints/events.out.tfevents.1614075555.astro.1549507.0 differ
diff --git a/vis_checkpoints/events.out.tfevents.1614075639.astro.1550008.0 b/vis_checkpoints/events.out.tfevents.1614075639.astro.1550008.0
new file mode 100644
index 0000000..c25629d
Binary files /dev/null and b/vis_checkpoints/events.out.tfevents.1614075639.astro.1550008.0 differ
diff --git a/vis_checkpoints/events.out.tfevents.1614075763.astro.1550511.0 b/vis_checkpoints/events.out.tfevents.1614075763.astro.1550511.0
new file mode 100644
index 0000000..d10c829
Binary files /dev/null and b/vis_checkpoints/events.out.tfevents.1614075763.astro.1550511.0 differ
diff --git a/vis_checkpoints/events.out.tfevents.1614075871.astro.1551076.0 b/vis_checkpoints/events.out.tfevents.1614075871.astro.1551076.0
new file mode 100644
index 0000000..a0f3415
Binary files /dev/null and b/vis_checkpoints/events.out.tfevents.1614075871.astro.1551076.0 differ
diff --git a/vis_checkpoints/events.out.tfevents.1614075950.astro.1551487.0 b/vis_checkpoints/events.out.tfevents.1614075950.astro.1551487.0
new file mode 100644
index 0000000..d436c48
Binary files /dev/null and b/vis_checkpoints/events.out.tfevents.1614075950.astro.1551487.0 differ
diff --git a/wandb/debug-internal.log b/wandb/debug-internal.log
index f09c7de..e6b23c2 120000
--- a/wandb/debug-internal.log
+++ b/wandb/debug-internal.log
@@ -1 +1 @@
-run-20210222_215454-1602o824/logs/debug-internal.log
\ No newline at end of file
+run-20210223_113629-27wx65a6/logs/debug-internal.log
\ No newline at end of file
diff --git a/wandb/debug.log b/wandb/debug.log
index 9d13059..c17eace 120000
--- a/wandb/debug.log
+++ b/wandb/debug.log
@@ -1 +1 @@
-run-20210222_215454-1602o824/logs/debug.log
\ No newline at end of file
+run-20210223_113629-27wx65a6/logs/debug.log
\ No newline at end of file
diff --git a/wandb/latest-run b/wandb/latest-run
index 15abb24..a24d5eb 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20210222_215454-1602o824
\ No newline at end of file
+run-20210223_113629-27wx65a6
\ No newline at end of file
diff --git a/wandb/run-20210223_111914-2v7c5oze/files/code/train_uniter.py b/wandb/run-20210223_111914-2v7c5oze/files/code/train_uniter.py
new file mode 100644
index 0000000..44d832c
--- /dev/null
+++ b/wandb/run-20210223_111914-2v7c5oze/files/code/train_uniter.py
@@ -0,0 +1,654 @@
+import wandb
+import argparse
+import os
+import time
+import datetime
+import shutil
+import random
+import sys
+import os
+import json
+import re
+import numpy as np
+from statistics import mean, stdev
+import torch
+from torch.utils.tensorboard import SummaryWriter
+import torch.nn as nn
+import torch.nn.functional as F
+from sklearn.metrics import classification_report
+from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup
+from collections import defaultdict
+from functools import partial
+from torch.utils import data
+from transformers import BertTokenizer
+
+from utils.metrics import standard_metrics, find_optimal_threshold
+from utils.optim_utils import get_optimizer
+from utils.utils import calc_elapsed_time, print_stats, print_test_stats, log_tensorboard, set_seed, get_device, get_gather_index, get_attention_mask
+from utils.save import ModelSaver
+from model.meme_uniter import MemeUniter
+from model.pretrain import UniterForPretraining
+from utils.logger import LOGGER
+from data.meme_dataset import MemeDataset
+from model.model import UniterModel, UniterConfig
+from utils.const import IMG_DIM, IMG_LABEL_DIM
+
+
+class TrainerUniter():
+
+    def __init__(self, config):
+        self.preds_list, self.probs_list, self.labels_list, self.loss_list, self.short_loss_list, self.id_list = [], [], [], [], [], []
+        self.best_val_metrics, self.train_metrics = defaultdict(int), {}
+        self.best_auc = 0
+        self.not_improved = 0
+        self.best_val_loss = 1000
+        self.total_iters = 0
+        self.terminate_training = False
+        self.model_file = os.path.join(
+            config['model_path'], config['model_save_name'])
+        self.pretrained_model_file = None
+        if config['pretrained_model_file'] is not None:
+            self.pretrained_model_file = os.path.join(
+                config['model_path'], config['pretrained_model_file'])
+        self.start_epoch = 1
+        self.config = config
+        self.device = get_device()
+
+        if not isinstance(self.config['test_loader'], list):
+            self.config['test_loader'] = [self.config['test_loader']]
+
+        # Initialize the model, optimizer and loss function
+        self.init_training_params()
+
+    def init_training_params(self):
+        self.init_model()
+        wandb.watch(self.model)
+        self.model_saver = ModelSaver(self.model_file)
+
+        self.init_optimizer()
+        self.init_scheduler()
+
+        if self.config['loss_func'] == 'bce_logits':
+            self.criterion = nn.BCEWithLogitsLoss(
+                pos_weight=torch.tensor([self.config['pos_wt']]).to(self.device))
+        elif self.config['loss_func'] == 'bce':
+            self.criterion = nn.BCELoss()
+        else:
+            self.criterion = nn.CrossEntropyLoss()
+
+    def init_scheduler(self):
+        if self.config['scheduler'] == 'step':
+            self.scheduler = torch.optim.lr_scheduler.StepLR(
+                self.optimizer, step_size=self.config['lr_decay_step'], gamma=self.config['lr_decay_factor'])
+        elif self.config['scheduler'] == 'multi_step':
+            self.scheduler = torch.optim.lr_scheduler.MultiStepLR(
+                self.optimizer, milestones=[5, 10, 15, 25, 40], gamma=self.config['lr_decay_factor'])
+        elif self.config['scheduler'] == 'warmup':
+            self.scheduler = get_linear_schedule_with_warmup(self.optimizer, num_warmup_steps=self.config['warmup_steps'],
+                                                             num_training_steps=len(self.config['train_loader']) * self.config['max_epoch'])
+        elif self.config['scheduler'] == 'warmup_cosine':
+            self.scheduler = get_cosine_schedule_with_warmup(self.optimizer, num_warmup_steps=self.config['warmup_steps'],
+                                                             num_training_steps=len(self.config['train_loader']) * self.config['max_epoch'])
+
+    def init_optimizer(self):
+        self.optimizer = get_optimizer(self.model, self.config)
+
+    def init_model(self):
+        # pretrained model file is the original pretrained model - load and use this to fine-tune.
+        # If this argument is False, it will load the model file saved by you after fine-tuning
+        if self.pretrained_model_file:
+            checkpoint = torch.load(self.pretrained_model_file)
+            LOGGER.info('Using pretrained UNITER base model {}'.format(
+                self.pretrained_model_file))
+            base_model = UniterForPretraining.from_pretrained(self.config['config'],
+                                                              state_dict=checkpoint['model_state_dict'],
+                                                              img_dim=IMG_DIM,
+                                                              img_label_dim=IMG_LABEL_DIM)
+            self.model = MemeUniter(uniter_model=base_model.uniter,
+                                    hidden_size=base_model.uniter.config.hidden_size,
+                                    n_classes=self.config['n_classes'])
+        else:
+            self.load_model()
+        print("MemeUniter")
+        print(self.model)
+
+    def load_model(self):
+        # Load pretrained model
+        if self.model_file:
+            checkpoint = torch.load(self.model_file)
+            LOGGER.info('Using UNITER model {}'.format(self.model_file))
+        else:
+            checkpoint = {}
+
+        uniter_config = UniterConfig.from_json_file(self.config['config'])
+        uniter_model = UniterModel(uniter_config, img_dim=IMG_DIM)
+
+        self.model = MemeUniter(uniter_model=uniter_model,
+                                hidden_size=uniter_model.config.hidden_size,
+                                n_classes=self.config['n_classes'])
+        self.model.load_state_dict(checkpoint['model_state_dict'])
+
+    def average_gradients(self, steps):
+        # Used when grad_accumulation > 1
+        for param in self.model.parameters():
+            if param.requires_grad and param.grad is not None:
+                param.grad = param.grad / steps
+
+    def calculate_loss(self, preds, batch_label, grad_step):
+        if self.config['loss_func'] == 'bce':
+            preds = torch.sigmoid(preds)
+        preds = preds.squeeze(1).to(
+            self.device) if self.config['loss_func'] == 'bce_logits' else preds.to(self.device)
+        loss = self.criterion(preds, batch_label.to(
+            self.device) if self.config['loss_func'] == 'ce' else batch_label.float().to(self.device))
+
+        if grad_step and self.iters % self.config['gradient_accumulation'] == 0:
+            loss.backward()
+            self.average_gradients(steps=self.config['gradient_accumulation'])
+            torch.nn.utils.clip_grad_norm_(
+                self.model.parameters(), self.config['max_grad_norm'])
+            self.optimizer.step()
+            self.scheduler.step()
+            self.optimizer.zero_grad()
+        elif grad_step:
+            loss.backward()
+
+        if self.config['loss_func'] == 'bce':
+            probs = preds
+            preds = (preds > 0.5).type(torch.FloatTensor)
+        elif self.config['loss_func'] == 'ce':
+            probs = F.softmax(preds, dim=1)
+            preds = torch.argmax(probs, dim=1)
+        elif self.config['loss_func'] == 'bce_logits':
+            probs = torch.sigmoid(preds)
+            preds = (probs > 0.5).type(torch.FloatTensor)
+
+        self.probs_list.append(probs.cpu().detach().numpy())
+        self.preds_list.append(preds.cpu().detach().numpy())
+        self.labels_list.append(batch_label.cpu().detach().numpy())
+        self.loss_list.append(loss.detach().item())
+        if grad_step:
+            self.short_loss_list.append(loss.detach().item())
+
+    def eval_model(self, test=False, test_idx=0):
+        self.model.eval()
+        self.preds_list, self.probs_list, self.labels_list, self.loss_list, self.id_list = [], [], [], [], []
+        batch_loader = self.config['val_loader'] if not test else self.config['test_loader'][test_idx]
+        with torch.no_grad():
+            for iters, batch in enumerate(batch_loader):
+                batch = self.batch_to_device(batch)
+                if batch_loader.dataset.return_ids:
+                    self.id_list.append(batch['ids'])
+                self.eval_iter_step(iters, batch, test=test)
+
+            self.probs_list = [
+                prob for batch_prob in self.probs_list for prob in batch_prob]
+            self.preds_list = [
+                pred for batch_pred in self.preds_list for pred in batch_pred]
+            self.labels_list = [
+                label for batch_labels in self.labels_list for label in batch_labels]
+            self.id_list = [
+                data_id for batch_id in self.id_list for data_id in batch_id]
+
+            val_loss = sum(self.loss_list)/len(self.loss_list)
+            eval_metrics = standard_metrics(torch.tensor(
+                self.probs_list), torch.tensor(self.labels_list), add_optimal_acc=True)
+            # if test:
+            # 	print(classification_report(np.array(self.labels_list), np.array(self.preds_list)))
+        return eval_metrics, val_loss
+
+    @torch.no_grad()
+    def export_test_predictions(self, test_idx=0, threshold=0.5):
+        self.model.eval()
+
+        # Step 2: Run model on the test set (no loss!)
+        # Ensure that ids are actually returned
+        assert self.config['test_loader'][test_idx].dataset.return_ids, "Can only export test results if the IDs are returned in the test dataset."
+        test_name = self.config['test_loader'][test_idx].dataset.name
+
+        prob_list = []
+        id_list = []
+        for iters, batch in enumerate(self.config['test_loader'][test_idx]):
+            batch = self.batch_to_device(batch)
+            id_list.append(batch['ids'].cpu())
+            probs = self.test_iter_step(batch)
+            if self.config['loss_func'] == 'bce_logits':
+                probs = torch.sigmoid(probs)
+            prob_list.append(probs.detach().cpu())
+
+        probs = torch.cat(prob_list, dim=0)
+        ids = torch.cat(id_list, dim=0)
+        preds = (probs > threshold).long()
+
+        # Step 3: Export predictions
+        self._export_preds(ids, probs, preds,
+                           file_postfix="_%s_preds.csv" % test_name)
+
+        LOGGER.info("Finished export of test predictions")
+
+    @torch.no_grad()
+    def export_val_predictions(self, test=False, test_idx=0, threshold=0.5):
+        batch_loader = self.config['val_loader'] if not test else self.config['test_loader'][test_idx]
+        test_name = batch_loader.dataset.name
+        LOGGER.info("Exporting %s predictions..." % (test_name))
+        self.model.eval()
+
+        # Step 1: Find the optimal threshold on validation set
+        _, _ = self.eval_model(test=test, test_idx=test_idx)
+        val_probs = torch.tensor(self.probs_list)
+        val_labels = torch.tensor(self.labels_list)
+        if len(self.id_list) != 0:
+            val_ids = torch.tensor(self.id_list)
+        else:
+            val_ids = torch.zeros_like(val_labels)-1
+        val_preds = (val_probs > threshold).long()
+
+        self._export_preds(val_ids, val_probs, val_preds,
+                           labels=val_labels, file_postfix="_%s_preds.csv" % test_name)
+
+        LOGGER.info("Finished export of %s predictions" % test_name)
+
+    def _export_preds(self, ids, probs, preds, labels=None, file_postfix="_preds.csv"):
+        file_string = "id,proba,label%s\n" % (
+            ",gt" if labels is not None else "")
+        for i in range(ids.shape[0]):
+            file_string += "%i,%f,%i" % (ids[i].item(),
+                                         probs[i].item(), preds[i].item())
+            if labels is not None:
+                file_string += ",%i" % labels[i].item()
+            file_string += "\n"
+        filepath = os.path.join(
+            self.config['model_path'], self.config['model_save_name'].rsplit(".", 1)[0] + file_postfix)
+        with open(filepath, "w") as f:
+            f.write(file_string)
+        wandb.save(filepath) #Upload file to wandb
+
+    def check_early_stopping(self):
+        self.this_metric = self.val_loss if self.config[
+            'optimize_for'] == 'loss' else self.val_metrics[self.config['optimize_for']]
+        self.current_best = self.best_val_loss if self.config[
+            'optimize_for'] == 'loss' else self.best_val_metrics[self.config['optimize_for']]
+
+        new_best = self.this_metric < self.current_best if self.config[
+            'optimize_for'] == 'loss' else self.this_metric > self.current_best
+        if new_best:
+            LOGGER.info("New High Score! Saving model...")
+            self.best_val_metrics = self.val_metrics
+            self.best_val_loss = self.val_loss
+            wandb.log({'Best val metrics': self.best_val_metrics,
+                       'Best val loss': self.best_val_loss})
+
+            if not self.config["no_model_checkpoints"]:
+                self.model_saver.save(self.model)
+
+        ### Stopping Criteria based on patience and change-in-metric-threshold ###
+        diff = self.current_best - \
+            self.this_metric if self.config['optimize_for'] == 'loss' else self.this_metric - \
+            self.current_best
+        if diff < self.config['early_stop_thresh']:
+            self.not_improved += 1
+            if self.not_improved >= self.config['patience']:
+                self.terminate_training = True
+        else:
+            self.not_improved = 0
+        LOGGER.info("current patience: {}".format(self.not_improved))
+
+    def train_epoch_step(self):
+        self.model.train()
+        lr = self.scheduler.get_last_lr()
+        self.total_iters += self.iters + 1
+        self.probs_list = [
+            pred for batch_pred in self.probs_list for pred in batch_pred]
+        self.labels_list = [
+            label for batch_labels in self.labels_list for label in batch_labels]
+
+        # Evaluate on train set
+        self.train_metrics = standard_metrics(torch.tensor(
+            self.probs_list), torch.tensor(self.labels_list), add_optimal_acc=True)
+        log_tensorboard(self.config, self.config['writer'], self.model, self.epoch, self.iters, self.total_iters,
+                        self.loss_list, self.train_metrics, lr[0], loss_only=False, val=False)
+        self.train_loss = self.loss_list[:]
+
+        # Evaluate on dev set
+        val_time = time.time()
+        self.val_metrics, self.val_loss = self.eval_model()
+        self.config['writer'].add_scalar(
+            "Stats/time_validation", time.time() - val_time, self.total_iters)
+
+        # print stats
+        print_stats(self.config, self.epoch, self.train_metrics,
+                    self.train_loss, self.val_metrics, self.val_loss, self.start, lr[0])
+
+        # log validation stats in tensorboard
+        log_tensorboard(self.config, self.config['writer'], self.model, self.epoch, self.iters,
+                        self.total_iters, self.val_loss, self.val_metrics, lr[0], loss_only=False, val=True)
+
+        # Check for early stopping criteria
+        self.check_early_stopping()
+        self.probs_list = []
+        self.preds_list = []
+        self.labels_list = []
+        self.loss_list = []
+        self.id_list = []
+
+        self.train_loss = sum(self.train_loss)/len(self.train_loss)
+        del self.val_metrics
+        del self.val_loss
+
+    def end_training(self):
+        # Termination message
+        print("\n" + "-"*100)
+        if self.terminate_training:
+            LOGGER.info("Training terminated early because the Validation {} did not improve for  {}  epochs" .format(
+                self.config['optimize_for'], self.config['patience']))
+        else:
+            LOGGER.info("Maximum epochs of {} reached. Finished training !!".format(
+                self.config['max_epoch']))
+
+        print_test_stats(self.best_val_metrics, test=False)
+
+        print("-"*50 + "\n\t\tEvaluating on test set\n" + "-"*50)
+        if not self.config["no_model_checkpoints"]:
+            if os.path.isfile(self.model_file):
+                self.load_model()
+                self.model.to(self.device)
+            else:
+                raise ValueError("No Saved model state_dict found for the chosen model...!!! \nAborting evaluation on test set...".format(
+                    self.config['model_name']))
+
+            self.export_val_predictions()  # Runs evaluation, no need to run it again here
+            val_probs = torch.tensor(self.probs_list)
+            val_labels = torch.tensor(self.labels_list)
+            threshold = 0.5  # the default threshelod for binary classification
+            # Uncomment below line if you have implemented this optional feature
+            # threshold = find_optimal_threshold(val_probs, val_labels, metric="accuracy")
+            best_val_metrics = standard_metrics(
+                val_probs, val_labels, threshold=threshold, add_aucroc=False)
+            LOGGER.info("Optimal threshold on validation dataset: %.4f (accuracy=%4.2f%%)" % (
+                threshold, 100.0*best_val_metrics["accuracy"]))
+
+            # Testing is in the standard form not possible, as we do not have any labels (gives an error in standard_metrics)
+            # Instead, we should write out the predictions in the form of the leaderboard
+            self.test_metrics = dict()
+            for test_idx in range(len(self.config['test_loader'])):
+                test_name = self.config['test_loader'][test_idx].dataset.name
+                LOGGER.info("Export and testing on %s..." % test_name)
+                if hasattr(self.config['test_loader'][test_idx].dataset, "data") and \
+                   hasattr(self.config['test_loader'][test_idx].dataset.data, "labels") and \
+                   self.config['test_loader'][test_idx].dataset.data.labels[0] == -1:  # Step 1: Find the optimal threshold on validation set
+                    self.export_test_predictions(
+                        test_idx=test_idx, threshold=threshold)
+                    self.test_metrics[test_name] = dict()
+                else:
+                    test_idx_metrics, _ = self.eval_model(
+                        test=True, test_idx=test_idx)
+                    self.test_metrics[test_name] = test_idx_metrics
+                    print_test_stats(test_idx_metrics, test=True)
+                    self.export_val_predictions(
+                        test=True, test_idx=test_idx, threshold=threshold)
+        else:
+            LOGGER.info(
+                "No model checkpoints were saved. Hence, testing will be skipped.")
+            self.test_metrics = dict()
+
+        self.export_metrics()
+
+        self.config['writer'].close()
+
+        if self.config['remove_checkpoints']:
+            LOGGER.info("Removing checkpoint %s..." % self.model_file)
+            os.remove(self.model_file)
+
+    def export_metrics(self):
+        metric_export_file = os.path.join(
+            self.config['model_path'], self.config['model_save_name'].rsplit(".", 1)[0] + "_metrics.json")
+        metric_dict = {}
+        metric_dict["dev"] = self.best_val_metrics
+        metric_dict["dev"]["loss"] = self.best_val_loss
+        metric_dict["train"] = self.train_metrics
+        metric_dict["train"]["loss"] = sum(self.train_loss)/len(
+            self.train_loss) if isinstance(self.train_loss, list) else self.train_loss
+        if hasattr(self, "test_metrics") and len(self.test_metrics) > 0:
+            metric_dict["test"] = self.test_metrics
+
+        with open(metric_export_file, "w") as f:
+            json.dump(metric_dict, f, indent=4)
+
+    def train_main(self, cache=False):
+        print("\n\n" + "="*100 + "\n\t\t\t\t\t Training Network\n" + "="*100)
+
+        self.start = time.time()
+        print("\nBeginning training at:  {} \n".format(datetime.datetime.now()))
+
+        self.model.to(self.device)
+
+        for self.epoch in range(self.start_epoch, self.config['max_epoch']+1):
+            train_times = []
+            for self.iters, self.batch in enumerate(self.config['train_loader']):
+                self.model.train()
+
+                iter_time = time.time()
+                self.batch = self.batch_to_device(self.batch)
+                self.train_iter_step()
+                train_times.append(time.time() - iter_time)
+
+                # Loss only logging
+                if (self.total_iters+self.iters+1) % self.config['log_every'] == 0:
+                    log_tensorboard(self.config, self.config['writer'], self.model, self.epoch,
+                                    self.iters, self.total_iters, self.short_loss_list, loss_only=True, val=False)
+                    self.config['writer'].add_scalar(
+                        'Stats/time_per_train_iter', mean(train_times), (self.iters+self.total_iters+1))
+                    self.config['writer'].add_scalar(
+                        'Stats/learning_rate', self.scheduler.get_last_lr()[0], (self.iters+self.total_iters+1))
+                    train_times = []
+                    self.short_loss_list = []
+            self.train_epoch_step()
+
+            if self.terminate_training:
+                break
+
+        self.end_training()
+        return self.best_val_metrics, self.test_metrics
+
+    def batch_to_device(self, batch):
+        batch = {k: (v.to(self.device) if isinstance(v, torch.Tensor) else v)
+                 for k, v in batch.items()}
+        return batch
+
+    def eval_iter_step(self, iters, batch, test):
+        # Forward pass
+        preds = self.model(img_feat=batch['img_feat'], img_pos_feat=batch['img_pos_feat'], input_ids=batch['input_ids'],
+                           position_ids=batch['position_ids'], attention_mask=batch['attn_mask'], gather_index=batch['gather_index'],
+                           output_all_encoded_layers=False, gender_race_probs=batch['gender_race_probs'])
+        self.calculate_loss(preds, batch['labels'], grad_step=False)
+
+    def train_iter_step(self):
+        # Forward pass
+        self.preds = self.model(img_feat=self.batch['img_feat'], img_pos_feat=self.batch['img_pos_feat'], input_ids=self.batch['input_ids'],
+                                position_ids=self.batch['position_ids'], attention_mask=self.batch[
+                                    'attn_mask'], gather_index=self.batch['gather_index'],
+                                output_all_encoded_layers=False, gender_race_probs=self.batch['gender_race_probs'])
+        self.calculate_loss(self.preds, self.batch['labels'], grad_step=True)
+
+    def test_iter_step(self, batch):
+        # Forward pass
+        preds = self.model(img_feat=batch['img_feat'], img_pos_feat=batch['img_pos_feat'], input_ids=batch['input_ids'],
+                           position_ids=batch['position_ids'], attention_mask=batch['attn_mask'], gather_index=batch['gather_index'],
+                           output_all_encoded_layers=False, gender_race_probs=batch['gender_race_probs'])
+        return preds.squeeze()
+
+
+if __name__ == '__main__':
+    wandb.init(project="multimodal-nlp2")
+    wandb.tensorboard.patch(root_logdir='./vis_checkpoints',
+                            pytorch=True, tensorboardX=False)
+    parser = argparse.ArgumentParser()
+    defaults = dict()
+
+    # Required Paths
+    parser.add_argument('--data_path', type=str, default='./dataset',
+                        help='path to dataset folder that contains the processed data files')
+    parser.add_argument('--model_path', type=str, default='./model_checkpoints',
+                        help='Directory for saving trained model checkpoints')
+    parser.add_argument('--vis_path', type=str, default='./vis_checkpoints',
+                        help='Directory for saving tensorboard checkpoints')
+    parser.add_argument("--model_save_name", type=str, default='best_model.pt',
+                        help='saved model name')
+    parser.add_argument("--no_model_checkpoints", action="store_true",
+                        help='If selected, no model checkpoints will be created, and no testing performed (for gridsearches etc.)')
+    parser.add_argument("--remove_checkpoints", action="store_true",
+                        help='If selected, model checkpoints will be deleted after finishing testing.')
+    parser.add_argument('--config', type=str, default='./config/uniter-base.json',
+                        help='JSON config file')
+    parser.add_argument('--feature_path', type=str, default='./dataset/img_feats',
+                        help='Path to image features')
+
+    # Load pretrained model
+    parser.add_argument('--pretrained_model_file', type=str,
+                        help='Name of the original pretrained model')
+
+    #### Pre-processing Params ####
+    parser.add_argument('--max_txt_len', type=int, default=60,
+                        help='max number of tokens in text (BERT BPE)')
+    parser.add_argument('--max_bb', type=int, default=100,
+                        help='max number of bounding boxes')
+    parser.add_argument('--min_bb', type=int, default=10,
+                        help='min number of bounding boxes')
+    parser.add_argument('--num_bb', type=int, default=36,
+                        help='static number of bounding boxes')
+
+    #### Training Params ####
+    # Named parameters
+    parser.add_argument('--optimizer', type=str, default=defaults.get('optimizer', 'adam'),
+                        help='Optimizer to use for training: adam / adamx / adamw')
+    parser.add_argument('--loss_func', type=str, default=defaults.get('loss_func', 'bce_logits'),
+                        help='Loss function to use for optimization: bce / bce_logits / ce')
+    parser.add_argument('--optimize_for', type=str, default=defaults.get('optimize_for', 'aucroc'),
+                        help='Optimize for what measure during training and early stopping: loss / F1 / aucroc / accuracy')
+    parser.add_argument('--scheduler', type=str, default=defaults.get('scheduler', 'warmup_cosine'),
+                        help='The type of lr scheduler to use anneal learning rate: step/multi_step/warmup/warmp_cosine')
+
+    # Numerical parameters
+    parser.add_argument('--beta1', type=float, default=defaults.get('beta1', 0.9),
+                        help='beta1 parameter in Adam optimizer')
+    parser.add_argument('--beta2', type=float, default=defaults.get('beta2', 0.999),
+                        help='beta2 parameter in Adam optimizer')
+    parser.add_argument('--batch_size', type=int, default=defaults.get('batch_size', 8),
+                        help='batch size for training')
+    parser.add_argument('--num_workers', type=int, default=defaults.get('num_workers', 0),
+                        help='Number of workers to start per dataset')
+    parser.add_argument('--gradient_accumulation', type=int, default=defaults.get('gradient_accumulation', 1),
+                        help='No. of update steps to accumulate before performing backward pass')
+    parser.add_argument('--max_grad_norm', type=int, default=defaults.get('max_grad_norm', 5),
+                        help='max gradient norm for gradient clipping')
+    parser.add_argument('--pos_wt', type=float, default=defaults.get('pos_wt', 1),
+                        help='Loss reweighting for the positive class to deal with class imbalance')
+    parser.add_argument('--lr', type=float, default=defaults.get('lr', 1e-4),
+                        help='Learning rate for training')
+    parser.add_argument('--warmup_steps', type=int, default=defaults.get('warmup_steps', 50),
+                        help='No. of steps to perform linear lr warmup for')
+    parser.add_argument('--weight_decay', type=float, default=defaults.get('weight_decay', 1e-3),
+                        help='weight decay for optimizer')
+    parser.add_argument('--max_epoch', type=int, default=defaults.get('max_epoch', 20),
+                        help='Max epochs to train for')
+    parser.add_argument('--lr_decay_step', type=float, default=defaults.get('lr_decay_step', 3),
+                        help='No. of epochs after which learning rate should be decreased')
+    parser.add_argument('--lr_decay_factor', type=float, default=defaults.get('lr_decay_factor', 0.8),
+                        help='Decay the learning rate of the optimizer by this multiplicative amount')
+    parser.add_argument('--patience', type=float, default=defaults.get('patience', 5),
+                        help='Patience no. of epochs for early stopping')
+    parser.add_argument('--early_stop_thresh', type=float, default=defaults.get('early_stop_thresh', 1e-3),
+                        help='Patience no. of epochs for early stopping')
+    parser.add_argument('--seed', type=int, default=defaults.get('seed', 42),
+                        help='set seed for reproducability')
+    parser.add_argument('--log_every', type=int, default=defaults.get('log_every', 2000),
+                        help='Log stats in Tensorboard every x iterations (not epochs) of training')
+    parser.add_argument('--fc_dim', type=int, default=64,
+                        help='dimen of FC layer"')
+    parser.add_argument('--dropout', type=float, default=0.2,
+                        help='Standard dropout regularization')
+
+    # New parameters by team
+    parser.add_argument('--filter_text', action='store_true',
+                        help='Filter out bounding boxes around text')
+    parser.add_argument('--no_normalize_img', action='store_false',
+                        help='Normalize images by dividing them by their height and width. Default=True')
+    parser.add_argument('--train_filename', type=str, default='train.jsonl',
+                        help='The name of the trainin json file to load.')
+    parser.add_argument('--upsample_multiplier', type=int, default=0,
+                        help='Multiplier used to increase the amount of confounders in training data')
+    parser.add_argument('--note', type=str, default='',
+                        help='Add a note that can be seen in wandb')
+    args, unparsed = parser.parse_known_args()
+    config = args.__dict__
+    wandb.config.update(config)
+    config['device'] = get_device()
+    config['n_classes'] = 2 if config['loss_func'] == 'ce' else 1
+
+    # Check all provided paths:
+    if not os.path.exists(config['data_path']):
+        raise ValueError("[!] ERROR: Dataset path does not exist")
+    else:
+        LOGGER.info("Data path checked..")
+    if not os.path.exists(config['model_path']):
+        LOGGER.warning("Creating checkpoint path for saved models at:  {}\n".format(
+            config['model_path']))
+        os.makedirs(config['model_path'])
+    else:
+        LOGGER.info("Model save path checked..")
+    if 'config' in config:
+        if not os.path.isfile(config['config']):
+            raise ValueError("[!] ERROR: config JSON path does not exist")
+        else:
+            LOGGER.info("config JSON path checked..")
+    if not os.path.exists(config['vis_path']):
+        LOGGER.warning("Creating checkpoint path for Tensorboard visualizations at:  {}\n".format(
+            config['vis_path']))
+        os.makedirs(config['vis_path'])
+    else:
+        LOGGER.info("Tensorboard Visualization path checked..")
+        LOGGER.info(
+            "Cleaning Visualization path of older tensorboard files...\n")
+        # shutil.rmtree(config['vis_path'])
+
+    # Print args
+    print("\n" + "x"*50 + "\n\nRunning training with the following parameters: \n")
+    for key, value in config.items():
+        if not key.endswith('transf'):
+            print(key + ' : ' + str(value))
+    print("\n" + "x"*50)
+
+    config['writer'] = SummaryWriter(config['vis_path'])
+
+    set_seed(config['seed'])
+
+    # Tokenize
+    tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
+    tokenizer_func = partial(tokenizer, max_length=config['max_txt_len'], padding='max_length',
+                             truncation=True, return_tensors='pt', return_length=True)
+
+    # Prepare the datasets and dataloaders for training and evaluation
+    train_dataset = MemeDataset(filepath=os.path.join(config['data_path'], config['train_filename']),
+                                feature_dir=config['feature_path'], text_padding=tokenizer_func, filter_text=config["filter_text"],
+                                upsample_multiplier=config["upsample_multiplier"])
+    val_dataset = MemeDataset(filepath=os.path.join(config['data_path'], 'dev_seen.jsonl'),
+                              feature_dir=config['feature_path'], text_padding=tokenizer_func, filter_text=config["filter_text"])
+    test_dataset = MemeDataset(filepath=os.path.join(config['data_path'], 'test_seen.jsonl'),
+                               feature_dir=config['feature_path'], text_padding=tokenizer_func, filter_text=config["filter_text"])
+
+    config['train_loader'] = data.DataLoader(train_dataset, batch_size=config['batch_size'],
+                                             num_workers=config['num_workers'], collate_fn=train_dataset.get_collate_fn(), shuffle=True, pin_memory=True)
+    config['val_loader'] = data.DataLoader(val_dataset, batch_size=config['batch_size'],
+                                           num_workers=config['num_workers'], collate_fn=val_dataset.get_collate_fn())
+    config['test_loader'] = data.DataLoader(test_dataset, batch_size=config['batch_size'],
+                                            num_workers=config['num_workers'], collate_fn=test_dataset.get_collate_fn())
+
+    try:
+        trainer = TrainerUniter(config)
+        trainer.train_main()
+        wandb.save('vis_checkpoints/*', base_path="vis_checkpoints/")
+        wandb.finish()
+    except KeyboardInterrupt:
+        LOGGER.warning(
+            "Keyboard interrupt by user detected...\nClosing the tensorboard writer!")
+        config['writer'].close()
diff --git a/wandb/run-20210223_111914-2v7c5oze/files/config.yaml b/wandb/run-20210223_111914-2v7c5oze/files/config.yaml
new file mode 100644
index 0000000..bb39cb8
--- /dev/null
+++ b/wandb/run-20210223_111914-2v7c5oze/files/config.yaml
@@ -0,0 +1,149 @@
+wandb_version: 1
+
+_wandb:
+  desc: null
+  value:
+    cli_version: 0.10.19
+    code_path: code/train_uniter.py
+    framework: huggingface
+    huggingface_version: 3.2.0
+    is_jupyter_run: false
+    is_kaggle_kernel: false
+    python_version: 3.7.5
+    t:
+      1:
+      - 1
+      - 3
+      - 5
+      - 11
+      2:
+      - 1
+      - 3
+      - 5
+      - 11
+      4: 3.7.5
+      5: 0.10.19
+      6: 3.2.0
+batch_size:
+  desc: null
+  value: 16
+beta1:
+  desc: null
+  value: 0.9
+beta2:
+  desc: null
+  value: 0.999
+config:
+  desc: null
+  value: config/uniter-base.json
+data_path:
+  desc: null
+  value: ./dataset
+dropout:
+  desc: null
+  value: 0.2
+early_stop_thresh:
+  desc: null
+  value: 0.001
+fc_dim:
+  desc: null
+  value: 64
+feature_path:
+  desc: null
+  value: ./dataset/own_features
+filter_text:
+  desc: null
+  value: false
+gradient_accumulation:
+  desc: null
+  value: 2
+log_every:
+  desc: null
+  value: 2000
+loss_func:
+  desc: null
+  value: bce_logits
+lr:
+  desc: null
+  value: 3.0e-05
+lr_decay_factor:
+  desc: null
+  value: 0.8
+lr_decay_step:
+  desc: null
+  value: 3
+max_bb:
+  desc: null
+  value: 100
+max_epoch:
+  desc: null
+  value: 30
+max_grad_norm:
+  desc: null
+  value: 5
+max_txt_len:
+  desc: null
+  value: 60
+min_bb:
+  desc: null
+  value: 10
+model_path:
+  desc: null
+  value: ./model_checkpoints
+model_save_name:
+  desc: null
+  value: meme.pt
+no_model_checkpoints:
+  desc: null
+  value: false
+no_normalize_img:
+  desc: null
+  value: true
+note:
+  desc: null
+  value: ''
+num_bb:
+  desc: null
+  value: 36
+num_workers:
+  desc: null
+  value: 0
+optimize_for:
+  desc: null
+  value: aucroc
+optimizer:
+  desc: null
+  value: adam
+patience:
+  desc: null
+  value: 5.0
+pos_wt:
+  desc: null
+  value: 1.0
+pretrained_model_file:
+  desc: null
+  value: uniter-base.pt
+remove_checkpoints:
+  desc: null
+  value: false
+scheduler:
+  desc: null
+  value: warmup_cosine
+seed:
+  desc: null
+  value: 43
+train_filename:
+  desc: null
+  value: train.jsonl
+upsample_multiplier:
+  desc: null
+  value: 0
+vis_path:
+  desc: null
+  value: ./vis_checkpoints
+warmup_steps:
+  desc: null
+  value: 500
+weight_decay:
+  desc: null
+  value: 0.001
diff --git a/wandb/run-20210223_111914-2v7c5oze/files/diff.patch b/wandb/run-20210223_111914-2v7c5oze/files/diff.patch
new file mode 100644
index 0000000..68189d5
--- /dev/null
+++ b/wandb/run-20210223_111914-2v7c5oze/files/diff.patch
@@ -0,0 +1,27 @@
+diff --git a/wandb/debug-internal.log b/wandb/debug-internal.log
+index f09c7de..3a193c6 120000
+--- a/wandb/debug-internal.log
++++ b/wandb/debug-internal.log
+@@ -1 +1 @@
+-run-20210222_215454-1602o824/logs/debug-internal.log
+\ No newline at end of file
++run-20210223_111914-2v7c5oze/logs/debug-internal.log
+\ No newline at end of file
+diff --git a/wandb/debug.log b/wandb/debug.log
+index 9d13059..1ae7646 120000
+--- a/wandb/debug.log
++++ b/wandb/debug.log
+@@ -1 +1 @@
+-run-20210222_215454-1602o824/logs/debug.log
+\ No newline at end of file
++run-20210223_111914-2v7c5oze/logs/debug.log
+\ No newline at end of file
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 15abb24..60aad5b 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20210222_215454-1602o824
+\ No newline at end of file
++run-20210223_111914-2v7c5oze
+\ No newline at end of file
diff --git a/wandb/run-20210223_111914-2v7c5oze/files/events.out.tfevents.1614075555.astro.1549507.0 b/wandb/run-20210223_111914-2v7c5oze/files/events.out.tfevents.1614075555.astro.1549507.0
new file mode 120000
index 0000000..4a8aa81
--- /dev/null
+++ b/wandb/run-20210223_111914-2v7c5oze/files/events.out.tfevents.1614075555.astro.1549507.0
@@ -0,0 +1 @@
+/home/astro/Documents/UvA/Block 4 - NLP2/Multimodal NLP/Multimodal-NLP/vis_checkpoints/events.out.tfevents.1614075555.astro.1549507.0
\ No newline at end of file
diff --git a/wandb/run-20210223_111914-2v7c5oze/files/output.log b/wandb/run-20210223_111914-2v7c5oze/files/output.log
new file mode 100644
index 0000000..67f583e
--- /dev/null
+++ b/wandb/run-20210223_111914-2v7c5oze/files/output.log
@@ -0,0 +1,66 @@
+
+xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
+
+Running training with the following parameters: 
+23/02/2021 11:19:15 AM : INFO - Data path checked..
+
+data_path : ./dataset
+model_path : ./model_checkpoints
+vis_path : ./vis_checkpoints
+model_save_name : meme.pt
+no_model_checkpoints : False
+remove_checkpoints : False
+config : config/uniter-base.json
+feature_path : ./dataset/own_features
+pretrained_model_file : uniter-base.pt
+max_txt_len : 60
+max_bb : 100
+min_bb : 10
+num_bb : 36
+optimizer : adam
+loss_func : bce_logits
+optimize_for : aucroc
+23/02/2021 11:19:15 AM : INFO - Model save path checked..
+23/02/2021 11:19:15 AM : INFO - config JSON path checked..
+23/02/2021 11:19:15 AM : INFO - Tensorboard Visualization path checked..
+23/02/2021 11:19:15 AM : INFO - Cleaning Visualization path of older tensorboard files...
+
+scheduler : warmup_cosine
+beta1 : 0.9
+beta2 : 0.999
+batch_size : 16
+num_workers : 0
+gradient_accumulation : 2
+max_grad_norm : 5
+pos_wt : 1.0
+lr : 3e-05
+warmup_steps : 500
+weight_decay : 0.001
+max_epoch : 30
+lr_decay_step : 3
+lr_decay_factor : 0.8
+patience : 5.0
+early_stop_thresh : 0.001
+seed : 43
+log_every : 2000
+fc_dim : 64
+dropout : 0.2
+filter_text : False
+no_normalize_img : True
+train_filename : train.jsonl
+upsample_multiplier : 0
+note : 
+device : cuda
+n_classes : 1
+
+xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
+filter text False
+Loaded dataset contains  8500 samples
+Traceback (most recent call last):
+  File "train_uniter.py", line 633, in <module>
+    upsample_multiplier=config["upsample_multiplier"])
+  File "/home/astro/Documents/UvA/Block 4 - NLP2/Multimodal NLP/Multimodal-NLP/data/meme_dataset.py", line 64, in __init__
+    self._prepare_data_list()
+  File "/home/astro/Documents/UvA/Block 4 - NLP2/Multimodal NLP/Multimodal-NLP/data/meme_dataset.py", line 204, in _prepare_data_list
+    self._load_gender_race_preds()
+AttributeError: 'MemeDataset' object has no attribute '_load_gender_race_preds'
diff --git a/wandb/run-20210223_111914-2v7c5oze/files/requirements.txt b/wandb/run-20210223_111914-2v7c5oze/files/requirements.txt
new file mode 100644
index 0000000..0d8abf6
--- /dev/null
+++ b/wandb/run-20210223_111914-2v7c5oze/files/requirements.txt
@@ -0,0 +1,176 @@
+absl-py==0.10.0
+aiohttp==3.6.2
+apex==0.1
+argon2-cffi==20.1.0
+astor==0.8.1
+astunparse==1.6.3
+async-generator==1.10
+async-timeout==3.0.1
+attrs==20.2.0
+autopep8==1.5.5
+backcall==0.2.0
+beautifulsoup4==4.9.2
+bleach==3.2.1
+boto3==1.15.6
+botocore==1.18.6
+cached-property==1.5.2
+cachetools==4.1.1
+certifi==2020.6.20
+cffi==1.14.3
+chardet==3.0.4
+click==7.1.2
+configparser==5.0.1
+cycler==0.10.0
+dbfread==2.0.4
+deap==1.3.1
+decorator==4.4.2
+defusedxml==0.6.0
+demjson==2.2.4
+dlib==19.21.1
+docker-pycreds==0.4.0
+editdistance==0.5.3
+entrypoints==0.3
+fasttext==0.9.1
+filelock==3.0.12
+flask==1.1.2
+flatbuffers==1.12
+future==0.18.2
+gast==0.3.3
+gdown==3.12.2
+gitdb==4.0.5
+gitpython==3.1.0
+google-auth-oauthlib==0.4.1
+google-auth==1.22.0
+google-pasta==0.2.0
+gql==2.0.0
+graphql-core==2.3.2
+grpcio==1.32.0
+h5py==2.10.0
+idna==2.10
+ijson==2.6.1
+imagehash==4.2.0
+importlib-metadata==2.0.0
+ipykernel==5.3.4
+ipython-genutils==0.2.0
+ipython==7.20.0
+ipywidgets==7.5.1
+itsdangerous==1.1.0
+jedi==0.17.2
+jinja2==2.11.2
+jmespath==0.10.0
+joblib==0.16.0
+jsonschema==3.2.0
+jupyter-client==6.1.7
+jupyter-core==4.6.3
+jupyterlab-pygments==0.1.2
+keras-applications==1.0.8
+keras-preprocessing==1.1.2
+keras==2.4.3
+kiwisolver==1.2.0
+lightgbm==3.1.1
+lmdb==0.98
+markdown==3.2.2
+markupsafe==1.1.1
+matplotlib==3.3.2
+mdbtools==0.3.14
+meza==0.42.5
+mistune==0.8.4
+mtcnn==0.1.0
+multidict==4.7.6
+nbclient==0.5.0
+nbconvert==6.0.7
+nbformat==5.0.7
+nest-asyncio==1.4.1
+nltk==3.4.5
+notebook==6.1.4
+numpy==1.19.2
+nvidia-ml-py3==7.352.0
+oauthlib==3.1.0
+omegaconf==2.0.1rc4
+opencv-contrib-python==4.5.1.48
+opencv-python==4.5.1.48
+opt-einsum==3.3.0
+packaging==20.4
+pandas==1.1.2
+pandocfilters==1.4.2
+parso==0.7.1
+pathtools==0.1.2
+pexpect==4.8.0
+pickleshare==0.7.5
+pillow==7.2.0
+pip==20.3.3
+prometheus-client==0.8.0
+promise==2.3
+prompt-toolkit==3.0.7
+protobuf==3.13.0
+psutil==5.8.0
+ptyprocess==0.6.0
+pyasn1-modules==0.2.8
+pyasn1==0.4.8
+pybind11==2.6.2
+pycodestyle==2.6.0
+pycparser==2.20
+pygments==2.7.1
+pygogo==0.13.2
+pymongo==3.11.0
+pyparsing==2.4.7
+pyrsistent==0.17.3
+pysocks==1.7.1
+python-dateutil==2.8.1
+python-slugify==1.2.6
+pytz==2020.1
+pywavelets==1.1.1
+pyyaml==5.3.1
+pyzmq==19.0.2
+regex==2020.9.27
+requests-oauthlib==1.3.0
+requests==2.23.0
+rsa==4.6
+rx==1.6.1
+s3transfer==0.3.3
+sacremoses==0.0.43
+scikit-learn==0.23.2
+scipy==1.5.2
+seaborn==0.11.0
+send2trash==1.5.0
+sentencepiece==0.1.91
+sentry-sdk==0.20.3
+setuptools==52.0.0.post20210125
+shortuuid==1.0.1
+six==1.15.0
+sklearn==0.0
+smmap==3.0.4
+soupsieve==2.0.1
+subprocess32==3.5.4
+tensorboard-plugin-wit==1.7.0
+tensorboard==2.4.1
+tensorflow-estimator==2.4.0
+tensorflow==2.4.1
+termcolor==1.1.0
+terminado==0.9.1
+testpath==0.4.4
+threadpoolctl==2.1.0
+tokenizers==0.8.1rc2
+toml==0.10.2
+toolz==0.11.1
+torch==1.6.0+cu101
+torchtext==0.5.0
+torchvision==0.7.0+cu101
+tornado==6.0.4
+tqdm==4.50.0
+traitlets==5.0.5
+transformers==3.2.0
+typing-extensions==3.7.4.3
+unidecode==1.1.1
+urllib3==1.25.10
+wandb==0.10.19
+watchdog==2.0.1
+wcwidth==0.2.5
+webencodings==0.5.1
+werkzeug==1.0.1
+wheel==0.36.2
+widgetsnbextension==3.5.1
+wrapt==1.12.1
+xlrd==1.2.0
+yarl==1.6.0
+zipp==3.2.0
\ No newline at end of file
diff --git a/wandb/run-20210223_111914-2v7c5oze/files/wandb-metadata.json b/wandb/run-20210223_111914-2v7c5oze/files/wandb-metadata.json
new file mode 100644
index 0000000..64d6651
--- /dev/null
+++ b/wandb/run-20210223_111914-2v7c5oze/files/wandb-metadata.json
@@ -0,0 +1,55 @@
+{
+    "os": "Linux-5.8.0-43-generic-x86_64-with-debian-bullseye-sid",
+    "python": "3.7.5",
+    "heartbeatAt": "2021-02-23T10:19:15.654859",
+    "startedAt": "2021-02-23T10:19:14.265430",
+    "docker": null,
+    "gpu": "GeForce RTX 2060 SUPER",
+    "gpu_count": 1,
+    "cpu_count": 24,
+    "cuda": "11.0.228",
+    "args": [
+        "--config",
+        "config/uniter-base.json",
+        "--data_path",
+        "./dataset",
+        "--model_path",
+        "./model_checkpoints",
+        "--pretrained_model_file",
+        "uniter-base.pt",
+        "--feature_path",
+        "./dataset/own_features",
+        "--lr",
+        "3e-5",
+        "--scheduler",
+        "warmup_cosine",
+        "--warmup_steps",
+        "500",
+        "--max_epoch",
+        "30",
+        "--batch_size",
+        "16",
+        "--patience",
+        "5",
+        "--gradient_accumulation",
+        "2",
+        "--model_save_name",
+        "meme.pt",
+        "--seed",
+        "43",
+        "--pos_wt",
+        "1"
+    ],
+    "state": "running",
+    "codePath": "train_uniter.py",
+    "program": "train_uniter.py",
+    "git": {
+        "remote": "https://github.com/Noixas/Multimodal-NLP.git",
+        "commit": "83666a1f2f64375554a1633e19e39a7c7cef964b"
+    },
+    "email": "rodrigo.mulsa@outlook.com",
+    "root": "/home/astro/Documents/UvA/Block 4 - NLP2/Multimodal NLP/Multimodal-NLP",
+    "host": "astro",
+    "username": "astro",
+    "executable": "/home/astro/anaconda3/envs/nlp2-multimodal/bin/python"
+}
diff --git a/wandb/run-20210223_111914-2v7c5oze/files/wandb-summary.json b/wandb/run-20210223_111914-2v7c5oze/files/wandb-summary.json
new file mode 100644
index 0000000..9e26dfe
--- /dev/null
+++ b/wandb/run-20210223_111914-2v7c5oze/files/wandb-summary.json
@@ -0,0 +1 @@
+{}
\ No newline at end of file
diff --git a/wandb/run-20210223_111914-2v7c5oze/logs/debug-internal.log b/wandb/run-20210223_111914-2v7c5oze/logs/debug-internal.log
new file mode 100644
index 0000000..0f3d29d
--- /dev/null
+++ b/wandb/run-20210223_111914-2v7c5oze/logs/debug-internal.log
@@ -0,0 +1,126 @@
+2021-02-23 11:19:14,656 INFO    MainThread:1549566 [internal.py:wandb_internal():91] W&B internal server running at pid: 1549566, started at: 2021-02-23 11:19:14.655910
+2021-02-23 11:19:14,657 DEBUG   HandlerThread:1549566 [handler.py:handle_request():94] handle_request: check_version
+2021-02-23 11:19:14,657 INFO    WriterThread:1549566 [datastore.py:open_for_write():77] open: /home/astro/Documents/UvA/Block 4 - NLP2/Multimodal NLP/Multimodal-NLP/wandb/run-20210223_111914-2v7c5oze/run-2v7c5oze.wandb
+2021-02-23 11:19:14,658 DEBUG   SenderThread:1549566 [sender.py:send():117] send: header
+2021-02-23 11:19:14,658 DEBUG   SenderThread:1549566 [sender.py:send():117] send: request
+2021-02-23 11:19:14,658 DEBUG   SenderThread:1549566 [sender.py:send_request():126] send_request: check_version
+2021-02-23 11:19:14,695 DEBUG   SenderThread:1549566 [sender.py:send():117] send: run
+2021-02-23 11:19:14,867 INFO    SenderThread:1549566 [sender.py:_start_run_threads():596] run started: 2v7c5oze with start time 1614075554
+2021-02-23 11:19:14,867 DEBUG   SenderThread:1549566 [sender.py:send():117] send: summary
+2021-02-23 11:19:14,868 DEBUG   HandlerThread:1549566 [handler.py:handle_request():94] handle_request: run_start
+2021-02-23 11:19:14,868 INFO    SenderThread:1549566 [sender.py:_save_file():682] saving file wandb-summary.json with policy end
+2021-02-23 11:19:15,654 DEBUG   HandlerThread:1549566 [meta.py:__init__():34] meta init
+2021-02-23 11:19:15,654 DEBUG   HandlerThread:1549566 [meta.py:__init__():48] meta init done
+2021-02-23 11:19:15,654 DEBUG   HandlerThread:1549566 [meta.py:probe():190] probe
+2021-02-23 11:19:15,660 DEBUG   HandlerThread:1549566 [meta.py:_setup_git():180] setup git
+2021-02-23 11:19:15,676 DEBUG   HandlerThread:1549566 [meta.py:_setup_git():187] setup git done
+2021-02-23 11:19:15,677 DEBUG   HandlerThread:1549566 [meta.py:_save_code():69] save code
+2021-02-23 11:19:15,685 DEBUG   HandlerThread:1549566 [meta.py:_save_code():90] save code done
+2021-02-23 11:19:15,685 DEBUG   HandlerThread:1549566 [meta.py:_save_patches():107] save patches
+2021-02-23 11:19:15,775 DEBUG   HandlerThread:1549566 [meta.py:_save_patches():149] save patches done
+2021-02-23 11:19:15,775 DEBUG   HandlerThread:1549566 [meta.py:_save_pip():52] save pip
+2021-02-23 11:19:15,776 DEBUG   HandlerThread:1549566 [meta.py:_save_pip():66] save pip done
+2021-02-23 11:19:15,776 DEBUG   HandlerThread:1549566 [meta.py:probe():231] probe done
+2021-02-23 11:19:15,780 DEBUG   SenderThread:1549566 [sender.py:send():117] send: files
+2021-02-23 11:19:15,780 INFO    SenderThread:1549566 [sender.py:_save_file():682] saving file wandb-metadata.json with policy now
+2021-02-23 11:19:15,781 INFO    SenderThread:1549566 [sender.py:_save_file():682] saving file code/train_uniter.py with policy now
+2021-02-23 11:19:15,781 INFO    SenderThread:1549566 [sender.py:_save_file():682] saving file diff.patch with policy now
+2021-02-23 11:19:15,786 DEBUG   HandlerThread:1549566 [handler.py:handle_request():94] handle_request: status
+2021-02-23 11:19:15,787 DEBUG   SenderThread:1549566 [sender.py:send():117] send: request
+2021-02-23 11:19:15,787 DEBUG   SenderThread:1549566 [sender.py:send_request():126] send_request: status
+2021-02-23 11:19:15,796 INFO    HandlerThread:1549566 [handler.py:handle_tbrecord():307] handling tbrecord: tbrecord {
+  log_dir: "./vis_checkpoints"
+  save: true
+  root_dir: "./vis_checkpoints"
+}
+
+2021-02-23 11:19:15,797 DEBUG   HandlerThread:1549566 [config_util.py:dict_from_config_file():99] no default config file found in config-defaults.yaml
+2021-02-23 11:19:15,924 DEBUG   SenderThread:1549566 [sender.py:send():117] send: config
+2021-02-23 11:19:16,094 DEBUG   SenderThread:1549566 [sender.py:send():117] send: tbrecord
+2021-02-23 11:19:16,094 DEBUG   SenderThread:1549566 [sender.py:send():117] send: files
+2021-02-23 11:19:16,094 INFO    SenderThread:1549566 [sender.py:_save_file():682] saving file events.out.tfevents.1614075555.astro.1549507.0 with policy live
+2021-02-23 11:19:19,054 DEBUG   SenderThread:1549566 [sender.py:send():117] send: stats
+2021-02-23 11:19:23,523 DEBUG   SenderThread:1549566 [sender.py:send():117] send: stats
+2021-02-23 11:19:27,998 DEBUG   SenderThread:1549566 [sender.py:send():117] send: stats
+2021-02-23 11:19:30,925 DEBUG   HandlerThread:1549566 [handler.py:handle_request():94] handle_request: status
+2021-02-23 11:19:30,925 DEBUG   SenderThread:1549566 [sender.py:send():117] send: request
+2021-02-23 11:19:30,925 DEBUG   SenderThread:1549566 [sender.py:send_request():126] send_request: status
+2021-02-23 11:19:32,470 DEBUG   SenderThread:1549566 [sender.py:send():117] send: stats
+2021-02-23 11:19:32,848 DEBUG   SenderThread:1549566 [sender.py:send():117] send: telemetry
+2021-02-23 11:19:32,848 DEBUG   HandlerThread:1549566 [handler.py:handle_request():94] handle_request: poll_exit
+2021-02-23 11:19:33,013 DEBUG   SenderThread:1549566 [sender.py:send():117] send: exit
+2021-02-23 11:19:33,013 INFO    SenderThread:1549566 [sender.py:send_exit():195] handling exit code: 1
+2021-02-23 11:19:33,013 INFO    SenderThread:1549566 [sender.py:send_exit():203] send defer
+2021-02-23 11:19:33,014 DEBUG   SenderThread:1549566 [sender.py:send():117] send: request
+2021-02-23 11:19:33,014 DEBUG   SenderThread:1549566 [sender.py:send_request():126] send_request: poll_exit
+2021-02-23 11:19:33,014 DEBUG   HandlerThread:1549566 [handler.py:handle_request():94] handle_request: defer
+2021-02-23 11:19:33,014 INFO    HandlerThread:1549566 [handler.py:handle_request_defer():108] handle defer: 0
+2021-02-23 11:19:33,014 DEBUG   SenderThread:1549566 [sender.py:send():117] send: request
+2021-02-23 11:19:33,014 DEBUG   SenderThread:1549566 [sender.py:send_request():126] send_request: defer
+2021-02-23 11:19:33,014 INFO    SenderThread:1549566 [sender.py:send_request_defer():212] handle sender defer: 0
+2021-02-23 11:19:33,014 INFO    SenderThread:1549566 [sender.py:send_request_defer():248] send defer: 1
+2021-02-23 11:19:33,014 DEBUG   HandlerThread:1549566 [handler.py:handle_request():94] handle_request: defer
+2021-02-23 11:19:33,015 INFO    HandlerThread:1549566 [handler.py:handle_request_defer():108] handle defer: 1
+2021-02-23 11:19:33,085 DEBUG   SenderThread:1549566 [sender.py:send():117] send: request
+2021-02-23 11:19:33,085 DEBUG   SenderThread:1549566 [sender.py:send_request():126] send_request: defer
+2021-02-23 11:19:33,085 INFO    SenderThread:1549566 [sender.py:send_request_defer():212] handle sender defer: 1
+2021-02-23 11:19:33,085 INFO    SenderThread:1549566 [sender.py:send_request_defer():248] send defer: 2
+2021-02-23 11:19:33,085 DEBUG   SenderThread:1549566 [sender.py:send():117] send: stats
+2021-02-23 11:19:33,086 DEBUG   HandlerThread:1549566 [handler.py:handle_request():94] handle_request: defer
+2021-02-23 11:19:33,086 INFO    HandlerThread:1549566 [handler.py:handle_request_defer():108] handle defer: 2
+2021-02-23 11:19:39,800 DEBUG   HandlerThread:1549566 [handler.py:handle_request():94] handle_request: poll_exit
+2021-02-23 11:19:39,800 DEBUG   SenderThread:1549566 [sender.py:send():117] send: request
+2021-02-23 11:19:39,801 DEBUG   SenderThread:1549566 [sender.py:send_request():126] send_request: defer
+2021-02-23 11:19:39,801 INFO    SenderThread:1549566 [sender.py:send_request_defer():212] handle sender defer: 2
+2021-02-23 11:19:39,801 INFO    SenderThread:1549566 [sender.py:send_request_defer():248] send defer: 3
+2021-02-23 11:19:39,801 DEBUG   SenderThread:1549566 [sender.py:send():117] send: request
+2021-02-23 11:19:39,801 DEBUG   SenderThread:1549566 [sender.py:send_request():126] send_request: poll_exit
+2021-02-23 11:19:39,801 DEBUG   HandlerThread:1549566 [handler.py:handle_request():94] handle_request: defer
+2021-02-23 11:19:39,802 INFO    HandlerThread:1549566 [handler.py:handle_request_defer():108] handle defer: 3
+2021-02-23 11:19:39,802 DEBUG   SenderThread:1549566 [sender.py:send():117] send: summary
+2021-02-23 11:19:39,804 INFO    SenderThread:1549566 [sender.py:_save_file():682] saving file wandb-summary.json with policy end
+2021-02-23 11:19:39,804 DEBUG   SenderThread:1549566 [sender.py:send():117] send: request
+2021-02-23 11:19:39,804 DEBUG   SenderThread:1549566 [sender.py:send_request():126] send_request: defer
+2021-02-23 11:19:39,804 INFO    SenderThread:1549566 [sender.py:send_request_defer():212] handle sender defer: 3
+2021-02-23 11:19:39,804 INFO    SenderThread:1549566 [sender.py:send_request_defer():248] send defer: 4
+2021-02-23 11:19:39,804 DEBUG   HandlerThread:1549566 [handler.py:handle_request():94] handle_request: defer
+2021-02-23 11:19:39,805 INFO    HandlerThread:1549566 [handler.py:handle_request_defer():108] handle defer: 4
+2021-02-23 11:19:39,805 DEBUG   SenderThread:1549566 [sender.py:send():117] send: request
+2021-02-23 11:19:39,805 DEBUG   SenderThread:1549566 [sender.py:send_request():126] send_request: defer
+2021-02-23 11:19:39,805 INFO    SenderThread:1549566 [sender.py:send_request_defer():212] handle sender defer: 4
+2021-02-23 11:19:39,890 INFO    SenderThread:1549566 [sender.py:send_request_defer():248] send defer: 5
+2021-02-23 11:19:39,891 DEBUG   HandlerThread:1549566 [handler.py:handle_request():94] handle_request: defer
+2021-02-23 11:19:39,891 INFO    HandlerThread:1549566 [handler.py:handle_request_defer():108] handle defer: 5
+2021-02-23 11:19:39,891 DEBUG   SenderThread:1549566 [sender.py:send():117] send: request
+2021-02-23 11:19:39,891 DEBUG   SenderThread:1549566 [sender.py:send_request():126] send_request: defer
+2021-02-23 11:19:39,892 INFO    SenderThread:1549566 [sender.py:send_request_defer():212] handle sender defer: 5
+2021-02-23 11:19:39,892 INFO    SenderThread:1549566 [sender.py:send_request_defer():248] send defer: 6
+2021-02-23 11:19:39,892 DEBUG   HandlerThread:1549566 [handler.py:handle_request():94] handle_request: defer
+2021-02-23 11:19:39,892 INFO    HandlerThread:1549566 [handler.py:handle_request_defer():108] handle defer: 6
+2021-02-23 11:19:39,892 DEBUG   SenderThread:1549566 [sender.py:send():117] send: request
+2021-02-23 11:19:39,892 DEBUG   SenderThread:1549566 [sender.py:send_request():126] send_request: defer
+2021-02-23 11:19:39,892 INFO    SenderThread:1549566 [sender.py:send_request_defer():212] handle sender defer: 6
+2021-02-23 11:19:40,062 INFO    SenderThread:1549566 [sender.py:send_request_defer():248] send defer: 7
+2021-02-23 11:19:40,062 DEBUG   HandlerThread:1549566 [handler.py:handle_request():94] handle_request: defer
+2021-02-23 11:19:40,062 INFO    HandlerThread:1549566 [handler.py:handle_request_defer():108] handle defer: 7
+2021-02-23 11:19:40,062 DEBUG   SenderThread:1549566 [sender.py:send():117] send: request
+2021-02-23 11:19:40,062 DEBUG   SenderThread:1549566 [sender.py:send_request():126] send_request: defer
+2021-02-23 11:19:40,062 INFO    SenderThread:1549566 [sender.py:send_request_defer():212] handle sender defer: 7
+2021-02-23 11:19:40,062 INFO    SenderThread:1549566 [sender.py:send_request_defer():248] send defer: 8
+2021-02-23 11:19:40,063 DEBUG   SenderThread:1549566 [sender.py:send():117] send: final
+2021-02-23 11:19:40,063 DEBUG   HandlerThread:1549566 [handler.py:handle_request():94] handle_request: defer
+2021-02-23 11:19:40,063 DEBUG   SenderThread:1549566 [sender.py:send():117] send: footer
+2021-02-23 11:19:40,063 INFO    HandlerThread:1549566 [handler.py:handle_request_defer():108] handle defer: 8
+2021-02-23 11:19:40,063 DEBUG   SenderThread:1549566 [sender.py:send():117] send: request
+2021-02-23 11:19:40,063 DEBUG   SenderThread:1549566 [sender.py:send_request():126] send_request: defer
+2021-02-23 11:19:40,063 INFO    SenderThread:1549566 [sender.py:send_request_defer():212] handle sender defer: 8
+2021-02-23 11:19:41,803 DEBUG   HandlerThread:1549566 [handler.py:handle_request():94] handle_request: poll_exit
+2021-02-23 11:19:41,803 DEBUG   SenderThread:1549566 [sender.py:send():117] send: request
+2021-02-23 11:19:41,803 DEBUG   SenderThread:1549566 [sender.py:send_request():126] send_request: poll_exit
+2021-02-23 11:19:41,804 DEBUG   HandlerThread:1549566 [handler.py:handle_request():94] handle_request: get_summary
+2021-02-23 11:19:41,805 DEBUG   HandlerThread:1549566 [handler.py:handle_request():94] handle_request: sampled_history
+2021-02-23 11:19:41,805 DEBUG   HandlerThread:1549566 [handler.py:handle_request():94] handle_request: shutdown
+2021-02-23 11:19:41,805 INFO    HandlerThread:1549566 [handler.py:finish():333] shutting down handler
+2021-02-23 11:19:42,063 INFO    WriterThread:1549566 [datastore.py:close():258] close: /home/astro/Documents/UvA/Block 4 - NLP2/Multimodal NLP/Multimodal-NLP/wandb/run-20210223_111914-2v7c5oze/run-2v7c5oze.wandb
+2021-02-23 11:19:42,804 INFO    SenderThread:1549566 [sender.py:finish():766] shutting down sender
+2021-02-23 11:19:42,805 INFO    MainThread:1549566 [internal.py:handle_exit():78] Internal process exited
diff --git a/wandb/run-20210223_111914-2v7c5oze/logs/debug.log b/wandb/run-20210223_111914-2v7c5oze/logs/debug.log
new file mode 100644
index 0000000..0bf5043
--- /dev/null
+++ b/wandb/run-20210223_111914-2v7c5oze/logs/debug.log
@@ -0,0 +1,78 @@
+2021-02-23 11:19:14,267 INFO    MainThread:1549507 [wandb_setup.py:_flush():70] setting env: {}
+2021-02-23 11:19:14,267 INFO    MainThread:1549507 [wandb_setup.py:_flush():70] multiprocessing start_methods=fork,spawn,forkserver, using: spawn
+2021-02-23 11:19:14,267 INFO    MainThread:1549507 [wandb_setup.py:_flush():70] setting login settings: {}
+2021-02-23 11:19:14,267 INFO    MainThread:1549507 [wandb_init.py:_log_setup():319] Logging user logs to /home/astro/Documents/UvA/Block 4 - NLP2/Multimodal NLP/Multimodal-NLP/wandb/run-20210223_111914-2v7c5oze/logs/debug.log
+2021-02-23 11:19:14,267 INFO    MainThread:1549507 [wandb_init.py:_log_setup():320] Logging internal logs to /home/astro/Documents/UvA/Block 4 - NLP2/Multimodal NLP/Multimodal-NLP/wandb/run-20210223_111914-2v7c5oze/logs/debug-internal.log
+2021-02-23 11:19:14,267 INFO    MainThread:1549507 [wandb_init.py:init():351] calling init triggers
+2021-02-23 11:19:14,267 INFO    MainThread:1549507 [wandb_init.py:init():358] wandb.init called with sweep_config: {}
+config: {}
+2021-02-23 11:19:14,267 INFO    MainThread:1549507 [wandb_init.py:init():404] starting backend
+2021-02-23 11:19:14,279 INFO    MainThread:1549507 [backend.py:ensure_launched():81] starting backend process...
+2021-02-23 11:19:14,285 INFO    MainThread:1549507 [backend.py:ensure_launched():86] started backend process with pid: 1549566
+2021-02-23 11:19:14,286 INFO    MainThread:1549507 [wandb_init.py:init():413] backend started and connected
+2021-02-23 11:19:14,287 INFO    MainThread:1549507 [wandb_init.py:init():436] updated telemetry
+2021-02-23 11:19:14,287 INFO    MainThread:1549507 [wandb_init.py:init():459] communicating current version
+2021-02-23 11:19:14,694 INFO    MainThread:1549507 [wandb_init.py:init():464] got version response upgrade_message: "wandb version 0.10.20 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
+
+2021-02-23 11:19:14,695 INFO    MainThread:1549507 [wandb_init.py:init():472] communicating run to backend with 30 second timeout
+2021-02-23 11:19:14,867 INFO    MainThread:1549507 [wandb_init.py:init():495] starting run threads in backend
+2021-02-23 11:19:15,784 INFO    MainThread:1549507 [wandb_run.py:_console_start():1411] atexit reg
+2021-02-23 11:19:15,784 INFO    MainThread:1549507 [wandb_run.py:_redirect():1274] redirect: SettingsConsole.REDIRECT
+2021-02-23 11:19:15,784 INFO    MainThread:1549507 [wandb_run.py:_redirect():1279] Redirecting console.
+2021-02-23 11:19:15,784 INFO    MainThread:1549507 [redirect.py:install():213] install start
+2021-02-23 11:19:15,785 INFO    MainThread:1549507 [redirect.py:install():228] install stop
+2021-02-23 11:19:15,785 INFO    MainThread:1549507 [redirect.py:install():213] install start
+2021-02-23 11:19:15,785 INFO    MainThread:1549507 [redirect.py:install():228] install stop
+2021-02-23 11:19:15,785 INFO    MainThread:1549507 [wandb_run.py:_redirect():1325] Redirects installed.
+2021-02-23 11:19:15,785 INFO    MainThread:1549507 [wandb_init.py:init():518] run started, returning control to user process
+2021-02-23 11:19:15,789 INFO    MainThread:1549507 [wandb_run.py:_config_callback():663] config_cb None None {'data_path': './dataset', 'model_path': './model_checkpoints', 'vis_path': './vis_checkpoints', 'model_save_name': 'meme.pt', 'no_model_checkpoints': False, 'remove_checkpoints': False, 'config': 'config/uniter-base.json', 'feature_path': './dataset/own_features', 'pretrained_model_file': 'uniter-base.pt', 'max_txt_len': 60, 'max_bb': 100, 'min_bb': 10, 'num_bb': 36, 'optimizer': 'adam', 'loss_func': 'bce_logits', 'optimize_for': 'aucroc', 'scheduler': 'warmup_cosine', 'beta1': 0.9, 'beta2': 0.999, 'batch_size': 16, 'num_workers': 0, 'gradient_accumulation': 2, 'max_grad_norm': 5, 'pos_wt': 1.0, 'lr': 3e-05, 'warmup_steps': 500, 'weight_decay': 0.001, 'max_epoch': 30, 'lr_decay_step': 3, 'lr_decay_factor': 0.8, 'patience': 5.0, 'early_stop_thresh': 0.001, 'seed': 43, 'log_every': 2000, 'fc_dim': 64, 'dropout': 0.2, 'filter_text': False, 'no_normalize_img': True, 'train_filename': 'train.jsonl', 'upsample_multiplier': 0, 'note': ''}
+2021-02-23 11:19:15,791 INFO    MainThread:1549507 [wandb_run.py:_tensorboard_callback():734] tensorboard callback: ./vis_checkpoints, None
+2021-02-23 11:19:32,846 INFO    MainThread:1549507 [wandb_run.py:_atexit_cleanup():1381] got exitcode: 1
+2021-02-23 11:19:32,847 INFO    MainThread:1549507 [wandb_run.py:_restore():1353] restore
+2021-02-23 11:19:32,847 INFO    MainThread:1549507 [redirect.py:uninstall():232] uninstall start
+2021-02-23 11:19:32,847 INFO    MainThread:1549507 [redirect.py:_stop():287] _stop: stdout
+2021-02-23 11:19:32,847 INFO    MainThread:1549507 [redirect.py:_stop():293] _stop closed: stdout
+2021-02-23 11:19:32,847 INFO    stdout    :1549507 [redirect.py:_pipe_relay():129] relay done saw last write: stdout
+2021-02-23 11:19:32,847 INFO    stdout    :1549507 [redirect.py:_pipe_relay():145] relay done done: stdout
+2021-02-23 11:19:32,847 INFO    MainThread:1549507 [redirect.py:_stop():299] _stop joined: stdout
+2021-02-23 11:19:32,847 INFO    MainThread:1549507 [redirect.py:_stop():301] _stop rd closed: stdout
+2021-02-23 11:19:32,847 INFO    MainThread:1549507 [redirect.py:uninstall():236] uninstall done
+2021-02-23 11:19:32,847 INFO    MainThread:1549507 [redirect.py:uninstall():232] uninstall start
+2021-02-23 11:19:32,848 INFO    MainThread:1549507 [redirect.py:_stop():287] _stop: stderr
+2021-02-23 11:19:32,848 INFO    MainThread:1549507 [redirect.py:_stop():293] _stop closed: stderr
+2021-02-23 11:19:32,848 INFO    stderr    :1549507 [redirect.py:_pipe_relay():129] relay done saw last write: stderr
+2021-02-23 11:19:32,848 INFO    stderr    :1549507 [redirect.py:_pipe_relay():145] relay done done: stderr
+2021-02-23 11:19:32,848 INFO    MainThread:1549507 [redirect.py:_stop():299] _stop joined: stderr
+2021-02-23 11:19:32,848 INFO    MainThread:1549507 [redirect.py:_stop():301] _stop rd closed: stderr
+2021-02-23 11:19:32,848 INFO    MainThread:1549507 [redirect.py:uninstall():236] uninstall done
+2021-02-23 11:19:33,014 INFO    MainThread:1549507 [wandb_run.py:_wait_for_finish():1504] got exit ret: file_counts {
+  wandb_count: 2
+  other_count: 2
+}
+pusher_stats {
+  uploaded_bytes: 35959
+  total_bytes: 35959
+}
+
+2021-02-23 11:19:39,802 INFO    MainThread:1549507 [wandb_run.py:_wait_for_finish():1504] got exit ret: file_counts {
+  wandb_count: 2
+  other_count: 2
+}
+pusher_stats {
+  uploaded_bytes: 35959
+  total_bytes: 35959
+}
+
+2021-02-23 11:19:41,804 INFO    MainThread:1549507 [wandb_run.py:_wait_for_finish():1504] got exit ret: done: true
+exit_result {
+}
+file_counts {
+  wandb_count: 6
+  other_count: 2
+}
+pusher_stats {
+  uploaded_bytes: 43061
+  total_bytes: 43061
+}
+
+2021-02-23 11:19:43,046 INFO    MainThread:1549507 [wandb_run.py:_show_files():1726] logging synced files
diff --git a/wandb/run-20210223_111914-2v7c5oze/run-2v7c5oze.wandb b/wandb/run-20210223_111914-2v7c5oze/run-2v7c5oze.wandb
new file mode 100644
index 0000000..ff4b514
Binary files /dev/null and b/wandb/run-20210223_111914-2v7c5oze/run-2v7c5oze.wandb differ
diff --git a/wandb/run-20210223_112037-1rc3eqxy/files/code/train_uniter.py b/wandb/run-20210223_112037-1rc3eqxy/files/code/train_uniter.py
new file mode 100644
index 0000000..44d832c
--- /dev/null
+++ b/wandb/run-20210223_112037-1rc3eqxy/files/code/train_uniter.py
@@ -0,0 +1,654 @@
+import wandb
+import argparse
+import os
+import time
+import datetime
+import shutil
+import random
+import sys
+import os
+import json
+import re
+import numpy as np
+from statistics import mean, stdev
+import torch
+from torch.utils.tensorboard import SummaryWriter
+import torch.nn as nn
+import torch.nn.functional as F
+from sklearn.metrics import classification_report
+from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup
+from collections import defaultdict
+from functools import partial
+from torch.utils import data
+from transformers import BertTokenizer
+
+from utils.metrics import standard_metrics, find_optimal_threshold
+from utils.optim_utils import get_optimizer
+from utils.utils import calc_elapsed_time, print_stats, print_test_stats, log_tensorboard, set_seed, get_device, get_gather_index, get_attention_mask
+from utils.save import ModelSaver
+from model.meme_uniter import MemeUniter
+from model.pretrain import UniterForPretraining
+from utils.logger import LOGGER
+from data.meme_dataset import MemeDataset
+from model.model import UniterModel, UniterConfig
+from utils.const import IMG_DIM, IMG_LABEL_DIM
+
+
+class TrainerUniter():
+
+    def __init__(self, config):
+        self.preds_list, self.probs_list, self.labels_list, self.loss_list, self.short_loss_list, self.id_list = [], [], [], [], [], []
+        self.best_val_metrics, self.train_metrics = defaultdict(int), {}
+        self.best_auc = 0
+        self.not_improved = 0
+        self.best_val_loss = 1000
+        self.total_iters = 0
+        self.terminate_training = False
+        self.model_file = os.path.join(
+            config['model_path'], config['model_save_name'])
+        self.pretrained_model_file = None
+        if config['pretrained_model_file'] is not None:
+            self.pretrained_model_file = os.path.join(
+                config['model_path'], config['pretrained_model_file'])
+        self.start_epoch = 1
+        self.config = config
+        self.device = get_device()
+
+        if not isinstance(self.config['test_loader'], list):
+            self.config['test_loader'] = [self.config['test_loader']]
+
+        # Initialize the model, optimizer and loss function
+        self.init_training_params()
+
+    def init_training_params(self):
+        self.init_model()
+        wandb.watch(self.model)
+        self.model_saver = ModelSaver(self.model_file)
+
+        self.init_optimizer()
+        self.init_scheduler()
+
+        if self.config['loss_func'] == 'bce_logits':
+            self.criterion = nn.BCEWithLogitsLoss(
+                pos_weight=torch.tensor([self.config['pos_wt']]).to(self.device))
+        elif self.config['loss_func'] == 'bce':
+            self.criterion = nn.BCELoss()
+        else:
+            self.criterion = nn.CrossEntropyLoss()
+
+    def init_scheduler(self):
+        if self.config['scheduler'] == 'step':
+            self.scheduler = torch.optim.lr_scheduler.StepLR(
+                self.optimizer, step_size=self.config['lr_decay_step'], gamma=self.config['lr_decay_factor'])
+        elif self.config['scheduler'] == 'multi_step':
+            self.scheduler = torch.optim.lr_scheduler.MultiStepLR(
+                self.optimizer, milestones=[5, 10, 15, 25, 40], gamma=self.config['lr_decay_factor'])
+        elif self.config['scheduler'] == 'warmup':
+            self.scheduler = get_linear_schedule_with_warmup(self.optimizer, num_warmup_steps=self.config['warmup_steps'],
+                                                             num_training_steps=len(self.config['train_loader']) * self.config['max_epoch'])
+        elif self.config['scheduler'] == 'warmup_cosine':
+            self.scheduler = get_cosine_schedule_with_warmup(self.optimizer, num_warmup_steps=self.config['warmup_steps'],
+                                                             num_training_steps=len(self.config['train_loader']) * self.config['max_epoch'])
+
+    def init_optimizer(self):
+        self.optimizer = get_optimizer(self.model, self.config)
+
+    def init_model(self):
+        # pretrained model file is the original pretrained model - load and use this to fine-tune.
+        # If this argument is False, it will load the model file saved by you after fine-tuning
+        if self.pretrained_model_file:
+            checkpoint = torch.load(self.pretrained_model_file)
+            LOGGER.info('Using pretrained UNITER base model {}'.format(
+                self.pretrained_model_file))
+            base_model = UniterForPretraining.from_pretrained(self.config['config'],
+                                                              state_dict=checkpoint['model_state_dict'],
+                                                              img_dim=IMG_DIM,
+                                                              img_label_dim=IMG_LABEL_DIM)
+            self.model = MemeUniter(uniter_model=base_model.uniter,
+                                    hidden_size=base_model.uniter.config.hidden_size,
+                                    n_classes=self.config['n_classes'])
+        else:
+            self.load_model()
+        print("MemeUniter")
+        print(self.model)
+
+    def load_model(self):
+        # Load pretrained model
+        if self.model_file:
+            checkpoint = torch.load(self.model_file)
+            LOGGER.info('Using UNITER model {}'.format(self.model_file))
+        else:
+            checkpoint = {}
+
+        uniter_config = UniterConfig.from_json_file(self.config['config'])
+        uniter_model = UniterModel(uniter_config, img_dim=IMG_DIM)
+
+        self.model = MemeUniter(uniter_model=uniter_model,
+                                hidden_size=uniter_model.config.hidden_size,
+                                n_classes=self.config['n_classes'])
+        self.model.load_state_dict(checkpoint['model_state_dict'])
+
+    def average_gradients(self, steps):
+        # Used when grad_accumulation > 1
+        for param in self.model.parameters():
+            if param.requires_grad and param.grad is not None:
+                param.grad = param.grad / steps
+
+    def calculate_loss(self, preds, batch_label, grad_step):
+        if self.config['loss_func'] == 'bce':
+            preds = torch.sigmoid(preds)
+        preds = preds.squeeze(1).to(
+            self.device) if self.config['loss_func'] == 'bce_logits' else preds.to(self.device)
+        loss = self.criterion(preds, batch_label.to(
+            self.device) if self.config['loss_func'] == 'ce' else batch_label.float().to(self.device))
+
+        if grad_step and self.iters % self.config['gradient_accumulation'] == 0:
+            loss.backward()
+            self.average_gradients(steps=self.config['gradient_accumulation'])
+            torch.nn.utils.clip_grad_norm_(
+                self.model.parameters(), self.config['max_grad_norm'])
+            self.optimizer.step()
+            self.scheduler.step()
+            self.optimizer.zero_grad()
+        elif grad_step:
+            loss.backward()
+
+        if self.config['loss_func'] == 'bce':
+            probs = preds
+            preds = (preds > 0.5).type(torch.FloatTensor)
+        elif self.config['loss_func'] == 'ce':
+            probs = F.softmax(preds, dim=1)
+            preds = torch.argmax(probs, dim=1)
+        elif self.config['loss_func'] == 'bce_logits':
+            probs = torch.sigmoid(preds)
+            preds = (probs > 0.5).type(torch.FloatTensor)
+
+        self.probs_list.append(probs.cpu().detach().numpy())
+        self.preds_list.append(preds.cpu().detach().numpy())
+        self.labels_list.append(batch_label.cpu().detach().numpy())
+        self.loss_list.append(loss.detach().item())
+        if grad_step:
+            self.short_loss_list.append(loss.detach().item())
+
+    def eval_model(self, test=False, test_idx=0):
+        self.model.eval()
+        self.preds_list, self.probs_list, self.labels_list, self.loss_list, self.id_list = [], [], [], [], []
+        batch_loader = self.config['val_loader'] if not test else self.config['test_loader'][test_idx]
+        with torch.no_grad():
+            for iters, batch in enumerate(batch_loader):
+                batch = self.batch_to_device(batch)
+                if batch_loader.dataset.return_ids:
+                    self.id_list.append(batch['ids'])
+                self.eval_iter_step(iters, batch, test=test)
+
+            self.probs_list = [
+                prob for batch_prob in self.probs_list for prob in batch_prob]
+            self.preds_list = [
+                pred for batch_pred in self.preds_list for pred in batch_pred]
+            self.labels_list = [
+                label for batch_labels in self.labels_list for label in batch_labels]
+            self.id_list = [
+                data_id for batch_id in self.id_list for data_id in batch_id]
+
+            val_loss = sum(self.loss_list)/len(self.loss_list)
+            eval_metrics = standard_metrics(torch.tensor(
+                self.probs_list), torch.tensor(self.labels_list), add_optimal_acc=True)
+            # if test:
+            # 	print(classification_report(np.array(self.labels_list), np.array(self.preds_list)))
+        return eval_metrics, val_loss
+
+    @torch.no_grad()
+    def export_test_predictions(self, test_idx=0, threshold=0.5):
+        self.model.eval()
+
+        # Step 2: Run model on the test set (no loss!)
+        # Ensure that ids are actually returned
+        assert self.config['test_loader'][test_idx].dataset.return_ids, "Can only export test results if the IDs are returned in the test dataset."
+        test_name = self.config['test_loader'][test_idx].dataset.name
+
+        prob_list = []
+        id_list = []
+        for iters, batch in enumerate(self.config['test_loader'][test_idx]):
+            batch = self.batch_to_device(batch)
+            id_list.append(batch['ids'].cpu())
+            probs = self.test_iter_step(batch)
+            if self.config['loss_func'] == 'bce_logits':
+                probs = torch.sigmoid(probs)
+            prob_list.append(probs.detach().cpu())
+
+        probs = torch.cat(prob_list, dim=0)
+        ids = torch.cat(id_list, dim=0)
+        preds = (probs > threshold).long()
+
+        # Step 3: Export predictions
+        self._export_preds(ids, probs, preds,
+                           file_postfix="_%s_preds.csv" % test_name)
+
+        LOGGER.info("Finished export of test predictions")
+
+    @torch.no_grad()
+    def export_val_predictions(self, test=False, test_idx=0, threshold=0.5):
+        batch_loader = self.config['val_loader'] if not test else self.config['test_loader'][test_idx]
+        test_name = batch_loader.dataset.name
+        LOGGER.info("Exporting %s predictions..." % (test_name))
+        self.model.eval()
+
+        # Step 1: Find the optimal threshold on validation set
+        _, _ = self.eval_model(test=test, test_idx=test_idx)
+        val_probs = torch.tensor(self.probs_list)
+        val_labels = torch.tensor(self.labels_list)
+        if len(self.id_list) != 0:
+            val_ids = torch.tensor(self.id_list)
+        else:
+            val_ids = torch.zeros_like(val_labels)-1
+        val_preds = (val_probs > threshold).long()
+
+        self._export_preds(val_ids, val_probs, val_preds,
+                           labels=val_labels, file_postfix="_%s_preds.csv" % test_name)
+
+        LOGGER.info("Finished export of %s predictions" % test_name)
+
+    def _export_preds(self, ids, probs, preds, labels=None, file_postfix="_preds.csv"):
+        file_string = "id,proba,label%s\n" % (
+            ",gt" if labels is not None else "")
+        for i in range(ids.shape[0]):
+            file_string += "%i,%f,%i" % (ids[i].item(),
+                                         probs[i].item(), preds[i].item())
+            if labels is not None:
+                file_string += ",%i" % labels[i].item()
+            file_string += "\n"
+        filepath = os.path.join(
+            self.config['model_path'], self.config['model_save_name'].rsplit(".", 1)[0] + file_postfix)
+        with open(filepath, "w") as f:
+            f.write(file_string)
+        wandb.save(filepath) #Upload file to wandb
+
+    def check_early_stopping(self):
+        self.this_metric = self.val_loss if self.config[
+            'optimize_for'] == 'loss' else self.val_metrics[self.config['optimize_for']]
+        self.current_best = self.best_val_loss if self.config[
+            'optimize_for'] == 'loss' else self.best_val_metrics[self.config['optimize_for']]
+
+        new_best = self.this_metric < self.current_best if self.config[
+            'optimize_for'] == 'loss' else self.this_metric > self.current_best
+        if new_best:
+            LOGGER.info("New High Score! Saving model...")
+            self.best_val_metrics = self.val_metrics
+            self.best_val_loss = self.val_loss
+            wandb.log({'Best val metrics': self.best_val_metrics,
+                       'Best val loss': self.best_val_loss})
+
+            if not self.config["no_model_checkpoints"]:
+                self.model_saver.save(self.model)
+
+        ### Stopping Criteria based on patience and change-in-metric-threshold ###
+        diff = self.current_best - \
+            self.this_metric if self.config['optimize_for'] == 'loss' else self.this_metric - \
+            self.current_best
+        if diff < self.config['early_stop_thresh']:
+            self.not_improved += 1
+            if self.not_improved >= self.config['patience']:
+                self.terminate_training = True
+        else:
+            self.not_improved = 0
+        LOGGER.info("current patience: {}".format(self.not_improved))
+
+    def train_epoch_step(self):
+        self.model.train()
+        lr = self.scheduler.get_last_lr()
+        self.total_iters += self.iters + 1
+        self.probs_list = [
+            pred for batch_pred in self.probs_list for pred in batch_pred]
+        self.labels_list = [
+            label for batch_labels in self.labels_list for label in batch_labels]
+
+        # Evaluate on train set
+        self.train_metrics = standard_metrics(torch.tensor(
+            self.probs_list), torch.tensor(self.labels_list), add_optimal_acc=True)
+        log_tensorboard(self.config, self.config['writer'], self.model, self.epoch, self.iters, self.total_iters,
+                        self.loss_list, self.train_metrics, lr[0], loss_only=False, val=False)
+        self.train_loss = self.loss_list[:]
+
+        # Evaluate on dev set
+        val_time = time.time()
+        self.val_metrics, self.val_loss = self.eval_model()
+        self.config['writer'].add_scalar(
+            "Stats/time_validation", time.time() - val_time, self.total_iters)
+
+        # print stats
+        print_stats(self.config, self.epoch, self.train_metrics,
+                    self.train_loss, self.val_metrics, self.val_loss, self.start, lr[0])
+
+        # log validation stats in tensorboard
+        log_tensorboard(self.config, self.config['writer'], self.model, self.epoch, self.iters,
+                        self.total_iters, self.val_loss, self.val_metrics, lr[0], loss_only=False, val=True)
+
+        # Check for early stopping criteria
+        self.check_early_stopping()
+        self.probs_list = []
+        self.preds_list = []
+        self.labels_list = []
+        self.loss_list = []
+        self.id_list = []
+
+        self.train_loss = sum(self.train_loss)/len(self.train_loss)
+        del self.val_metrics
+        del self.val_loss
+
+    def end_training(self):
+        # Termination message
+        print("\n" + "-"*100)
+        if self.terminate_training:
+            LOGGER.info("Training terminated early because the Validation {} did not improve for  {}  epochs" .format(
+                self.config['optimize_for'], self.config['patience']))
+        else:
+            LOGGER.info("Maximum epochs of {} reached. Finished training !!".format(
+                self.config['max_epoch']))
+
+        print_test_stats(self.best_val_metrics, test=False)
+
+        print("-"*50 + "\n\t\tEvaluating on test set\n" + "-"*50)
+        if not self.config["no_model_checkpoints"]:
+            if os.path.isfile(self.model_file):
+                self.load_model()
+                self.model.to(self.device)
+            else:
+                raise ValueError("No Saved model state_dict found for the chosen model...!!! \nAborting evaluation on test set...".format(
+                    self.config['model_name']))
+
+            self.export_val_predictions()  # Runs evaluation, no need to run it again here
+            val_probs = torch.tensor(self.probs_list)
+            val_labels = torch.tensor(self.labels_list)
+            threshold = 0.5  # the default threshelod for binary classification
+            # Uncomment below line if you have implemented this optional feature
+            # threshold = find_optimal_threshold(val_probs, val_labels, metric="accuracy")
+            best_val_metrics = standard_metrics(
+                val_probs, val_labels, threshold=threshold, add_aucroc=False)
+            LOGGER.info("Optimal threshold on validation dataset: %.4f (accuracy=%4.2f%%)" % (
+                threshold, 100.0*best_val_metrics["accuracy"]))
+
+            # Testing is in the standard form not possible, as we do not have any labels (gives an error in standard_metrics)
+            # Instead, we should write out the predictions in the form of the leaderboard
+            self.test_metrics = dict()
+            for test_idx in range(len(self.config['test_loader'])):
+                test_name = self.config['test_loader'][test_idx].dataset.name
+                LOGGER.info("Export and testing on %s..." % test_name)
+                if hasattr(self.config['test_loader'][test_idx].dataset, "data") and \
+                   hasattr(self.config['test_loader'][test_idx].dataset.data, "labels") and \
+                   self.config['test_loader'][test_idx].dataset.data.labels[0] == -1:  # Step 1: Find the optimal threshold on validation set
+                    self.export_test_predictions(
+                        test_idx=test_idx, threshold=threshold)
+                    self.test_metrics[test_name] = dict()
+                else:
+                    test_idx_metrics, _ = self.eval_model(
+                        test=True, test_idx=test_idx)
+                    self.test_metrics[test_name] = test_idx_metrics
+                    print_test_stats(test_idx_metrics, test=True)
+                    self.export_val_predictions(
+                        test=True, test_idx=test_idx, threshold=threshold)
+        else:
+            LOGGER.info(
+                "No model checkpoints were saved. Hence, testing will be skipped.")
+            self.test_metrics = dict()
+
+        self.export_metrics()
+
+        self.config['writer'].close()
+
+        if self.config['remove_checkpoints']:
+            LOGGER.info("Removing checkpoint %s..." % self.model_file)
+            os.remove(self.model_file)
+
+    def export_metrics(self):
+        metric_export_file = os.path.join(
+            self.config['model_path'], self.config['model_save_name'].rsplit(".", 1)[0] + "_metrics.json")
+        metric_dict = {}
+        metric_dict["dev"] = self.best_val_metrics
+        metric_dict["dev"]["loss"] = self.best_val_loss
+        metric_dict["train"] = self.train_metrics
+        metric_dict["train"]["loss"] = sum(self.train_loss)/len(
+            self.train_loss) if isinstance(self.train_loss, list) else self.train_loss
+        if hasattr(self, "test_metrics") and len(self.test_metrics) > 0:
+            metric_dict["test"] = self.test_metrics
+
+        with open(metric_export_file, "w") as f:
+            json.dump(metric_dict, f, indent=4)
+
+    def train_main(self, cache=False):
+        print("\n\n" + "="*100 + "\n\t\t\t\t\t Training Network\n" + "="*100)
+
+        self.start = time.time()
+        print("\nBeginning training at:  {} \n".format(datetime.datetime.now()))
+
+        self.model.to(self.device)
+
+        for self.epoch in range(self.start_epoch, self.config['max_epoch']+1):
+            train_times = []
+            for self.iters, self.batch in enumerate(self.config['train_loader']):
+                self.model.train()
+
+                iter_time = time.time()
+                self.batch = self.batch_to_device(self.batch)
+                self.train_iter_step()
+                train_times.append(time.time() - iter_time)
+
+                # Loss only logging
+                if (self.total_iters+self.iters+1) % self.config['log_every'] == 0:
+                    log_tensorboard(self.config, self.config['writer'], self.model, self.epoch,
+                                    self.iters, self.total_iters, self.short_loss_list, loss_only=True, val=False)
+                    self.config['writer'].add_scalar(
+                        'Stats/time_per_train_iter', mean(train_times), (self.iters+self.total_iters+1))
+                    self.config['writer'].add_scalar(
+                        'Stats/learning_rate', self.scheduler.get_last_lr()[0], (self.iters+self.total_iters+1))
+                    train_times = []
+                    self.short_loss_list = []
+            self.train_epoch_step()
+
+            if self.terminate_training:
+                break
+
+        self.end_training()
+        return self.best_val_metrics, self.test_metrics
+
+    def batch_to_device(self, batch):
+        batch = {k: (v.to(self.device) if isinstance(v, torch.Tensor) else v)
+                 for k, v in batch.items()}
+        return batch
+
+    def eval_iter_step(self, iters, batch, test):
+        # Forward pass
+        preds = self.model(img_feat=batch['img_feat'], img_pos_feat=batch['img_pos_feat'], input_ids=batch['input_ids'],
+                           position_ids=batch['position_ids'], attention_mask=batch['attn_mask'], gather_index=batch['gather_index'],
+                           output_all_encoded_layers=False, gender_race_probs=batch['gender_race_probs'])
+        self.calculate_loss(preds, batch['labels'], grad_step=False)
+
+    def train_iter_step(self):
+        # Forward pass
+        self.preds = self.model(img_feat=self.batch['img_feat'], img_pos_feat=self.batch['img_pos_feat'], input_ids=self.batch['input_ids'],
+                                position_ids=self.batch['position_ids'], attention_mask=self.batch[
+                                    'attn_mask'], gather_index=self.batch['gather_index'],
+                                output_all_encoded_layers=False, gender_race_probs=self.batch['gender_race_probs'])
+        self.calculate_loss(self.preds, self.batch['labels'], grad_step=True)
+
+    def test_iter_step(self, batch):
+        # Forward pass
+        preds = self.model(img_feat=batch['img_feat'], img_pos_feat=batch['img_pos_feat'], input_ids=batch['input_ids'],
+                           position_ids=batch['position_ids'], attention_mask=batch['attn_mask'], gather_index=batch['gather_index'],
+                           output_all_encoded_layers=False, gender_race_probs=batch['gender_race_probs'])
+        return preds.squeeze()
+
+
+if __name__ == '__main__':
+    wandb.init(project="multimodal-nlp2")
+    wandb.tensorboard.patch(root_logdir='./vis_checkpoints',
+                            pytorch=True, tensorboardX=False)
+    parser = argparse.ArgumentParser()
+    defaults = dict()
+
+    # Required Paths
+    parser.add_argument('--data_path', type=str, default='./dataset',
+                        help='path to dataset folder that contains the processed data files')
+    parser.add_argument('--model_path', type=str, default='./model_checkpoints',
+                        help='Directory for saving trained model checkpoints')
+    parser.add_argument('--vis_path', type=str, default='./vis_checkpoints',
+                        help='Directory for saving tensorboard checkpoints')
+    parser.add_argument("--model_save_name", type=str, default='best_model.pt',
+                        help='saved model name')
+    parser.add_argument("--no_model_checkpoints", action="store_true",
+                        help='If selected, no model checkpoints will be created, and no testing performed (for gridsearches etc.)')
+    parser.add_argument("--remove_checkpoints", action="store_true",
+                        help='If selected, model checkpoints will be deleted after finishing testing.')
+    parser.add_argument('--config', type=str, default='./config/uniter-base.json',
+                        help='JSON config file')
+    parser.add_argument('--feature_path', type=str, default='./dataset/img_feats',
+                        help='Path to image features')
+
+    # Load pretrained model
+    parser.add_argument('--pretrained_model_file', type=str,
+                        help='Name of the original pretrained model')
+
+    #### Pre-processing Params ####
+    parser.add_argument('--max_txt_len', type=int, default=60,
+                        help='max number of tokens in text (BERT BPE)')
+    parser.add_argument('--max_bb', type=int, default=100,
+                        help='max number of bounding boxes')
+    parser.add_argument('--min_bb', type=int, default=10,
+                        help='min number of bounding boxes')
+    parser.add_argument('--num_bb', type=int, default=36,
+                        help='static number of bounding boxes')
+
+    #### Training Params ####
+    # Named parameters
+    parser.add_argument('--optimizer', type=str, default=defaults.get('optimizer', 'adam'),
+                        help='Optimizer to use for training: adam / adamx / adamw')
+    parser.add_argument('--loss_func', type=str, default=defaults.get('loss_func', 'bce_logits'),
+                        help='Loss function to use for optimization: bce / bce_logits / ce')
+    parser.add_argument('--optimize_for', type=str, default=defaults.get('optimize_for', 'aucroc'),
+                        help='Optimize for what measure during training and early stopping: loss / F1 / aucroc / accuracy')
+    parser.add_argument('--scheduler', type=str, default=defaults.get('scheduler', 'warmup_cosine'),
+                        help='The type of lr scheduler to use anneal learning rate: step/multi_step/warmup/warmp_cosine')
+
+    # Numerical parameters
+    parser.add_argument('--beta1', type=float, default=defaults.get('beta1', 0.9),
+                        help='beta1 parameter in Adam optimizer')
+    parser.add_argument('--beta2', type=float, default=defaults.get('beta2', 0.999),
+                        help='beta2 parameter in Adam optimizer')
+    parser.add_argument('--batch_size', type=int, default=defaults.get('batch_size', 8),
+                        help='batch size for training')
+    parser.add_argument('--num_workers', type=int, default=defaults.get('num_workers', 0),
+                        help='Number of workers to start per dataset')
+    parser.add_argument('--gradient_accumulation', type=int, default=defaults.get('gradient_accumulation', 1),
+                        help='No. of update steps to accumulate before performing backward pass')
+    parser.add_argument('--max_grad_norm', type=int, default=defaults.get('max_grad_norm', 5),
+                        help='max gradient norm for gradient clipping')
+    parser.add_argument('--pos_wt', type=float, default=defaults.get('pos_wt', 1),
+                        help='Loss reweighting for the positive class to deal with class imbalance')
+    parser.add_argument('--lr', type=float, default=defaults.get('lr', 1e-4),
+                        help='Learning rate for training')
+    parser.add_argument('--warmup_steps', type=int, default=defaults.get('warmup_steps', 50),
+                        help='No. of steps to perform linear lr warmup for')
+    parser.add_argument('--weight_decay', type=float, default=defaults.get('weight_decay', 1e-3),
+                        help='weight decay for optimizer')
+    parser.add_argument('--max_epoch', type=int, default=defaults.get('max_epoch', 20),
+                        help='Max epochs to train for')
+    parser.add_argument('--lr_decay_step', type=float, default=defaults.get('lr_decay_step', 3),
+                        help='No. of epochs after which learning rate should be decreased')
+    parser.add_argument('--lr_decay_factor', type=float, default=defaults.get('lr_decay_factor', 0.8),
+                        help='Decay the learning rate of the optimizer by this multiplicative amount')
+    parser.add_argument('--patience', type=float, default=defaults.get('patience', 5),
+                        help='Patience no. of epochs for early stopping')
+    parser.add_argument('--early_stop_thresh', type=float, default=defaults.get('early_stop_thresh', 1e-3),
+                        help='Patience no. of epochs for early stopping')
+    parser.add_argument('--seed', type=int, default=defaults.get('seed', 42),
+                        help='set seed for reproducability')
+    parser.add_argument('--log_every', type=int, default=defaults.get('log_every', 2000),
+                        help='Log stats in Tensorboard every x iterations (not epochs) of training')
+    parser.add_argument('--fc_dim', type=int, default=64,
+                        help='dimen of FC layer"')
+    parser.add_argument('--dropout', type=float, default=0.2,
+                        help='Standard dropout regularization')
+
+    # New parameters by team
+    parser.add_argument('--filter_text', action='store_true',
+                        help='Filter out bounding boxes around text')
+    parser.add_argument('--no_normalize_img', action='store_false',
+                        help='Normalize images by dividing them by their height and width. Default=True')
+    parser.add_argument('--train_filename', type=str, default='train.jsonl',
+                        help='The name of the trainin json file to load.')
+    parser.add_argument('--upsample_multiplier', type=int, default=0,
+                        help='Multiplier used to increase the amount of confounders in training data')
+    parser.add_argument('--note', type=str, default='',
+                        help='Add a note that can be seen in wandb')
+    args, unparsed = parser.parse_known_args()
+    config = args.__dict__
+    wandb.config.update(config)
+    config['device'] = get_device()
+    config['n_classes'] = 2 if config['loss_func'] == 'ce' else 1
+
+    # Check all provided paths:
+    if not os.path.exists(config['data_path']):
+        raise ValueError("[!] ERROR: Dataset path does not exist")
+    else:
+        LOGGER.info("Data path checked..")
+    if not os.path.exists(config['model_path']):
+        LOGGER.warning("Creating checkpoint path for saved models at:  {}\n".format(
+            config['model_path']))
+        os.makedirs(config['model_path'])
+    else:
+        LOGGER.info("Model save path checked..")
+    if 'config' in config:
+        if not os.path.isfile(config['config']):
+            raise ValueError("[!] ERROR: config JSON path does not exist")
+        else:
+            LOGGER.info("config JSON path checked..")
+    if not os.path.exists(config['vis_path']):
+        LOGGER.warning("Creating checkpoint path for Tensorboard visualizations at:  {}\n".format(
+            config['vis_path']))
+        os.makedirs(config['vis_path'])
+    else:
+        LOGGER.info("Tensorboard Visualization path checked..")
+        LOGGER.info(
+            "Cleaning Visualization path of older tensorboard files...\n")
+        # shutil.rmtree(config['vis_path'])
+
+    # Print args
+    print("\n" + "x"*50 + "\n\nRunning training with the following parameters: \n")
+    for key, value in config.items():
+        if not key.endswith('transf'):
+            print(key + ' : ' + str(value))
+    print("\n" + "x"*50)
+
+    config['writer'] = SummaryWriter(config['vis_path'])
+
+    set_seed(config['seed'])
+
+    # Tokenize
+    tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
+    tokenizer_func = partial(tokenizer, max_length=config['max_txt_len'], padding='max_length',
+                             truncation=True, return_tensors='pt', return_length=True)
+
+    # Prepare the datasets and dataloaders for training and evaluation
+    train_dataset = MemeDataset(filepath=os.path.join(config['data_path'], config['train_filename']),
+                                feature_dir=config['feature_path'], text_padding=tokenizer_func, filter_text=config["filter_text"],
+                                upsample_multiplier=config["upsample_multiplier"])
+    val_dataset = MemeDataset(filepath=os.path.join(config['data_path'], 'dev_seen.jsonl'),
+                              feature_dir=config['feature_path'], text_padding=tokenizer_func, filter_text=config["filter_text"])
+    test_dataset = MemeDataset(filepath=os.path.join(config['data_path'], 'test_seen.jsonl'),
+                               feature_dir=config['feature_path'], text_padding=tokenizer_func, filter_text=config["filter_text"])
+
+    config['train_loader'] = data.DataLoader(train_dataset, batch_size=config['batch_size'],
+                                             num_workers=config['num_workers'], collate_fn=train_dataset.get_collate_fn(), shuffle=True, pin_memory=True)
+    config['val_loader'] = data.DataLoader(val_dataset, batch_size=config['batch_size'],
+                                           num_workers=config['num_workers'], collate_fn=val_dataset.get_collate_fn())
+    config['test_loader'] = data.DataLoader(test_dataset, batch_size=config['batch_size'],
+                                            num_workers=config['num_workers'], collate_fn=test_dataset.get_collate_fn())
+
+    try:
+        trainer = TrainerUniter(config)
+        trainer.train_main()
+        wandb.save('vis_checkpoints/*', base_path="vis_checkpoints/")
+        wandb.finish()
+    except KeyboardInterrupt:
+        LOGGER.warning(
+            "Keyboard interrupt by user detected...\nClosing the tensorboard writer!")
+        config['writer'].close()
diff --git a/wandb/run-20210223_112037-1rc3eqxy/files/config.yaml b/wandb/run-20210223_112037-1rc3eqxy/files/config.yaml
new file mode 100644
index 0000000..bb39cb8
--- /dev/null
+++ b/wandb/run-20210223_112037-1rc3eqxy/files/config.yaml
@@ -0,0 +1,149 @@
+wandb_version: 1
+
+_wandb:
+  desc: null
+  value:
+    cli_version: 0.10.19
+    code_path: code/train_uniter.py
+    framework: huggingface
+    huggingface_version: 3.2.0
+    is_jupyter_run: false
+    is_kaggle_kernel: false
+    python_version: 3.7.5
+    t:
+      1:
+      - 1
+      - 3
+      - 5
+      - 11
+      2:
+      - 1
+      - 3
+      - 5
+      - 11
+      4: 3.7.5
+      5: 0.10.19
+      6: 3.2.0
+batch_size:
+  desc: null
+  value: 16
+beta1:
+  desc: null
+  value: 0.9
+beta2:
+  desc: null
+  value: 0.999
+config:
+  desc: null
+  value: config/uniter-base.json
+data_path:
+  desc: null
+  value: ./dataset
+dropout:
+  desc: null
+  value: 0.2
+early_stop_thresh:
+  desc: null
+  value: 0.001
+fc_dim:
+  desc: null
+  value: 64
+feature_path:
+  desc: null
+  value: ./dataset/own_features
+filter_text:
+  desc: null
+  value: false
+gradient_accumulation:
+  desc: null
+  value: 2
+log_every:
+  desc: null
+  value: 2000
+loss_func:
+  desc: null
+  value: bce_logits
+lr:
+  desc: null
+  value: 3.0e-05
+lr_decay_factor:
+  desc: null
+  value: 0.8
+lr_decay_step:
+  desc: null
+  value: 3
+max_bb:
+  desc: null
+  value: 100
+max_epoch:
+  desc: null
+  value: 30
+max_grad_norm:
+  desc: null
+  value: 5
+max_txt_len:
+  desc: null
+  value: 60
+min_bb:
+  desc: null
+  value: 10
+model_path:
+  desc: null
+  value: ./model_checkpoints
+model_save_name:
+  desc: null
+  value: meme.pt
+no_model_checkpoints:
+  desc: null
+  value: false
+no_normalize_img:
+  desc: null
+  value: true
+note:
+  desc: null
+  value: ''
+num_bb:
+  desc: null
+  value: 36
+num_workers:
+  desc: null
+  value: 0
+optimize_for:
+  desc: null
+  value: aucroc
+optimizer:
+  desc: null
+  value: adam
+patience:
+  desc: null
+  value: 5.0
+pos_wt:
+  desc: null
+  value: 1.0
+pretrained_model_file:
+  desc: null
+  value: uniter-base.pt
+remove_checkpoints:
+  desc: null
+  value: false
+scheduler:
+  desc: null
+  value: warmup_cosine
+seed:
+  desc: null
+  value: 43
+train_filename:
+  desc: null
+  value: train.jsonl
+upsample_multiplier:
+  desc: null
+  value: 0
+vis_path:
+  desc: null
+  value: ./vis_checkpoints
+warmup_steps:
+  desc: null
+  value: 500
+weight_decay:
+  desc: null
+  value: 0.001
diff --git a/wandb/run-20210223_112037-1rc3eqxy/files/diff.patch b/wandb/run-20210223_112037-1rc3eqxy/files/diff.patch
new file mode 100644
index 0000000..499ae99
--- /dev/null
+++ b/wandb/run-20210223_112037-1rc3eqxy/files/diff.patch
@@ -0,0 +1,40 @@
+diff --git a/data/meme_dataset.py b/data/meme_dataset.py
+index 3e9ee53..f049f37 100644
+--- a/data/meme_dataset.py
++++ b/data/meme_dataset.py
+@@ -201,7 +201,7 @@ class MemeDataset(data.Dataset):
+                               for json_dict in f.readlines()]
+         print("Loaded dataset contains ", str(len(self.json_list)), "samples")
+         self._load_dataset()
+-        self._load_gender_race_preds()
++        self._load_gender_race_probs()
+     
+ 
+     def _load_dataset(self):
+diff --git a/wandb/debug-internal.log b/wandb/debug-internal.log
+index f09c7de..4ba1f06 120000
+--- a/wandb/debug-internal.log
++++ b/wandb/debug-internal.log
+@@ -1 +1 @@
+-run-20210222_215454-1602o824/logs/debug-internal.log
+\ No newline at end of file
++run-20210223_112037-1rc3eqxy/logs/debug-internal.log
+\ No newline at end of file
+diff --git a/wandb/debug.log b/wandb/debug.log
+index 9d13059..9cc7f55 120000
+--- a/wandb/debug.log
++++ b/wandb/debug.log
+@@ -1 +1 @@
+-run-20210222_215454-1602o824/logs/debug.log
+\ No newline at end of file
++run-20210223_112037-1rc3eqxy/logs/debug.log
+\ No newline at end of file
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 15abb24..264afe0 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20210222_215454-1602o824
+\ No newline at end of file
++run-20210223_112037-1rc3eqxy
+\ No newline at end of file
diff --git a/wandb/run-20210223_112037-1rc3eqxy/files/events.out.tfevents.1614075639.astro.1550008.0 b/wandb/run-20210223_112037-1rc3eqxy/files/events.out.tfevents.1614075639.astro.1550008.0
new file mode 120000
index 0000000..055725b
--- /dev/null
+++ b/wandb/run-20210223_112037-1rc3eqxy/files/events.out.tfevents.1614075639.astro.1550008.0
@@ -0,0 +1 @@
+/home/astro/Documents/UvA/Block 4 - NLP2/Multimodal NLP/Multimodal-NLP/vis_checkpoints/events.out.tfevents.1614075639.astro.1550008.0
\ No newline at end of file
diff --git a/wandb/run-20210223_112037-1rc3eqxy/files/output.log b/wandb/run-20210223_112037-1rc3eqxy/files/output.log
new file mode 100644
index 0000000..d72b172
--- /dev/null
+++ b/wandb/run-20210223_112037-1rc3eqxy/files/output.log
@@ -0,0 +1,68 @@
+23/02/2021 11:20:39 AM : INFO - Data path checked..
+
+xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
+
+Running training with the following parameters: 
+23/02/2021 11:20:39 AM : INFO - Model save path checked..
+23/02/2021 11:20:39 AM : INFO - config JSON path checked..
+23/02/2021 11:20:39 AM : INFO - Tensorboard Visualization path checked..
+23/02/2021 11:20:39 AM : INFO - Cleaning Visualization path of older tensorboard files...
+
+
+data_path : ./dataset
+model_path : ./model_checkpoints
+vis_path : ./vis_checkpoints
+model_save_name : meme.pt
+no_model_checkpoints : False
+remove_checkpoints : False
+config : config/uniter-base.json
+feature_path : ./dataset/own_features
+pretrained_model_file : uniter-base.pt
+max_txt_len : 60
+max_bb : 100
+min_bb : 10
+num_bb : 36
+optimizer : adam
+loss_func : bce_logits
+optimize_for : aucroc
+scheduler : warmup_cosine
+beta1 : 0.9
+beta2 : 0.999
+batch_size : 16
+num_workers : 0
+gradient_accumulation : 2
+max_grad_norm : 5
+pos_wt : 1.0
+lr : 3e-05
+warmup_steps : 500
+weight_decay : 0.001
+max_epoch : 30
+lr_decay_step : 3
+lr_decay_factor : 0.8
+patience : 5.0
+early_stop_thresh : 0.001
+seed : 43
+log_every : 2000
+fc_dim : 64
+dropout : 0.2
+filter_text : False
+no_normalize_img : True
+train_filename : train.jsonl
+upsample_multiplier : 0
+note : 
+device : cuda
+n_classes : 1
+
+xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
+filter text False
+Loaded dataset contains  8500 samples
+Traceback (most recent call last):
+  File "train_uniter.py", line 633, in <module>
+    upsample_multiplier=config["upsample_multiplier"])
+  File "/home/astro/Documents/UvA/Block 4 - NLP2/Multimodal NLP/Multimodal-NLP/data/meme_dataset.py", line 64, in __init__
+    self._prepare_data_list()
+  File "/home/astro/Documents/UvA/Block 4 - NLP2/Multimodal NLP/Multimodal-NLP/data/meme_dataset.py", line 204, in _prepare_data_list
+    self._load_gender_race_probs()
+  File "/home/astro/Documents/UvA/Block 4 - NLP2/Multimodal NLP/Multimodal-NLP/data/meme_dataset.py", line 245, in _load_gender_race_probs
+    with open(f'../dataset/gender_race_preds/{self.name}_gender_race_probs.pickle', 'rb') as f:
+FileNotFoundError: [Errno 2] No such file or directory: '../dataset/gender_race_preds/train_gender_race_probs.pickle'
diff --git a/wandb/run-20210223_112037-1rc3eqxy/files/requirements.txt b/wandb/run-20210223_112037-1rc3eqxy/files/requirements.txt
new file mode 100644
index 0000000..0d8abf6
--- /dev/null
+++ b/wandb/run-20210223_112037-1rc3eqxy/files/requirements.txt
@@ -0,0 +1,176 @@
+absl-py==0.10.0
+aiohttp==3.6.2
+apex==0.1
+argon2-cffi==20.1.0
+astor==0.8.1
+astunparse==1.6.3
+async-generator==1.10
+async-timeout==3.0.1
+attrs==20.2.0
+autopep8==1.5.5
+backcall==0.2.0
+beautifulsoup4==4.9.2
+bleach==3.2.1
+boto3==1.15.6
+botocore==1.18.6
+cached-property==1.5.2
+cachetools==4.1.1
+certifi==2020.6.20
+cffi==1.14.3
+chardet==3.0.4
+click==7.1.2
+configparser==5.0.1
+cycler==0.10.0
+dbfread==2.0.4
+deap==1.3.1
+decorator==4.4.2
+defusedxml==0.6.0
+demjson==2.2.4
+dlib==19.21.1
+docker-pycreds==0.4.0
+editdistance==0.5.3
+entrypoints==0.3
+fasttext==0.9.1
+filelock==3.0.12
+flask==1.1.2
+flatbuffers==1.12
+future==0.18.2
+gast==0.3.3
+gdown==3.12.2
+gitdb==4.0.5
+gitpython==3.1.0
+google-auth-oauthlib==0.4.1
+google-auth==1.22.0
+google-pasta==0.2.0
+gql==2.0.0
+graphql-core==2.3.2
+grpcio==1.32.0
+h5py==2.10.0
+idna==2.10
+ijson==2.6.1
+imagehash==4.2.0
+importlib-metadata==2.0.0
+ipykernel==5.3.4
+ipython-genutils==0.2.0
+ipython==7.20.0
+ipywidgets==7.5.1
+itsdangerous==1.1.0
+jedi==0.17.2
+jinja2==2.11.2
+jmespath==0.10.0
+joblib==0.16.0
+jsonschema==3.2.0
+jupyter-client==6.1.7
+jupyter-core==4.6.3
+jupyterlab-pygments==0.1.2
+keras-applications==1.0.8
+keras-preprocessing==1.1.2
+keras==2.4.3
+kiwisolver==1.2.0
+lightgbm==3.1.1
+lmdb==0.98
+markdown==3.2.2
+markupsafe==1.1.1
+matplotlib==3.3.2
+mdbtools==0.3.14
+meza==0.42.5
+mistune==0.8.4
+mtcnn==0.1.0
+multidict==4.7.6
+nbclient==0.5.0
+nbconvert==6.0.7
+nbformat==5.0.7
+nest-asyncio==1.4.1
+nltk==3.4.5
+notebook==6.1.4
+numpy==1.19.2
+nvidia-ml-py3==7.352.0
+oauthlib==3.1.0
+omegaconf==2.0.1rc4
+opencv-contrib-python==4.5.1.48
+opencv-python==4.5.1.48
+opt-einsum==3.3.0
+packaging==20.4
+pandas==1.1.2
+pandocfilters==1.4.2
+parso==0.7.1
+pathtools==0.1.2
+pexpect==4.8.0
+pickleshare==0.7.5
+pillow==7.2.0
+pip==20.3.3
+prometheus-client==0.8.0
+promise==2.3
+prompt-toolkit==3.0.7
+protobuf==3.13.0
+psutil==5.8.0
+ptyprocess==0.6.0
+pyasn1-modules==0.2.8
+pyasn1==0.4.8
+pybind11==2.6.2
+pycodestyle==2.6.0
+pycparser==2.20
+pygments==2.7.1
+pygogo==0.13.2
+pymongo==3.11.0
+pyparsing==2.4.7
+pyrsistent==0.17.3
+pysocks==1.7.1
+python-dateutil==2.8.1
+python-slugify==1.2.6
+pytz==2020.1
+pywavelets==1.1.1
+pyyaml==5.3.1
+pyzmq==19.0.2
+regex==2020.9.27
+requests-oauthlib==1.3.0
+requests==2.23.0
+rsa==4.6
+rx==1.6.1
+s3transfer==0.3.3
+sacremoses==0.0.43
+scikit-learn==0.23.2
+scipy==1.5.2
+seaborn==0.11.0
+send2trash==1.5.0
+sentencepiece==0.1.91
+sentry-sdk==0.20.3
+setuptools==52.0.0.post20210125
+shortuuid==1.0.1
+six==1.15.0
+sklearn==0.0
+smmap==3.0.4
+soupsieve==2.0.1
+subprocess32==3.5.4
+tensorboard-plugin-wit==1.7.0
+tensorboard==2.4.1
+tensorflow-estimator==2.4.0
+tensorflow==2.4.1
+termcolor==1.1.0
+terminado==0.9.1
+testpath==0.4.4
+threadpoolctl==2.1.0
+tokenizers==0.8.1rc2
+toml==0.10.2
+toolz==0.11.1
+torch==1.6.0+cu101
+torchtext==0.5.0
+torchvision==0.7.0+cu101
+tornado==6.0.4
+tqdm==4.50.0
+traitlets==5.0.5
+transformers==3.2.0
+typing-extensions==3.7.4.3
+unidecode==1.1.1
+urllib3==1.25.10
+wandb==0.10.19
+watchdog==2.0.1
+wcwidth==0.2.5
+webencodings==0.5.1
+werkzeug==1.0.1
+wheel==0.36.2
+widgetsnbextension==3.5.1
+wrapt==1.12.1
+xlrd==1.2.0
+yarl==1.6.0
+zipp==3.2.0
\ No newline at end of file
diff --git a/wandb/run-20210223_112037-1rc3eqxy/files/wandb-metadata.json b/wandb/run-20210223_112037-1rc3eqxy/files/wandb-metadata.json
new file mode 100644
index 0000000..5000328
--- /dev/null
+++ b/wandb/run-20210223_112037-1rc3eqxy/files/wandb-metadata.json
@@ -0,0 +1,55 @@
+{
+    "os": "Linux-5.8.0-43-generic-x86_64-with-debian-bullseye-sid",
+    "python": "3.7.5",
+    "heartbeatAt": "2021-02-23T10:20:39.254573",
+    "startedAt": "2021-02-23T10:20:37.917872",
+    "docker": null,
+    "gpu": "GeForce RTX 2060 SUPER",
+    "gpu_count": 1,
+    "cpu_count": 24,
+    "cuda": "11.0.228",
+    "args": [
+        "--config",
+        "config/uniter-base.json",
+        "--data_path",
+        "./dataset",
+        "--model_path",
+        "./model_checkpoints",
+        "--pretrained_model_file",
+        "uniter-base.pt",
+        "--feature_path",
+        "./dataset/own_features",
+        "--lr",
+        "3e-5",
+        "--scheduler",
+        "warmup_cosine",
+        "--warmup_steps",
+        "500",
+        "--max_epoch",
+        "30",
+        "--batch_size",
+        "16",
+        "--patience",
+        "5",
+        "--gradient_accumulation",
+        "2",
+        "--model_save_name",
+        "meme.pt",
+        "--seed",
+        "43",
+        "--pos_wt",
+        "1"
+    ],
+    "state": "running",
+    "codePath": "train_uniter.py",
+    "program": "train_uniter.py",
+    "git": {
+        "remote": "https://github.com/Noixas/Multimodal-NLP.git",
+        "commit": "83666a1f2f64375554a1633e19e39a7c7cef964b"
+    },
+    "email": "rodrigo.mulsa@outlook.com",
+    "root": "/home/astro/Documents/UvA/Block 4 - NLP2/Multimodal NLP/Multimodal-NLP",
+    "host": "astro",
+    "username": "astro",
+    "executable": "/home/astro/anaconda3/envs/nlp2-multimodal/bin/python"
+}
diff --git a/wandb/run-20210223_112037-1rc3eqxy/files/wandb-summary.json b/wandb/run-20210223_112037-1rc3eqxy/files/wandb-summary.json
new file mode 100644
index 0000000..9e26dfe
--- /dev/null
+++ b/wandb/run-20210223_112037-1rc3eqxy/files/wandb-summary.json
@@ -0,0 +1 @@
+{}
\ No newline at end of file
diff --git a/wandb/run-20210223_112037-1rc3eqxy/logs/debug-internal.log b/wandb/run-20210223_112037-1rc3eqxy/logs/debug-internal.log
new file mode 100644
index 0000000..bce1fca
--- /dev/null
+++ b/wandb/run-20210223_112037-1rc3eqxy/logs/debug-internal.log
@@ -0,0 +1,120 @@
+2021-02-23 11:20:38,274 INFO    MainThread:1550068 [internal.py:wandb_internal():91] W&B internal server running at pid: 1550068, started at: 2021-02-23 11:20:38.273852
+2021-02-23 11:20:38,275 DEBUG   HandlerThread:1550068 [handler.py:handle_request():94] handle_request: check_version
+2021-02-23 11:20:38,275 INFO    WriterThread:1550068 [datastore.py:open_for_write():77] open: /home/astro/Documents/UvA/Block 4 - NLP2/Multimodal NLP/Multimodal-NLP/wandb/run-20210223_112037-1rc3eqxy/run-1rc3eqxy.wandb
+2021-02-23 11:20:38,276 DEBUG   SenderThread:1550068 [sender.py:send():117] send: header
+2021-02-23 11:20:38,276 DEBUG   SenderThread:1550068 [sender.py:send():117] send: request
+2021-02-23 11:20:38,276 DEBUG   SenderThread:1550068 [sender.py:send_request():126] send_request: check_version
+2021-02-23 11:20:38,308 DEBUG   SenderThread:1550068 [sender.py:send():117] send: run
+2021-02-23 11:20:38,484 INFO    SenderThread:1550068 [sender.py:_start_run_threads():596] run started: 1rc3eqxy with start time 1614075637
+2021-02-23 11:20:38,484 DEBUG   SenderThread:1550068 [sender.py:send():117] send: summary
+2021-02-23 11:20:38,484 INFO    SenderThread:1550068 [sender.py:_save_file():682] saving file wandb-summary.json with policy end
+2021-02-23 11:20:38,484 DEBUG   HandlerThread:1550068 [handler.py:handle_request():94] handle_request: run_start
+2021-02-23 11:20:39,254 DEBUG   HandlerThread:1550068 [meta.py:__init__():34] meta init
+2021-02-23 11:20:39,254 DEBUG   HandlerThread:1550068 [meta.py:__init__():48] meta init done
+2021-02-23 11:20:39,254 DEBUG   HandlerThread:1550068 [meta.py:probe():190] probe
+2021-02-23 11:20:39,260 DEBUG   HandlerThread:1550068 [meta.py:_setup_git():180] setup git
+2021-02-23 11:20:39,277 DEBUG   HandlerThread:1550068 [meta.py:_setup_git():187] setup git done
+2021-02-23 11:20:39,277 DEBUG   HandlerThread:1550068 [meta.py:_save_code():69] save code
+2021-02-23 11:20:39,286 DEBUG   HandlerThread:1550068 [meta.py:_save_code():90] save code done
+2021-02-23 11:20:39,287 DEBUG   HandlerThread:1550068 [meta.py:_save_patches():107] save patches
+2021-02-23 11:20:39,389 DEBUG   HandlerThread:1550068 [meta.py:_save_patches():149] save patches done
+2021-02-23 11:20:39,389 DEBUG   HandlerThread:1550068 [meta.py:_save_pip():52] save pip
+2021-02-23 11:20:39,389 DEBUG   HandlerThread:1550068 [meta.py:_save_pip():66] save pip done
+2021-02-23 11:20:39,389 DEBUG   HandlerThread:1550068 [meta.py:probe():231] probe done
+2021-02-23 11:20:39,392 DEBUG   SenderThread:1550068 [sender.py:send():117] send: files
+2021-02-23 11:20:39,392 INFO    SenderThread:1550068 [sender.py:_save_file():682] saving file wandb-metadata.json with policy now
+2021-02-23 11:20:39,392 INFO    SenderThread:1550068 [sender.py:_save_file():682] saving file code/train_uniter.py with policy now
+2021-02-23 11:20:39,392 INFO    SenderThread:1550068 [sender.py:_save_file():682] saving file diff.patch with policy now
+2021-02-23 11:20:39,398 DEBUG   HandlerThread:1550068 [handler.py:handle_request():94] handle_request: status
+2021-02-23 11:20:39,398 DEBUG   SenderThread:1550068 [sender.py:send():117] send: request
+2021-02-23 11:20:39,399 DEBUG   SenderThread:1550068 [sender.py:send_request():126] send_request: status
+2021-02-23 11:20:39,405 INFO    HandlerThread:1550068 [handler.py:handle_tbrecord():307] handling tbrecord: tbrecord {
+  log_dir: "./vis_checkpoints"
+  save: true
+  root_dir: "./vis_checkpoints"
+}
+
+2021-02-23 11:20:39,406 DEBUG   HandlerThread:1550068 [config_util.py:dict_from_config_file():99] no default config file found in config-defaults.yaml
+2021-02-23 11:20:39,535 DEBUG   SenderThread:1550068 [sender.py:send():117] send: config
+2021-02-23 11:20:39,701 DEBUG   SenderThread:1550068 [sender.py:send():117] send: tbrecord
+2021-02-23 11:20:39,701 DEBUG   SenderThread:1550068 [sender.py:send():117] send: files
+2021-02-23 11:20:39,702 INFO    SenderThread:1550068 [sender.py:_save_file():682] saving file events.out.tfevents.1614075639.astro.1550008.0 with policy live
+2021-02-23 11:20:42,643 DEBUG   SenderThread:1550068 [sender.py:send():117] send: stats
+2021-02-23 11:20:45,023 DEBUG   SenderThread:1550068 [sender.py:send():117] send: telemetry
+2021-02-23 11:20:45,023 DEBUG   HandlerThread:1550068 [handler.py:handle_request():94] handle_request: poll_exit
+2021-02-23 11:20:45,189 DEBUG   SenderThread:1550068 [sender.py:send():117] send: exit
+2021-02-23 11:20:45,189 INFO    SenderThread:1550068 [sender.py:send_exit():195] handling exit code: 1
+2021-02-23 11:20:45,189 INFO    SenderThread:1550068 [sender.py:send_exit():203] send defer
+2021-02-23 11:20:45,189 DEBUG   SenderThread:1550068 [sender.py:send():117] send: request
+2021-02-23 11:20:45,189 DEBUG   SenderThread:1550068 [sender.py:send_request():126] send_request: poll_exit
+2021-02-23 11:20:45,190 DEBUG   HandlerThread:1550068 [handler.py:handle_request():94] handle_request: defer
+2021-02-23 11:20:45,190 INFO    HandlerThread:1550068 [handler.py:handle_request_defer():108] handle defer: 0
+2021-02-23 11:20:45,190 DEBUG   SenderThread:1550068 [sender.py:send():117] send: request
+2021-02-23 11:20:45,190 DEBUG   SenderThread:1550068 [sender.py:send_request():126] send_request: defer
+2021-02-23 11:20:45,190 INFO    SenderThread:1550068 [sender.py:send_request_defer():212] handle sender defer: 0
+2021-02-23 11:20:45,190 INFO    SenderThread:1550068 [sender.py:send_request_defer():248] send defer: 1
+2021-02-23 11:20:45,190 DEBUG   HandlerThread:1550068 [handler.py:handle_request():94] handle_request: defer
+2021-02-23 11:20:45,190 INFO    HandlerThread:1550068 [handler.py:handle_request_defer():108] handle defer: 1
+2021-02-23 11:20:45,287 DEBUG   SenderThread:1550068 [sender.py:send():117] send: request
+2021-02-23 11:20:45,287 DEBUG   SenderThread:1550068 [sender.py:send_request():126] send_request: defer
+2021-02-23 11:20:45,287 INFO    SenderThread:1550068 [sender.py:send_request_defer():212] handle sender defer: 1
+2021-02-23 11:20:45,287 INFO    SenderThread:1550068 [sender.py:send_request_defer():248] send defer: 2
+2021-02-23 11:20:45,287 DEBUG   SenderThread:1550068 [sender.py:send():117] send: stats
+2021-02-23 11:20:45,288 DEBUG   HandlerThread:1550068 [handler.py:handle_request():94] handle_request: defer
+2021-02-23 11:20:45,288 INFO    HandlerThread:1550068 [handler.py:handle_request_defer():108] handle defer: 2
+2021-02-23 11:20:51,408 DEBUG   SenderThread:1550068 [sender.py:send():117] send: request
+2021-02-23 11:20:51,408 DEBUG   HandlerThread:1550068 [handler.py:handle_request():94] handle_request: poll_exit
+2021-02-23 11:20:51,408 DEBUG   SenderThread:1550068 [sender.py:send_request():126] send_request: defer
+2021-02-23 11:20:51,408 INFO    SenderThread:1550068 [sender.py:send_request_defer():212] handle sender defer: 2
+2021-02-23 11:20:51,408 INFO    SenderThread:1550068 [sender.py:send_request_defer():248] send defer: 3
+2021-02-23 11:20:51,409 DEBUG   SenderThread:1550068 [sender.py:send():117] send: request
+2021-02-23 11:20:51,409 DEBUG   SenderThread:1550068 [sender.py:send_request():126] send_request: poll_exit
+2021-02-23 11:20:51,409 DEBUG   HandlerThread:1550068 [handler.py:handle_request():94] handle_request: defer
+2021-02-23 11:20:51,409 INFO    HandlerThread:1550068 [handler.py:handle_request_defer():108] handle defer: 3
+2021-02-23 11:20:51,409 DEBUG   SenderThread:1550068 [sender.py:send():117] send: summary
+2021-02-23 11:20:51,412 INFO    SenderThread:1550068 [sender.py:_save_file():682] saving file wandb-summary.json with policy end
+2021-02-23 11:20:51,412 DEBUG   SenderThread:1550068 [sender.py:send():117] send: request
+2021-02-23 11:20:51,412 DEBUG   SenderThread:1550068 [sender.py:send_request():126] send_request: defer
+2021-02-23 11:20:51,412 INFO    SenderThread:1550068 [sender.py:send_request_defer():212] handle sender defer: 3
+2021-02-23 11:20:51,413 INFO    SenderThread:1550068 [sender.py:send_request_defer():248] send defer: 4
+2021-02-23 11:20:51,413 DEBUG   HandlerThread:1550068 [handler.py:handle_request():94] handle_request: defer
+2021-02-23 11:20:51,413 INFO    HandlerThread:1550068 [handler.py:handle_request_defer():108] handle defer: 4
+2021-02-23 11:20:51,413 DEBUG   SenderThread:1550068 [sender.py:send():117] send: request
+2021-02-23 11:20:51,413 DEBUG   SenderThread:1550068 [sender.py:send_request():126] send_request: defer
+2021-02-23 11:20:51,413 INFO    SenderThread:1550068 [sender.py:send_request_defer():212] handle sender defer: 4
+2021-02-23 11:20:51,499 INFO    SenderThread:1550068 [sender.py:send_request_defer():248] send defer: 5
+2021-02-23 11:20:51,503 DEBUG   HandlerThread:1550068 [handler.py:handle_request():94] handle_request: defer
+2021-02-23 11:20:51,503 INFO    HandlerThread:1550068 [handler.py:handle_request_defer():108] handle defer: 5
+2021-02-23 11:20:51,503 DEBUG   SenderThread:1550068 [sender.py:send():117] send: request
+2021-02-23 11:20:51,504 DEBUG   SenderThread:1550068 [sender.py:send_request():126] send_request: defer
+2021-02-23 11:20:51,504 INFO    SenderThread:1550068 [sender.py:send_request_defer():212] handle sender defer: 5
+2021-02-23 11:20:51,504 INFO    SenderThread:1550068 [sender.py:send_request_defer():248] send defer: 6
+2021-02-23 11:20:51,504 DEBUG   HandlerThread:1550068 [handler.py:handle_request():94] handle_request: defer
+2021-02-23 11:20:51,504 INFO    HandlerThread:1550068 [handler.py:handle_request_defer():108] handle defer: 6
+2021-02-23 11:20:51,504 DEBUG   SenderThread:1550068 [sender.py:send():117] send: request
+2021-02-23 11:20:51,504 DEBUG   SenderThread:1550068 [sender.py:send_request():126] send_request: defer
+2021-02-23 11:20:51,504 INFO    SenderThread:1550068 [sender.py:send_request_defer():212] handle sender defer: 6
+2021-02-23 11:20:51,668 INFO    SenderThread:1550068 [sender.py:send_request_defer():248] send defer: 7
+2021-02-23 11:20:51,669 DEBUG   HandlerThread:1550068 [handler.py:handle_request():94] handle_request: defer
+2021-02-23 11:20:51,669 INFO    HandlerThread:1550068 [handler.py:handle_request_defer():108] handle defer: 7
+2021-02-23 11:20:51,669 DEBUG   SenderThread:1550068 [sender.py:send():117] send: request
+2021-02-23 11:20:51,669 DEBUG   SenderThread:1550068 [sender.py:send_request():126] send_request: defer
+2021-02-23 11:20:51,669 INFO    SenderThread:1550068 [sender.py:send_request_defer():212] handle sender defer: 7
+2021-02-23 11:20:51,669 INFO    SenderThread:1550068 [sender.py:send_request_defer():248] send defer: 8
+2021-02-23 11:20:51,669 DEBUG   SenderThread:1550068 [sender.py:send():117] send: final
+2021-02-23 11:20:51,669 DEBUG   SenderThread:1550068 [sender.py:send():117] send: footer
+2021-02-23 11:20:51,670 DEBUG   HandlerThread:1550068 [handler.py:handle_request():94] handle_request: defer
+2021-02-23 11:20:51,670 INFO    HandlerThread:1550068 [handler.py:handle_request_defer():108] handle defer: 8
+2021-02-23 11:20:51,670 DEBUG   SenderThread:1550068 [sender.py:send():117] send: request
+2021-02-23 11:20:51,670 DEBUG   SenderThread:1550068 [sender.py:send_request():126] send_request: defer
+2021-02-23 11:20:51,670 INFO    SenderThread:1550068 [sender.py:send_request_defer():212] handle sender defer: 8
+2021-02-23 11:20:53,411 DEBUG   HandlerThread:1550068 [handler.py:handle_request():94] handle_request: poll_exit
+2021-02-23 11:20:53,411 DEBUG   SenderThread:1550068 [sender.py:send():117] send: request
+2021-02-23 11:20:53,412 DEBUG   SenderThread:1550068 [sender.py:send_request():126] send_request: poll_exit
+2021-02-23 11:20:53,413 DEBUG   HandlerThread:1550068 [handler.py:handle_request():94] handle_request: get_summary
+2021-02-23 11:20:53,413 DEBUG   HandlerThread:1550068 [handler.py:handle_request():94] handle_request: sampled_history
+2021-02-23 11:20:53,414 DEBUG   HandlerThread:1550068 [handler.py:handle_request():94] handle_request: shutdown
+2021-02-23 11:20:53,414 INFO    HandlerThread:1550068 [handler.py:finish():333] shutting down handler
+2021-02-23 11:20:53,670 INFO    WriterThread:1550068 [datastore.py:close():258] close: /home/astro/Documents/UvA/Block 4 - NLP2/Multimodal NLP/Multimodal-NLP/wandb/run-20210223_112037-1rc3eqxy/run-1rc3eqxy.wandb
+2021-02-23 11:20:54,412 INFO    SenderThread:1550068 [sender.py:finish():766] shutting down sender
+2021-02-23 11:20:54,413 INFO    MainThread:1550068 [internal.py:handle_exit():78] Internal process exited
diff --git a/wandb/run-20210223_112037-1rc3eqxy/logs/debug.log b/wandb/run-20210223_112037-1rc3eqxy/logs/debug.log
new file mode 100644
index 0000000..152ef3f
--- /dev/null
+++ b/wandb/run-20210223_112037-1rc3eqxy/logs/debug.log
@@ -0,0 +1,78 @@
+2021-02-23 11:20:37,919 INFO    MainThread:1550008 [wandb_setup.py:_flush():70] setting env: {}
+2021-02-23 11:20:37,919 INFO    MainThread:1550008 [wandb_setup.py:_flush():70] multiprocessing start_methods=fork,spawn,forkserver, using: spawn
+2021-02-23 11:20:37,919 INFO    MainThread:1550008 [wandb_setup.py:_flush():70] setting login settings: {}
+2021-02-23 11:20:37,919 INFO    MainThread:1550008 [wandb_init.py:_log_setup():319] Logging user logs to /home/astro/Documents/UvA/Block 4 - NLP2/Multimodal NLP/Multimodal-NLP/wandb/run-20210223_112037-1rc3eqxy/logs/debug.log
+2021-02-23 11:20:37,920 INFO    MainThread:1550008 [wandb_init.py:_log_setup():320] Logging internal logs to /home/astro/Documents/UvA/Block 4 - NLP2/Multimodal NLP/Multimodal-NLP/wandb/run-20210223_112037-1rc3eqxy/logs/debug-internal.log
+2021-02-23 11:20:37,920 INFO    MainThread:1550008 [wandb_init.py:init():351] calling init triggers
+2021-02-23 11:20:37,920 INFO    MainThread:1550008 [wandb_init.py:init():358] wandb.init called with sweep_config: {}
+config: {}
+2021-02-23 11:20:37,920 INFO    MainThread:1550008 [wandb_init.py:init():404] starting backend
+2021-02-23 11:20:37,928 INFO    MainThread:1550008 [backend.py:ensure_launched():81] starting backend process...
+2021-02-23 11:20:37,932 INFO    MainThread:1550008 [backend.py:ensure_launched():86] started backend process with pid: 1550068
+2021-02-23 11:20:37,933 INFO    MainThread:1550008 [wandb_init.py:init():413] backend started and connected
+2021-02-23 11:20:37,934 INFO    MainThread:1550008 [wandb_init.py:init():436] updated telemetry
+2021-02-23 11:20:37,934 INFO    MainThread:1550008 [wandb_init.py:init():459] communicating current version
+2021-02-23 11:20:38,307 INFO    MainThread:1550008 [wandb_init.py:init():464] got version response upgrade_message: "wandb version 0.10.20 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
+
+2021-02-23 11:20:38,307 INFO    MainThread:1550008 [wandb_init.py:init():472] communicating run to backend with 30 second timeout
+2021-02-23 11:20:38,484 INFO    MainThread:1550008 [wandb_init.py:init():495] starting run threads in backend
+2021-02-23 11:20:39,394 INFO    MainThread:1550008 [wandb_run.py:_console_start():1411] atexit reg
+2021-02-23 11:20:39,395 INFO    MainThread:1550008 [wandb_run.py:_redirect():1274] redirect: SettingsConsole.REDIRECT
+2021-02-23 11:20:39,395 INFO    MainThread:1550008 [wandb_run.py:_redirect():1279] Redirecting console.
+2021-02-23 11:20:39,395 INFO    MainThread:1550008 [redirect.py:install():213] install start
+2021-02-23 11:20:39,396 INFO    MainThread:1550008 [redirect.py:install():228] install stop
+2021-02-23 11:20:39,396 INFO    MainThread:1550008 [redirect.py:install():213] install start
+2021-02-23 11:20:39,396 INFO    MainThread:1550008 [redirect.py:install():228] install stop
+2021-02-23 11:20:39,396 INFO    MainThread:1550008 [wandb_run.py:_redirect():1325] Redirects installed.
+2021-02-23 11:20:39,396 INFO    MainThread:1550008 [wandb_init.py:init():518] run started, returning control to user process
+2021-02-23 11:20:39,400 INFO    MainThread:1550008 [wandb_run.py:_config_callback():663] config_cb None None {'data_path': './dataset', 'model_path': './model_checkpoints', 'vis_path': './vis_checkpoints', 'model_save_name': 'meme.pt', 'no_model_checkpoints': False, 'remove_checkpoints': False, 'config': 'config/uniter-base.json', 'feature_path': './dataset/own_features', 'pretrained_model_file': 'uniter-base.pt', 'max_txt_len': 60, 'max_bb': 100, 'min_bb': 10, 'num_bb': 36, 'optimizer': 'adam', 'loss_func': 'bce_logits', 'optimize_for': 'aucroc', 'scheduler': 'warmup_cosine', 'beta1': 0.9, 'beta2': 0.999, 'batch_size': 16, 'num_workers': 0, 'gradient_accumulation': 2, 'max_grad_norm': 5, 'pos_wt': 1.0, 'lr': 3e-05, 'warmup_steps': 500, 'weight_decay': 0.001, 'max_epoch': 30, 'lr_decay_step': 3, 'lr_decay_factor': 0.8, 'patience': 5.0, 'early_stop_thresh': 0.001, 'seed': 43, 'log_every': 2000, 'fc_dim': 64, 'dropout': 0.2, 'filter_text': False, 'no_normalize_img': True, 'train_filename': 'train.jsonl', 'upsample_multiplier': 0, 'note': ''}
+2021-02-23 11:20:39,403 INFO    MainThread:1550008 [wandb_run.py:_tensorboard_callback():734] tensorboard callback: ./vis_checkpoints, None
+2021-02-23 11:20:45,021 INFO    MainThread:1550008 [wandb_run.py:_atexit_cleanup():1381] got exitcode: 1
+2021-02-23 11:20:45,021 INFO    MainThread:1550008 [wandb_run.py:_restore():1353] restore
+2021-02-23 11:20:45,021 INFO    MainThread:1550008 [redirect.py:uninstall():232] uninstall start
+2021-02-23 11:20:45,021 INFO    MainThread:1550008 [redirect.py:_stop():287] _stop: stdout
+2021-02-23 11:20:45,022 INFO    stdout    :1550008 [redirect.py:_pipe_relay():129] relay done saw last write: stdout
+2021-02-23 11:20:45,022 INFO    MainThread:1550008 [redirect.py:_stop():293] _stop closed: stdout
+2021-02-23 11:20:45,022 INFO    stdout    :1550008 [redirect.py:_pipe_relay():145] relay done done: stdout
+2021-02-23 11:20:45,022 INFO    MainThread:1550008 [redirect.py:_stop():299] _stop joined: stdout
+2021-02-23 11:20:45,022 INFO    MainThread:1550008 [redirect.py:_stop():301] _stop rd closed: stdout
+2021-02-23 11:20:45,022 INFO    MainThread:1550008 [redirect.py:uninstall():236] uninstall done
+2021-02-23 11:20:45,022 INFO    MainThread:1550008 [redirect.py:uninstall():232] uninstall start
+2021-02-23 11:20:45,022 INFO    MainThread:1550008 [redirect.py:_stop():287] _stop: stderr
+2021-02-23 11:20:45,022 INFO    stderr    :1550008 [redirect.py:_pipe_relay():129] relay done saw last write: stderr
+2021-02-23 11:20:45,023 INFO    MainThread:1550008 [redirect.py:_stop():293] _stop closed: stderr
+2021-02-23 11:20:45,023 INFO    stderr    :1550008 [redirect.py:_pipe_relay():145] relay done done: stderr
+2021-02-23 11:20:45,023 INFO    MainThread:1550008 [redirect.py:_stop():299] _stop joined: stderr
+2021-02-23 11:20:45,023 INFO    MainThread:1550008 [redirect.py:_stop():301] _stop rd closed: stderr
+2021-02-23 11:20:45,023 INFO    MainThread:1550008 [redirect.py:uninstall():236] uninstall done
+2021-02-23 11:20:45,190 INFO    MainThread:1550008 [wandb_run.py:_wait_for_finish():1504] got exit ret: file_counts {
+  wandb_count: 2
+  other_count: 2
+}
+pusher_stats {
+  uploaded_bytes: 36444
+  total_bytes: 36444
+}
+
+2021-02-23 11:20:51,409 INFO    MainThread:1550008 [wandb_run.py:_wait_for_finish():1504] got exit ret: file_counts {
+  wandb_count: 2
+  other_count: 2
+}
+pusher_stats {
+  uploaded_bytes: 36444
+  total_bytes: 36444
+}
+
+2021-02-23 11:20:53,412 INFO    MainThread:1550008 [wandb_run.py:_wait_for_finish():1504] got exit ret: done: true
+exit_result {
+}
+file_counts {
+  wandb_count: 6
+  other_count: 2
+}
+pusher_stats {
+  uploaded_bytes: 43819
+  total_bytes: 43819
+}
+
+2021-02-23 11:20:54,651 INFO    MainThread:1550008 [wandb_run.py:_show_files():1726] logging synced files
diff --git a/wandb/run-20210223_112037-1rc3eqxy/run-1rc3eqxy.wandb b/wandb/run-20210223_112037-1rc3eqxy/run-1rc3eqxy.wandb
new file mode 100644
index 0000000..0a4c23f
Binary files /dev/null and b/wandb/run-20210223_112037-1rc3eqxy/run-1rc3eqxy.wandb differ
diff --git a/wandb/run-20210223_112242-2uoof2pv/files/code/train_uniter.py b/wandb/run-20210223_112242-2uoof2pv/files/code/train_uniter.py
new file mode 100644
index 0000000..44d832c
--- /dev/null
+++ b/wandb/run-20210223_112242-2uoof2pv/files/code/train_uniter.py
@@ -0,0 +1,654 @@
+import wandb
+import argparse
+import os
+import time
+import datetime
+import shutil
+import random
+import sys
+import os
+import json
+import re
+import numpy as np
+from statistics import mean, stdev
+import torch
+from torch.utils.tensorboard import SummaryWriter
+import torch.nn as nn
+import torch.nn.functional as F
+from sklearn.metrics import classification_report
+from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup
+from collections import defaultdict
+from functools import partial
+from torch.utils import data
+from transformers import BertTokenizer
+
+from utils.metrics import standard_metrics, find_optimal_threshold
+from utils.optim_utils import get_optimizer
+from utils.utils import calc_elapsed_time, print_stats, print_test_stats, log_tensorboard, set_seed, get_device, get_gather_index, get_attention_mask
+from utils.save import ModelSaver
+from model.meme_uniter import MemeUniter
+from model.pretrain import UniterForPretraining
+from utils.logger import LOGGER
+from data.meme_dataset import MemeDataset
+from model.model import UniterModel, UniterConfig
+from utils.const import IMG_DIM, IMG_LABEL_DIM
+
+
+class TrainerUniter():
+
+    def __init__(self, config):
+        self.preds_list, self.probs_list, self.labels_list, self.loss_list, self.short_loss_list, self.id_list = [], [], [], [], [], []
+        self.best_val_metrics, self.train_metrics = defaultdict(int), {}
+        self.best_auc = 0
+        self.not_improved = 0
+        self.best_val_loss = 1000
+        self.total_iters = 0
+        self.terminate_training = False
+        self.model_file = os.path.join(
+            config['model_path'], config['model_save_name'])
+        self.pretrained_model_file = None
+        if config['pretrained_model_file'] is not None:
+            self.pretrained_model_file = os.path.join(
+                config['model_path'], config['pretrained_model_file'])
+        self.start_epoch = 1
+        self.config = config
+        self.device = get_device()
+
+        if not isinstance(self.config['test_loader'], list):
+            self.config['test_loader'] = [self.config['test_loader']]
+
+        # Initialize the model, optimizer and loss function
+        self.init_training_params()
+
+    def init_training_params(self):
+        self.init_model()
+        wandb.watch(self.model)
+        self.model_saver = ModelSaver(self.model_file)
+
+        self.init_optimizer()
+        self.init_scheduler()
+
+        if self.config['loss_func'] == 'bce_logits':
+            self.criterion = nn.BCEWithLogitsLoss(
+                pos_weight=torch.tensor([self.config['pos_wt']]).to(self.device))
+        elif self.config['loss_func'] == 'bce':
+            self.criterion = nn.BCELoss()
+        else:
+            self.criterion = nn.CrossEntropyLoss()
+
+    def init_scheduler(self):
+        if self.config['scheduler'] == 'step':
+            self.scheduler = torch.optim.lr_scheduler.StepLR(
+                self.optimizer, step_size=self.config['lr_decay_step'], gamma=self.config['lr_decay_factor'])
+        elif self.config['scheduler'] == 'multi_step':
+            self.scheduler = torch.optim.lr_scheduler.MultiStepLR(
+                self.optimizer, milestones=[5, 10, 15, 25, 40], gamma=self.config['lr_decay_factor'])
+        elif self.config['scheduler'] == 'warmup':
+            self.scheduler = get_linear_schedule_with_warmup(self.optimizer, num_warmup_steps=self.config['warmup_steps'],
+                                                             num_training_steps=len(self.config['train_loader']) * self.config['max_epoch'])
+        elif self.config['scheduler'] == 'warmup_cosine':
+            self.scheduler = get_cosine_schedule_with_warmup(self.optimizer, num_warmup_steps=self.config['warmup_steps'],
+                                                             num_training_steps=len(self.config['train_loader']) * self.config['max_epoch'])
+
+    def init_optimizer(self):
+        self.optimizer = get_optimizer(self.model, self.config)
+
+    def init_model(self):
+        # pretrained model file is the original pretrained model - load and use this to fine-tune.
+        # If this argument is False, it will load the model file saved by you after fine-tuning
+        if self.pretrained_model_file:
+            checkpoint = torch.load(self.pretrained_model_file)
+            LOGGER.info('Using pretrained UNITER base model {}'.format(
+                self.pretrained_model_file))
+            base_model = UniterForPretraining.from_pretrained(self.config['config'],
+                                                              state_dict=checkpoint['model_state_dict'],
+                                                              img_dim=IMG_DIM,
+                                                              img_label_dim=IMG_LABEL_DIM)
+            self.model = MemeUniter(uniter_model=base_model.uniter,
+                                    hidden_size=base_model.uniter.config.hidden_size,
+                                    n_classes=self.config['n_classes'])
+        else:
+            self.load_model()
+        print("MemeUniter")
+        print(self.model)
+
+    def load_model(self):
+        # Load pretrained model
+        if self.model_file:
+            checkpoint = torch.load(self.model_file)
+            LOGGER.info('Using UNITER model {}'.format(self.model_file))
+        else:
+            checkpoint = {}
+
+        uniter_config = UniterConfig.from_json_file(self.config['config'])
+        uniter_model = UniterModel(uniter_config, img_dim=IMG_DIM)
+
+        self.model = MemeUniter(uniter_model=uniter_model,
+                                hidden_size=uniter_model.config.hidden_size,
+                                n_classes=self.config['n_classes'])
+        self.model.load_state_dict(checkpoint['model_state_dict'])
+
+    def average_gradients(self, steps):
+        # Used when grad_accumulation > 1
+        for param in self.model.parameters():
+            if param.requires_grad and param.grad is not None:
+                param.grad = param.grad / steps
+
+    def calculate_loss(self, preds, batch_label, grad_step):
+        if self.config['loss_func'] == 'bce':
+            preds = torch.sigmoid(preds)
+        preds = preds.squeeze(1).to(
+            self.device) if self.config['loss_func'] == 'bce_logits' else preds.to(self.device)
+        loss = self.criterion(preds, batch_label.to(
+            self.device) if self.config['loss_func'] == 'ce' else batch_label.float().to(self.device))
+
+        if grad_step and self.iters % self.config['gradient_accumulation'] == 0:
+            loss.backward()
+            self.average_gradients(steps=self.config['gradient_accumulation'])
+            torch.nn.utils.clip_grad_norm_(
+                self.model.parameters(), self.config['max_grad_norm'])
+            self.optimizer.step()
+            self.scheduler.step()
+            self.optimizer.zero_grad()
+        elif grad_step:
+            loss.backward()
+
+        if self.config['loss_func'] == 'bce':
+            probs = preds
+            preds = (preds > 0.5).type(torch.FloatTensor)
+        elif self.config['loss_func'] == 'ce':
+            probs = F.softmax(preds, dim=1)
+            preds = torch.argmax(probs, dim=1)
+        elif self.config['loss_func'] == 'bce_logits':
+            probs = torch.sigmoid(preds)
+            preds = (probs > 0.5).type(torch.FloatTensor)
+
+        self.probs_list.append(probs.cpu().detach().numpy())
+        self.preds_list.append(preds.cpu().detach().numpy())
+        self.labels_list.append(batch_label.cpu().detach().numpy())
+        self.loss_list.append(loss.detach().item())
+        if grad_step:
+            self.short_loss_list.append(loss.detach().item())
+
+    def eval_model(self, test=False, test_idx=0):
+        self.model.eval()
+        self.preds_list, self.probs_list, self.labels_list, self.loss_list, self.id_list = [], [], [], [], []
+        batch_loader = self.config['val_loader'] if not test else self.config['test_loader'][test_idx]
+        with torch.no_grad():
+            for iters, batch in enumerate(batch_loader):
+                batch = self.batch_to_device(batch)
+                if batch_loader.dataset.return_ids:
+                    self.id_list.append(batch['ids'])
+                self.eval_iter_step(iters, batch, test=test)
+
+            self.probs_list = [
+                prob for batch_prob in self.probs_list for prob in batch_prob]
+            self.preds_list = [
+                pred for batch_pred in self.preds_list for pred in batch_pred]
+            self.labels_list = [
+                label for batch_labels in self.labels_list for label in batch_labels]
+            self.id_list = [
+                data_id for batch_id in self.id_list for data_id in batch_id]
+
+            val_loss = sum(self.loss_list)/len(self.loss_list)
+            eval_metrics = standard_metrics(torch.tensor(
+                self.probs_list), torch.tensor(self.labels_list), add_optimal_acc=True)
+            # if test:
+            # 	print(classification_report(np.array(self.labels_list), np.array(self.preds_list)))
+        return eval_metrics, val_loss
+
+    @torch.no_grad()
+    def export_test_predictions(self, test_idx=0, threshold=0.5):
+        self.model.eval()
+
+        # Step 2: Run model on the test set (no loss!)
+        # Ensure that ids are actually returned
+        assert self.config['test_loader'][test_idx].dataset.return_ids, "Can only export test results if the IDs are returned in the test dataset."
+        test_name = self.config['test_loader'][test_idx].dataset.name
+
+        prob_list = []
+        id_list = []
+        for iters, batch in enumerate(self.config['test_loader'][test_idx]):
+            batch = self.batch_to_device(batch)
+            id_list.append(batch['ids'].cpu())
+            probs = self.test_iter_step(batch)
+            if self.config['loss_func'] == 'bce_logits':
+                probs = torch.sigmoid(probs)
+            prob_list.append(probs.detach().cpu())
+
+        probs = torch.cat(prob_list, dim=0)
+        ids = torch.cat(id_list, dim=0)
+        preds = (probs > threshold).long()
+
+        # Step 3: Export predictions
+        self._export_preds(ids, probs, preds,
+                           file_postfix="_%s_preds.csv" % test_name)
+
+        LOGGER.info("Finished export of test predictions")
+
+    @torch.no_grad()
+    def export_val_predictions(self, test=False, test_idx=0, threshold=0.5):
+        batch_loader = self.config['val_loader'] if not test else self.config['test_loader'][test_idx]
+        test_name = batch_loader.dataset.name
+        LOGGER.info("Exporting %s predictions..." % (test_name))
+        self.model.eval()
+
+        # Step 1: Find the optimal threshold on validation set
+        _, _ = self.eval_model(test=test, test_idx=test_idx)
+        val_probs = torch.tensor(self.probs_list)
+        val_labels = torch.tensor(self.labels_list)
+        if len(self.id_list) != 0:
+            val_ids = torch.tensor(self.id_list)
+        else:
+            val_ids = torch.zeros_like(val_labels)-1
+        val_preds = (val_probs > threshold).long()
+
+        self._export_preds(val_ids, val_probs, val_preds,
+                           labels=val_labels, file_postfix="_%s_preds.csv" % test_name)
+
+        LOGGER.info("Finished export of %s predictions" % test_name)
+
+    def _export_preds(self, ids, probs, preds, labels=None, file_postfix="_preds.csv"):
+        file_string = "id,proba,label%s\n" % (
+            ",gt" if labels is not None else "")
+        for i in range(ids.shape[0]):
+            file_string += "%i,%f,%i" % (ids[i].item(),
+                                         probs[i].item(), preds[i].item())
+            if labels is not None:
+                file_string += ",%i" % labels[i].item()
+            file_string += "\n"
+        filepath = os.path.join(
+            self.config['model_path'], self.config['model_save_name'].rsplit(".", 1)[0] + file_postfix)
+        with open(filepath, "w") as f:
+            f.write(file_string)
+        wandb.save(filepath) #Upload file to wandb
+
+    def check_early_stopping(self):
+        self.this_metric = self.val_loss if self.config[
+            'optimize_for'] == 'loss' else self.val_metrics[self.config['optimize_for']]
+        self.current_best = self.best_val_loss if self.config[
+            'optimize_for'] == 'loss' else self.best_val_metrics[self.config['optimize_for']]
+
+        new_best = self.this_metric < self.current_best if self.config[
+            'optimize_for'] == 'loss' else self.this_metric > self.current_best
+        if new_best:
+            LOGGER.info("New High Score! Saving model...")
+            self.best_val_metrics = self.val_metrics
+            self.best_val_loss = self.val_loss
+            wandb.log({'Best val metrics': self.best_val_metrics,
+                       'Best val loss': self.best_val_loss})
+
+            if not self.config["no_model_checkpoints"]:
+                self.model_saver.save(self.model)
+
+        ### Stopping Criteria based on patience and change-in-metric-threshold ###
+        diff = self.current_best - \
+            self.this_metric if self.config['optimize_for'] == 'loss' else self.this_metric - \
+            self.current_best
+        if diff < self.config['early_stop_thresh']:
+            self.not_improved += 1
+            if self.not_improved >= self.config['patience']:
+                self.terminate_training = True
+        else:
+            self.not_improved = 0
+        LOGGER.info("current patience: {}".format(self.not_improved))
+
+    def train_epoch_step(self):
+        self.model.train()
+        lr = self.scheduler.get_last_lr()
+        self.total_iters += self.iters + 1
+        self.probs_list = [
+            pred for batch_pred in self.probs_list for pred in batch_pred]
+        self.labels_list = [
+            label for batch_labels in self.labels_list for label in batch_labels]
+
+        # Evaluate on train set
+        self.train_metrics = standard_metrics(torch.tensor(
+            self.probs_list), torch.tensor(self.labels_list), add_optimal_acc=True)
+        log_tensorboard(self.config, self.config['writer'], self.model, self.epoch, self.iters, self.total_iters,
+                        self.loss_list, self.train_metrics, lr[0], loss_only=False, val=False)
+        self.train_loss = self.loss_list[:]
+
+        # Evaluate on dev set
+        val_time = time.time()
+        self.val_metrics, self.val_loss = self.eval_model()
+        self.config['writer'].add_scalar(
+            "Stats/time_validation", time.time() - val_time, self.total_iters)
+
+        # print stats
+        print_stats(self.config, self.epoch, self.train_metrics,
+                    self.train_loss, self.val_metrics, self.val_loss, self.start, lr[0])
+
+        # log validation stats in tensorboard
+        log_tensorboard(self.config, self.config['writer'], self.model, self.epoch, self.iters,
+                        self.total_iters, self.val_loss, self.val_metrics, lr[0], loss_only=False, val=True)
+
+        # Check for early stopping criteria
+        self.check_early_stopping()
+        self.probs_list = []
+        self.preds_list = []
+        self.labels_list = []
+        self.loss_list = []
+        self.id_list = []
+
+        self.train_loss = sum(self.train_loss)/len(self.train_loss)
+        del self.val_metrics
+        del self.val_loss
+
+    def end_training(self):
+        # Termination message
+        print("\n" + "-"*100)
+        if self.terminate_training:
+            LOGGER.info("Training terminated early because the Validation {} did not improve for  {}  epochs" .format(
+                self.config['optimize_for'], self.config['patience']))
+        else:
+            LOGGER.info("Maximum epochs of {} reached. Finished training !!".format(
+                self.config['max_epoch']))
+
+        print_test_stats(self.best_val_metrics, test=False)
+
+        print("-"*50 + "\n\t\tEvaluating on test set\n" + "-"*50)
+        if not self.config["no_model_checkpoints"]:
+            if os.path.isfile(self.model_file):
+                self.load_model()
+                self.model.to(self.device)
+            else:
+                raise ValueError("No Saved model state_dict found for the chosen model...!!! \nAborting evaluation on test set...".format(
+                    self.config['model_name']))
+
+            self.export_val_predictions()  # Runs evaluation, no need to run it again here
+            val_probs = torch.tensor(self.probs_list)
+            val_labels = torch.tensor(self.labels_list)
+            threshold = 0.5  # the default threshelod for binary classification
+            # Uncomment below line if you have implemented this optional feature
+            # threshold = find_optimal_threshold(val_probs, val_labels, metric="accuracy")
+            best_val_metrics = standard_metrics(
+                val_probs, val_labels, threshold=threshold, add_aucroc=False)
+            LOGGER.info("Optimal threshold on validation dataset: %.4f (accuracy=%4.2f%%)" % (
+                threshold, 100.0*best_val_metrics["accuracy"]))
+
+            # Testing is in the standard form not possible, as we do not have any labels (gives an error in standard_metrics)
+            # Instead, we should write out the predictions in the form of the leaderboard
+            self.test_metrics = dict()
+            for test_idx in range(len(self.config['test_loader'])):
+                test_name = self.config['test_loader'][test_idx].dataset.name
+                LOGGER.info("Export and testing on %s..." % test_name)
+                if hasattr(self.config['test_loader'][test_idx].dataset, "data") and \
+                   hasattr(self.config['test_loader'][test_idx].dataset.data, "labels") and \
+                   self.config['test_loader'][test_idx].dataset.data.labels[0] == -1:  # Step 1: Find the optimal threshold on validation set
+                    self.export_test_predictions(
+                        test_idx=test_idx, threshold=threshold)
+                    self.test_metrics[test_name] = dict()
+                else:
+                    test_idx_metrics, _ = self.eval_model(
+                        test=True, test_idx=test_idx)
+                    self.test_metrics[test_name] = test_idx_metrics
+                    print_test_stats(test_idx_metrics, test=True)
+                    self.export_val_predictions(
+                        test=True, test_idx=test_idx, threshold=threshold)
+        else:
+            LOGGER.info(
+                "No model checkpoints were saved. Hence, testing will be skipped.")
+            self.test_metrics = dict()
+
+        self.export_metrics()
+
+        self.config['writer'].close()
+
+        if self.config['remove_checkpoints']:
+            LOGGER.info("Removing checkpoint %s..." % self.model_file)
+            os.remove(self.model_file)
+
+    def export_metrics(self):
+        metric_export_file = os.path.join(
+            self.config['model_path'], self.config['model_save_name'].rsplit(".", 1)[0] + "_metrics.json")
+        metric_dict = {}
+        metric_dict["dev"] = self.best_val_metrics
+        metric_dict["dev"]["loss"] = self.best_val_loss
+        metric_dict["train"] = self.train_metrics
+        metric_dict["train"]["loss"] = sum(self.train_loss)/len(
+            self.train_loss) if isinstance(self.train_loss, list) else self.train_loss
+        if hasattr(self, "test_metrics") and len(self.test_metrics) > 0:
+            metric_dict["test"] = self.test_metrics
+
+        with open(metric_export_file, "w") as f:
+            json.dump(metric_dict, f, indent=4)
+
+    def train_main(self, cache=False):
+        print("\n\n" + "="*100 + "\n\t\t\t\t\t Training Network\n" + "="*100)
+
+        self.start = time.time()
+        print("\nBeginning training at:  {} \n".format(datetime.datetime.now()))
+
+        self.model.to(self.device)
+
+        for self.epoch in range(self.start_epoch, self.config['max_epoch']+1):
+            train_times = []
+            for self.iters, self.batch in enumerate(self.config['train_loader']):
+                self.model.train()
+
+                iter_time = time.time()
+                self.batch = self.batch_to_device(self.batch)
+                self.train_iter_step()
+                train_times.append(time.time() - iter_time)
+
+                # Loss only logging
+                if (self.total_iters+self.iters+1) % self.config['log_every'] == 0:
+                    log_tensorboard(self.config, self.config['writer'], self.model, self.epoch,
+                                    self.iters, self.total_iters, self.short_loss_list, loss_only=True, val=False)
+                    self.config['writer'].add_scalar(
+                        'Stats/time_per_train_iter', mean(train_times), (self.iters+self.total_iters+1))
+                    self.config['writer'].add_scalar(
+                        'Stats/learning_rate', self.scheduler.get_last_lr()[0], (self.iters+self.total_iters+1))
+                    train_times = []
+                    self.short_loss_list = []
+            self.train_epoch_step()
+
+            if self.terminate_training:
+                break
+
+        self.end_training()
+        return self.best_val_metrics, self.test_metrics
+
+    def batch_to_device(self, batch):
+        batch = {k: (v.to(self.device) if isinstance(v, torch.Tensor) else v)
+                 for k, v in batch.items()}
+        return batch
+
+    def eval_iter_step(self, iters, batch, test):
+        # Forward pass
+        preds = self.model(img_feat=batch['img_feat'], img_pos_feat=batch['img_pos_feat'], input_ids=batch['input_ids'],
+                           position_ids=batch['position_ids'], attention_mask=batch['attn_mask'], gather_index=batch['gather_index'],
+                           output_all_encoded_layers=False, gender_race_probs=batch['gender_race_probs'])
+        self.calculate_loss(preds, batch['labels'], grad_step=False)
+
+    def train_iter_step(self):
+        # Forward pass
+        self.preds = self.model(img_feat=self.batch['img_feat'], img_pos_feat=self.batch['img_pos_feat'], input_ids=self.batch['input_ids'],
+                                position_ids=self.batch['position_ids'], attention_mask=self.batch[
+                                    'attn_mask'], gather_index=self.batch['gather_index'],
+                                output_all_encoded_layers=False, gender_race_probs=self.batch['gender_race_probs'])
+        self.calculate_loss(self.preds, self.batch['labels'], grad_step=True)
+
+    def test_iter_step(self, batch):
+        # Forward pass
+        preds = self.model(img_feat=batch['img_feat'], img_pos_feat=batch['img_pos_feat'], input_ids=batch['input_ids'],
+                           position_ids=batch['position_ids'], attention_mask=batch['attn_mask'], gather_index=batch['gather_index'],
+                           output_all_encoded_layers=False, gender_race_probs=batch['gender_race_probs'])
+        return preds.squeeze()
+
+
+if __name__ == '__main__':
+    wandb.init(project="multimodal-nlp2")
+    wandb.tensorboard.patch(root_logdir='./vis_checkpoints',
+                            pytorch=True, tensorboardX=False)
+    parser = argparse.ArgumentParser()
+    defaults = dict()
+
+    # Required Paths
+    parser.add_argument('--data_path', type=str, default='./dataset',
+                        help='path to dataset folder that contains the processed data files')
+    parser.add_argument('--model_path', type=str, default='./model_checkpoints',
+                        help='Directory for saving trained model checkpoints')
+    parser.add_argument('--vis_path', type=str, default='./vis_checkpoints',
+                        help='Directory for saving tensorboard checkpoints')
+    parser.add_argument("--model_save_name", type=str, default='best_model.pt',
+                        help='saved model name')
+    parser.add_argument("--no_model_checkpoints", action="store_true",
+                        help='If selected, no model checkpoints will be created, and no testing performed (for gridsearches etc.)')
+    parser.add_argument("--remove_checkpoints", action="store_true",
+                        help='If selected, model checkpoints will be deleted after finishing testing.')
+    parser.add_argument('--config', type=str, default='./config/uniter-base.json',
+                        help='JSON config file')
+    parser.add_argument('--feature_path', type=str, default='./dataset/img_feats',
+                        help='Path to image features')
+
+    # Load pretrained model
+    parser.add_argument('--pretrained_model_file', type=str,
+                        help='Name of the original pretrained model')
+
+    #### Pre-processing Params ####
+    parser.add_argument('--max_txt_len', type=int, default=60,
+                        help='max number of tokens in text (BERT BPE)')
+    parser.add_argument('--max_bb', type=int, default=100,
+                        help='max number of bounding boxes')
+    parser.add_argument('--min_bb', type=int, default=10,
+                        help='min number of bounding boxes')
+    parser.add_argument('--num_bb', type=int, default=36,
+                        help='static number of bounding boxes')
+
+    #### Training Params ####
+    # Named parameters
+    parser.add_argument('--optimizer', type=str, default=defaults.get('optimizer', 'adam'),
+                        help='Optimizer to use for training: adam / adamx / adamw')
+    parser.add_argument('--loss_func', type=str, default=defaults.get('loss_func', 'bce_logits'),
+                        help='Loss function to use for optimization: bce / bce_logits / ce')
+    parser.add_argument('--optimize_for', type=str, default=defaults.get('optimize_for', 'aucroc'),
+                        help='Optimize for what measure during training and early stopping: loss / F1 / aucroc / accuracy')
+    parser.add_argument('--scheduler', type=str, default=defaults.get('scheduler', 'warmup_cosine'),
+                        help='The type of lr scheduler to use anneal learning rate: step/multi_step/warmup/warmp_cosine')
+
+    # Numerical parameters
+    parser.add_argument('--beta1', type=float, default=defaults.get('beta1', 0.9),
+                        help='beta1 parameter in Adam optimizer')
+    parser.add_argument('--beta2', type=float, default=defaults.get('beta2', 0.999),
+                        help='beta2 parameter in Adam optimizer')
+    parser.add_argument('--batch_size', type=int, default=defaults.get('batch_size', 8),
+                        help='batch size for training')
+    parser.add_argument('--num_workers', type=int, default=defaults.get('num_workers', 0),
+                        help='Number of workers to start per dataset')
+    parser.add_argument('--gradient_accumulation', type=int, default=defaults.get('gradient_accumulation', 1),
+                        help='No. of update steps to accumulate before performing backward pass')
+    parser.add_argument('--max_grad_norm', type=int, default=defaults.get('max_grad_norm', 5),
+                        help='max gradient norm for gradient clipping')
+    parser.add_argument('--pos_wt', type=float, default=defaults.get('pos_wt', 1),
+                        help='Loss reweighting for the positive class to deal with class imbalance')
+    parser.add_argument('--lr', type=float, default=defaults.get('lr', 1e-4),
+                        help='Learning rate for training')
+    parser.add_argument('--warmup_steps', type=int, default=defaults.get('warmup_steps', 50),
+                        help='No. of steps to perform linear lr warmup for')
+    parser.add_argument('--weight_decay', type=float, default=defaults.get('weight_decay', 1e-3),
+                        help='weight decay for optimizer')
+    parser.add_argument('--max_epoch', type=int, default=defaults.get('max_epoch', 20),
+                        help='Max epochs to train for')
+    parser.add_argument('--lr_decay_step', type=float, default=defaults.get('lr_decay_step', 3),
+                        help='No. of epochs after which learning rate should be decreased')
+    parser.add_argument('--lr_decay_factor', type=float, default=defaults.get('lr_decay_factor', 0.8),
+                        help='Decay the learning rate of the optimizer by this multiplicative amount')
+    parser.add_argument('--patience', type=float, default=defaults.get('patience', 5),
+                        help='Patience no. of epochs for early stopping')
+    parser.add_argument('--early_stop_thresh', type=float, default=defaults.get('early_stop_thresh', 1e-3),
+                        help='Patience no. of epochs for early stopping')
+    parser.add_argument('--seed', type=int, default=defaults.get('seed', 42),
+                        help='set seed for reproducability')
+    parser.add_argument('--log_every', type=int, default=defaults.get('log_every', 2000),
+                        help='Log stats in Tensorboard every x iterations (not epochs) of training')
+    parser.add_argument('--fc_dim', type=int, default=64,
+                        help='dimen of FC layer"')
+    parser.add_argument('--dropout', type=float, default=0.2,
+                        help='Standard dropout regularization')
+
+    # New parameters by team
+    parser.add_argument('--filter_text', action='store_true',
+                        help='Filter out bounding boxes around text')
+    parser.add_argument('--no_normalize_img', action='store_false',
+                        help='Normalize images by dividing them by their height and width. Default=True')
+    parser.add_argument('--train_filename', type=str, default='train.jsonl',
+                        help='The name of the trainin json file to load.')
+    parser.add_argument('--upsample_multiplier', type=int, default=0,
+                        help='Multiplier used to increase the amount of confounders in training data')
+    parser.add_argument('--note', type=str, default='',
+                        help='Add a note that can be seen in wandb')
+    args, unparsed = parser.parse_known_args()
+    config = args.__dict__
+    wandb.config.update(config)
+    config['device'] = get_device()
+    config['n_classes'] = 2 if config['loss_func'] == 'ce' else 1
+
+    # Check all provided paths:
+    if not os.path.exists(config['data_path']):
+        raise ValueError("[!] ERROR: Dataset path does not exist")
+    else:
+        LOGGER.info("Data path checked..")
+    if not os.path.exists(config['model_path']):
+        LOGGER.warning("Creating checkpoint path for saved models at:  {}\n".format(
+            config['model_path']))
+        os.makedirs(config['model_path'])
+    else:
+        LOGGER.info("Model save path checked..")
+    if 'config' in config:
+        if not os.path.isfile(config['config']):
+            raise ValueError("[!] ERROR: config JSON path does not exist")
+        else:
+            LOGGER.info("config JSON path checked..")
+    if not os.path.exists(config['vis_path']):
+        LOGGER.warning("Creating checkpoint path for Tensorboard visualizations at:  {}\n".format(
+            config['vis_path']))
+        os.makedirs(config['vis_path'])
+    else:
+        LOGGER.info("Tensorboard Visualization path checked..")
+        LOGGER.info(
+            "Cleaning Visualization path of older tensorboard files...\n")
+        # shutil.rmtree(config['vis_path'])
+
+    # Print args
+    print("\n" + "x"*50 + "\n\nRunning training with the following parameters: \n")
+    for key, value in config.items():
+        if not key.endswith('transf'):
+            print(key + ' : ' + str(value))
+    print("\n" + "x"*50)
+
+    config['writer'] = SummaryWriter(config['vis_path'])
+
+    set_seed(config['seed'])
+
+    # Tokenize
+    tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
+    tokenizer_func = partial(tokenizer, max_length=config['max_txt_len'], padding='max_length',
+                             truncation=True, return_tensors='pt', return_length=True)
+
+    # Prepare the datasets and dataloaders for training and evaluation
+    train_dataset = MemeDataset(filepath=os.path.join(config['data_path'], config['train_filename']),
+                                feature_dir=config['feature_path'], text_padding=tokenizer_func, filter_text=config["filter_text"],
+                                upsample_multiplier=config["upsample_multiplier"])
+    val_dataset = MemeDataset(filepath=os.path.join(config['data_path'], 'dev_seen.jsonl'),
+                              feature_dir=config['feature_path'], text_padding=tokenizer_func, filter_text=config["filter_text"])
+    test_dataset = MemeDataset(filepath=os.path.join(config['data_path'], 'test_seen.jsonl'),
+                               feature_dir=config['feature_path'], text_padding=tokenizer_func, filter_text=config["filter_text"])
+
+    config['train_loader'] = data.DataLoader(train_dataset, batch_size=config['batch_size'],
+                                             num_workers=config['num_workers'], collate_fn=train_dataset.get_collate_fn(), shuffle=True, pin_memory=True)
+    config['val_loader'] = data.DataLoader(val_dataset, batch_size=config['batch_size'],
+                                           num_workers=config['num_workers'], collate_fn=val_dataset.get_collate_fn())
+    config['test_loader'] = data.DataLoader(test_dataset, batch_size=config['batch_size'],
+                                            num_workers=config['num_workers'], collate_fn=test_dataset.get_collate_fn())
+
+    try:
+        trainer = TrainerUniter(config)
+        trainer.train_main()
+        wandb.save('vis_checkpoints/*', base_path="vis_checkpoints/")
+        wandb.finish()
+    except KeyboardInterrupt:
+        LOGGER.warning(
+            "Keyboard interrupt by user detected...\nClosing the tensorboard writer!")
+        config['writer'].close()
diff --git a/wandb/run-20210223_112242-2uoof2pv/files/config.yaml b/wandb/run-20210223_112242-2uoof2pv/files/config.yaml
new file mode 100644
index 0000000..bb39cb8
--- /dev/null
+++ b/wandb/run-20210223_112242-2uoof2pv/files/config.yaml
@@ -0,0 +1,149 @@
+wandb_version: 1
+
+_wandb:
+  desc: null
+  value:
+    cli_version: 0.10.19
+    code_path: code/train_uniter.py
+    framework: huggingface
+    huggingface_version: 3.2.0
+    is_jupyter_run: false
+    is_kaggle_kernel: false
+    python_version: 3.7.5
+    t:
+      1:
+      - 1
+      - 3
+      - 5
+      - 11
+      2:
+      - 1
+      - 3
+      - 5
+      - 11
+      4: 3.7.5
+      5: 0.10.19
+      6: 3.2.0
+batch_size:
+  desc: null
+  value: 16
+beta1:
+  desc: null
+  value: 0.9
+beta2:
+  desc: null
+  value: 0.999
+config:
+  desc: null
+  value: config/uniter-base.json
+data_path:
+  desc: null
+  value: ./dataset
+dropout:
+  desc: null
+  value: 0.2
+early_stop_thresh:
+  desc: null
+  value: 0.001
+fc_dim:
+  desc: null
+  value: 64
+feature_path:
+  desc: null
+  value: ./dataset/own_features
+filter_text:
+  desc: null
+  value: false
+gradient_accumulation:
+  desc: null
+  value: 2
+log_every:
+  desc: null
+  value: 2000
+loss_func:
+  desc: null
+  value: bce_logits
+lr:
+  desc: null
+  value: 3.0e-05
+lr_decay_factor:
+  desc: null
+  value: 0.8
+lr_decay_step:
+  desc: null
+  value: 3
+max_bb:
+  desc: null
+  value: 100
+max_epoch:
+  desc: null
+  value: 30
+max_grad_norm:
+  desc: null
+  value: 5
+max_txt_len:
+  desc: null
+  value: 60
+min_bb:
+  desc: null
+  value: 10
+model_path:
+  desc: null
+  value: ./model_checkpoints
+model_save_name:
+  desc: null
+  value: meme.pt
+no_model_checkpoints:
+  desc: null
+  value: false
+no_normalize_img:
+  desc: null
+  value: true
+note:
+  desc: null
+  value: ''
+num_bb:
+  desc: null
+  value: 36
+num_workers:
+  desc: null
+  value: 0
+optimize_for:
+  desc: null
+  value: aucroc
+optimizer:
+  desc: null
+  value: adam
+patience:
+  desc: null
+  value: 5.0
+pos_wt:
+  desc: null
+  value: 1.0
+pretrained_model_file:
+  desc: null
+  value: uniter-base.pt
+remove_checkpoints:
+  desc: null
+  value: false
+scheduler:
+  desc: null
+  value: warmup_cosine
+seed:
+  desc: null
+  value: 43
+train_filename:
+  desc: null
+  value: train.jsonl
+upsample_multiplier:
+  desc: null
+  value: 0
+vis_path:
+  desc: null
+  value: ./vis_checkpoints
+warmup_steps:
+  desc: null
+  value: 500
+weight_decay:
+  desc: null
+  value: 0.001
diff --git a/wandb/run-20210223_112242-2uoof2pv/files/diff.patch b/wandb/run-20210223_112242-2uoof2pv/files/diff.patch
new file mode 100644
index 0000000..9babe22
--- /dev/null
+++ b/wandb/run-20210223_112242-2uoof2pv/files/diff.patch
@@ -0,0 +1,49 @@
+diff --git a/data/meme_dataset.py b/data/meme_dataset.py
+index 3e9ee53..5ad6976 100644
+--- a/data/meme_dataset.py
++++ b/data/meme_dataset.py
+@@ -201,7 +201,7 @@ class MemeDataset(data.Dataset):
+                               for json_dict in f.readlines()]
+         print("Loaded dataset contains ", str(len(self.json_list)), "samples")
+         self._load_dataset()
+-        self._load_gender_race_preds()
++        self._load_gender_race_probs()
+     
+ 
+     def _load_dataset(self):
+@@ -242,7 +242,7 @@ class MemeDataset(data.Dataset):
+             self.data.text = self.text_preprocess(self.data.text)
+ 
+     def _load_gender_race_probs(self):
+-        with open(f'../dataset/gender_race_preds/{self.name}_gender_race_probs.pickle', 'rb') as f:
++        with open(f'../dataset/gender_race_probs/{self.name}_gender_race_probs.pickle', 'rb') as f:
+             self.data.gender_race_probs = pickle.load(f)
+ 
+     def __len__(self):
+diff --git a/wandb/debug-internal.log b/wandb/debug-internal.log
+index f09c7de..667d7e8 120000
+--- a/wandb/debug-internal.log
++++ b/wandb/debug-internal.log
+@@ -1 +1 @@
+-run-20210222_215454-1602o824/logs/debug-internal.log
+\ No newline at end of file
++run-20210223_112242-2uoof2pv/logs/debug-internal.log
+\ No newline at end of file
+diff --git a/wandb/debug.log b/wandb/debug.log
+index 9d13059..c8d309b 120000
+--- a/wandb/debug.log
++++ b/wandb/debug.log
+@@ -1 +1 @@
+-run-20210222_215454-1602o824/logs/debug.log
+\ No newline at end of file
++run-20210223_112242-2uoof2pv/logs/debug.log
+\ No newline at end of file
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 15abb24..cefb9bd 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20210222_215454-1602o824
+\ No newline at end of file
++run-20210223_112242-2uoof2pv
+\ No newline at end of file
diff --git a/wandb/run-20210223_112242-2uoof2pv/files/events.out.tfevents.1614075763.astro.1550511.0 b/wandb/run-20210223_112242-2uoof2pv/files/events.out.tfevents.1614075763.astro.1550511.0
new file mode 120000
index 0000000..6f74d12
--- /dev/null
+++ b/wandb/run-20210223_112242-2uoof2pv/files/events.out.tfevents.1614075763.astro.1550511.0
@@ -0,0 +1 @@
+/home/astro/Documents/UvA/Block 4 - NLP2/Multimodal NLP/Multimodal-NLP/vis_checkpoints/events.out.tfevents.1614075763.astro.1550511.0
\ No newline at end of file
diff --git a/wandb/run-20210223_112242-2uoof2pv/files/output.log b/wandb/run-20210223_112242-2uoof2pv/files/output.log
new file mode 100644
index 0000000..6796a50
--- /dev/null
+++ b/wandb/run-20210223_112242-2uoof2pv/files/output.log
@@ -0,0 +1,68 @@
+23/02/2021 11:22:43 AM : INFO - Data path checked..
+23/02/2021 11:22:43 AM : INFO - Model save path checked..
+23/02/2021 11:22:43 AM : INFO - config JSON path checked..
+23/02/2021 11:22:43 AM : INFO - Tensorboard Visualization path checked..
+23/02/2021 11:22:43 AM : INFO - Cleaning Visualization path of older tensorboard files...
+
+
+xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
+
+Running training with the following parameters: 
+
+data_path : ./dataset
+model_path : ./model_checkpoints
+vis_path : ./vis_checkpoints
+model_save_name : meme.pt
+no_model_checkpoints : False
+remove_checkpoints : False
+config : config/uniter-base.json
+feature_path : ./dataset/own_features
+pretrained_model_file : uniter-base.pt
+max_txt_len : 60
+max_bb : 100
+min_bb : 10
+num_bb : 36
+optimizer : adam
+loss_func : bce_logits
+optimize_for : aucroc
+scheduler : warmup_cosine
+beta1 : 0.9
+beta2 : 0.999
+batch_size : 16
+num_workers : 0
+gradient_accumulation : 2
+max_grad_norm : 5
+pos_wt : 1.0
+lr : 3e-05
+warmup_steps : 500
+weight_decay : 0.001
+max_epoch : 30
+lr_decay_step : 3
+lr_decay_factor : 0.8
+patience : 5.0
+early_stop_thresh : 0.001
+seed : 43
+log_every : 2000
+fc_dim : 64
+dropout : 0.2
+filter_text : False
+no_normalize_img : True
+train_filename : train.jsonl
+upsample_multiplier : 0
+note : 
+device : cuda
+n_classes : 1
+
+xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
+filter text False
+Loaded dataset contains  8500 samples
+Traceback (most recent call last):
+  File "train_uniter.py", line 633, in <module>
+    upsample_multiplier=config["upsample_multiplier"])
+  File "/home/astro/Documents/UvA/Block 4 - NLP2/Multimodal NLP/Multimodal-NLP/data/meme_dataset.py", line 64, in __init__
+    self._prepare_data_list()
+  File "/home/astro/Documents/UvA/Block 4 - NLP2/Multimodal NLP/Multimodal-NLP/data/meme_dataset.py", line 204, in _prepare_data_list
+    self._load_gender_race_probs()
+  File "/home/astro/Documents/UvA/Block 4 - NLP2/Multimodal NLP/Multimodal-NLP/data/meme_dataset.py", line 245, in _load_gender_race_probs
+    with open(f'../dataset/gender_race_probs/{self.name}_gender_race_probs.pickle', 'rb') as f:
+FileNotFoundError: [Errno 2] No such file or directory: '../dataset/gender_race_probs/train_gender_race_probs.pickle'
diff --git a/wandb/run-20210223_112242-2uoof2pv/files/requirements.txt b/wandb/run-20210223_112242-2uoof2pv/files/requirements.txt
new file mode 100644
index 0000000..0d8abf6
--- /dev/null
+++ b/wandb/run-20210223_112242-2uoof2pv/files/requirements.txt
@@ -0,0 +1,176 @@
+absl-py==0.10.0
+aiohttp==3.6.2
+apex==0.1
+argon2-cffi==20.1.0
+astor==0.8.1
+astunparse==1.6.3
+async-generator==1.10
+async-timeout==3.0.1
+attrs==20.2.0
+autopep8==1.5.5
+backcall==0.2.0
+beautifulsoup4==4.9.2
+bleach==3.2.1
+boto3==1.15.6
+botocore==1.18.6
+cached-property==1.5.2
+cachetools==4.1.1
+certifi==2020.6.20
+cffi==1.14.3
+chardet==3.0.4
+click==7.1.2
+configparser==5.0.1
+cycler==0.10.0
+dbfread==2.0.4
+deap==1.3.1
+decorator==4.4.2
+defusedxml==0.6.0
+demjson==2.2.4
+dlib==19.21.1
+docker-pycreds==0.4.0
+editdistance==0.5.3
+entrypoints==0.3
+fasttext==0.9.1
+filelock==3.0.12
+flask==1.1.2
+flatbuffers==1.12
+future==0.18.2
+gast==0.3.3
+gdown==3.12.2
+gitdb==4.0.5
+gitpython==3.1.0
+google-auth-oauthlib==0.4.1
+google-auth==1.22.0
+google-pasta==0.2.0
+gql==2.0.0
+graphql-core==2.3.2
+grpcio==1.32.0
+h5py==2.10.0
+idna==2.10
+ijson==2.6.1
+imagehash==4.2.0
+importlib-metadata==2.0.0
+ipykernel==5.3.4
+ipython-genutils==0.2.0
+ipython==7.20.0
+ipywidgets==7.5.1
+itsdangerous==1.1.0
+jedi==0.17.2
+jinja2==2.11.2
+jmespath==0.10.0
+joblib==0.16.0
+jsonschema==3.2.0
+jupyter-client==6.1.7
+jupyter-core==4.6.3
+jupyterlab-pygments==0.1.2
+keras-applications==1.0.8
+keras-preprocessing==1.1.2
+keras==2.4.3
+kiwisolver==1.2.0
+lightgbm==3.1.1
+lmdb==0.98
+markdown==3.2.2
+markupsafe==1.1.1
+matplotlib==3.3.2
+mdbtools==0.3.14
+meza==0.42.5
+mistune==0.8.4
+mtcnn==0.1.0
+multidict==4.7.6
+nbclient==0.5.0
+nbconvert==6.0.7
+nbformat==5.0.7
+nest-asyncio==1.4.1
+nltk==3.4.5
+notebook==6.1.4
+numpy==1.19.2
+nvidia-ml-py3==7.352.0
+oauthlib==3.1.0
+omegaconf==2.0.1rc4
+opencv-contrib-python==4.5.1.48
+opencv-python==4.5.1.48
+opt-einsum==3.3.0
+packaging==20.4
+pandas==1.1.2
+pandocfilters==1.4.2
+parso==0.7.1
+pathtools==0.1.2
+pexpect==4.8.0
+pickleshare==0.7.5
+pillow==7.2.0
+pip==20.3.3
+prometheus-client==0.8.0
+promise==2.3
+prompt-toolkit==3.0.7
+protobuf==3.13.0
+psutil==5.8.0
+ptyprocess==0.6.0
+pyasn1-modules==0.2.8
+pyasn1==0.4.8
+pybind11==2.6.2
+pycodestyle==2.6.0
+pycparser==2.20
+pygments==2.7.1
+pygogo==0.13.2
+pymongo==3.11.0
+pyparsing==2.4.7
+pyrsistent==0.17.3
+pysocks==1.7.1
+python-dateutil==2.8.1
+python-slugify==1.2.6
+pytz==2020.1
+pywavelets==1.1.1
+pyyaml==5.3.1
+pyzmq==19.0.2
+regex==2020.9.27
+requests-oauthlib==1.3.0
+requests==2.23.0
+rsa==4.6
+rx==1.6.1
+s3transfer==0.3.3
+sacremoses==0.0.43
+scikit-learn==0.23.2
+scipy==1.5.2
+seaborn==0.11.0
+send2trash==1.5.0
+sentencepiece==0.1.91
+sentry-sdk==0.20.3
+setuptools==52.0.0.post20210125
+shortuuid==1.0.1
+six==1.15.0
+sklearn==0.0
+smmap==3.0.4
+soupsieve==2.0.1
+subprocess32==3.5.4
+tensorboard-plugin-wit==1.7.0
+tensorboard==2.4.1
+tensorflow-estimator==2.4.0
+tensorflow==2.4.1
+termcolor==1.1.0
+terminado==0.9.1
+testpath==0.4.4
+threadpoolctl==2.1.0
+tokenizers==0.8.1rc2
+toml==0.10.2
+toolz==0.11.1
+torch==1.6.0+cu101
+torchtext==0.5.0
+torchvision==0.7.0+cu101
+tornado==6.0.4
+tqdm==4.50.0
+traitlets==5.0.5
+transformers==3.2.0
+typing-extensions==3.7.4.3
+unidecode==1.1.1
+urllib3==1.25.10
+wandb==0.10.19
+watchdog==2.0.1
+wcwidth==0.2.5
+webencodings==0.5.1
+werkzeug==1.0.1
+wheel==0.36.2
+widgetsnbextension==3.5.1
+wrapt==1.12.1
+xlrd==1.2.0
+yarl==1.6.0
+zipp==3.2.0
\ No newline at end of file
diff --git a/wandb/run-20210223_112242-2uoof2pv/files/wandb-metadata.json b/wandb/run-20210223_112242-2uoof2pv/files/wandb-metadata.json
new file mode 100644
index 0000000..6d69206
--- /dev/null
+++ b/wandb/run-20210223_112242-2uoof2pv/files/wandb-metadata.json
@@ -0,0 +1,55 @@
+{
+    "os": "Linux-5.8.0-43-generic-x86_64-with-debian-bullseye-sid",
+    "python": "3.7.5",
+    "heartbeatAt": "2021-02-23T10:22:43.666265",
+    "startedAt": "2021-02-23T10:22:42.302971",
+    "docker": null,
+    "gpu": "GeForce RTX 2060 SUPER",
+    "gpu_count": 1,
+    "cpu_count": 24,
+    "cuda": "11.0.228",
+    "args": [
+        "--config",
+        "config/uniter-base.json",
+        "--data_path",
+        "./dataset",
+        "--model_path",
+        "./model_checkpoints",
+        "--pretrained_model_file",
+        "uniter-base.pt",
+        "--feature_path",
+        "./dataset/own_features",
+        "--lr",
+        "3e-5",
+        "--scheduler",
+        "warmup_cosine",
+        "--warmup_steps",
+        "500",
+        "--max_epoch",
+        "30",
+        "--batch_size",
+        "16",
+        "--patience",
+        "5",
+        "--gradient_accumulation",
+        "2",
+        "--model_save_name",
+        "meme.pt",
+        "--seed",
+        "43",
+        "--pos_wt",
+        "1"
+    ],
+    "state": "running",
+    "codePath": "train_uniter.py",
+    "program": "train_uniter.py",
+    "git": {
+        "remote": "https://github.com/Noixas/Multimodal-NLP.git",
+        "commit": "83666a1f2f64375554a1633e19e39a7c7cef964b"
+    },
+    "email": "rodrigo.mulsa@outlook.com",
+    "root": "/home/astro/Documents/UvA/Block 4 - NLP2/Multimodal NLP/Multimodal-NLP",
+    "host": "astro",
+    "username": "astro",
+    "executable": "/home/astro/anaconda3/envs/nlp2-multimodal/bin/python"
+}
diff --git a/wandb/run-20210223_112242-2uoof2pv/files/wandb-summary.json b/wandb/run-20210223_112242-2uoof2pv/files/wandb-summary.json
new file mode 100644
index 0000000..9e26dfe
--- /dev/null
+++ b/wandb/run-20210223_112242-2uoof2pv/files/wandb-summary.json
@@ -0,0 +1 @@
+{}
\ No newline at end of file
diff --git a/wandb/run-20210223_112242-2uoof2pv/logs/debug-internal.log b/wandb/run-20210223_112242-2uoof2pv/logs/debug-internal.log
new file mode 100644
index 0000000..0ef2a21
--- /dev/null
+++ b/wandb/run-20210223_112242-2uoof2pv/logs/debug-internal.log
@@ -0,0 +1,120 @@
+2021-02-23 11:22:42,669 INFO    MainThread:1550572 [internal.py:wandb_internal():91] W&B internal server running at pid: 1550572, started at: 2021-02-23 11:22:42.668971
+2021-02-23 11:22:42,670 DEBUG   HandlerThread:1550572 [handler.py:handle_request():94] handle_request: check_version
+2021-02-23 11:22:42,670 INFO    WriterThread:1550572 [datastore.py:open_for_write():77] open: /home/astro/Documents/UvA/Block 4 - NLP2/Multimodal NLP/Multimodal-NLP/wandb/run-20210223_112242-2uoof2pv/run-2uoof2pv.wandb
+2021-02-23 11:22:42,671 DEBUG   SenderThread:1550572 [sender.py:send():117] send: header
+2021-02-23 11:22:42,671 DEBUG   SenderThread:1550572 [sender.py:send():117] send: request
+2021-02-23 11:22:42,671 DEBUG   SenderThread:1550572 [sender.py:send_request():126] send_request: check_version
+2021-02-23 11:22:42,701 DEBUG   SenderThread:1550572 [sender.py:send():117] send: run
+2021-02-23 11:22:42,904 INFO    SenderThread:1550572 [sender.py:_start_run_threads():596] run started: 2uoof2pv with start time 1614075762
+2021-02-23 11:22:42,904 DEBUG   SenderThread:1550572 [sender.py:send():117] send: summary
+2021-02-23 11:22:42,904 INFO    SenderThread:1550572 [sender.py:_save_file():682] saving file wandb-summary.json with policy end
+2021-02-23 11:22:42,904 DEBUG   HandlerThread:1550572 [handler.py:handle_request():94] handle_request: run_start
+2021-02-23 11:22:43,666 DEBUG   HandlerThread:1550572 [meta.py:__init__():34] meta init
+2021-02-23 11:22:43,666 DEBUG   HandlerThread:1550572 [meta.py:__init__():48] meta init done
+2021-02-23 11:22:43,666 DEBUG   HandlerThread:1550572 [meta.py:probe():190] probe
+2021-02-23 11:22:43,672 DEBUG   HandlerThread:1550572 [meta.py:_setup_git():180] setup git
+2021-02-23 11:22:43,694 DEBUG   HandlerThread:1550572 [meta.py:_setup_git():187] setup git done
+2021-02-23 11:22:43,694 DEBUG   HandlerThread:1550572 [meta.py:_save_code():69] save code
+2021-02-23 11:22:43,704 DEBUG   HandlerThread:1550572 [meta.py:_save_code():90] save code done
+2021-02-23 11:22:43,704 DEBUG   HandlerThread:1550572 [meta.py:_save_patches():107] save patches
+2021-02-23 11:22:43,806 DEBUG   HandlerThread:1550572 [meta.py:_save_patches():149] save patches done
+2021-02-23 11:22:43,806 DEBUG   HandlerThread:1550572 [meta.py:_save_pip():52] save pip
+2021-02-23 11:22:43,807 DEBUG   HandlerThread:1550572 [meta.py:_save_pip():66] save pip done
+2021-02-23 11:22:43,807 DEBUG   HandlerThread:1550572 [meta.py:probe():231] probe done
+2021-02-23 11:22:43,810 DEBUG   SenderThread:1550572 [sender.py:send():117] send: files
+2021-02-23 11:22:43,810 INFO    SenderThread:1550572 [sender.py:_save_file():682] saving file wandb-metadata.json with policy now
+2021-02-23 11:22:43,810 INFO    SenderThread:1550572 [sender.py:_save_file():682] saving file code/train_uniter.py with policy now
+2021-02-23 11:22:43,811 INFO    SenderThread:1550572 [sender.py:_save_file():682] saving file diff.patch with policy now
+2021-02-23 11:22:43,816 DEBUG   HandlerThread:1550572 [handler.py:handle_request():94] handle_request: status
+2021-02-23 11:22:43,816 DEBUG   SenderThread:1550572 [sender.py:send():117] send: request
+2021-02-23 11:22:43,816 DEBUG   SenderThread:1550572 [sender.py:send_request():126] send_request: status
+2021-02-23 11:22:43,823 INFO    HandlerThread:1550572 [handler.py:handle_tbrecord():307] handling tbrecord: tbrecord {
+  log_dir: "./vis_checkpoints"
+  save: true
+  root_dir: "./vis_checkpoints"
+}
+
+2021-02-23 11:22:43,824 DEBUG   HandlerThread:1550572 [config_util.py:dict_from_config_file():99] no default config file found in config-defaults.yaml
+2021-02-23 11:22:43,958 DEBUG   SenderThread:1550572 [sender.py:send():117] send: config
+2021-02-23 11:22:44,124 DEBUG   SenderThread:1550572 [sender.py:send():117] send: tbrecord
+2021-02-23 11:22:44,124 DEBUG   SenderThread:1550572 [sender.py:send():117] send: files
+2021-02-23 11:22:44,124 INFO    SenderThread:1550572 [sender.py:_save_file():682] saving file events.out.tfevents.1614075763.astro.1550511.0 with policy live
+2021-02-23 11:22:47,063 DEBUG   SenderThread:1550572 [sender.py:send():117] send: stats
+2021-02-23 11:22:49,241 DEBUG   SenderThread:1550572 [sender.py:send():117] send: telemetry
+2021-02-23 11:22:49,241 DEBUG   HandlerThread:1550572 [handler.py:handle_request():94] handle_request: poll_exit
+2021-02-23 11:22:49,406 DEBUG   SenderThread:1550572 [sender.py:send():117] send: exit
+2021-02-23 11:22:49,407 INFO    SenderThread:1550572 [sender.py:send_exit():195] handling exit code: 1
+2021-02-23 11:22:49,407 INFO    SenderThread:1550572 [sender.py:send_exit():203] send defer
+2021-02-23 11:22:49,407 DEBUG   SenderThread:1550572 [sender.py:send():117] send: request
+2021-02-23 11:22:49,407 DEBUG   SenderThread:1550572 [sender.py:send_request():126] send_request: poll_exit
+2021-02-23 11:22:49,407 DEBUG   HandlerThread:1550572 [handler.py:handle_request():94] handle_request: defer
+2021-02-23 11:22:49,407 INFO    HandlerThread:1550572 [handler.py:handle_request_defer():108] handle defer: 0
+2021-02-23 11:22:49,408 DEBUG   SenderThread:1550572 [sender.py:send():117] send: request
+2021-02-23 11:22:49,408 DEBUG   SenderThread:1550572 [sender.py:send_request():126] send_request: defer
+2021-02-23 11:22:49,408 INFO    SenderThread:1550572 [sender.py:send_request_defer():212] handle sender defer: 0
+2021-02-23 11:22:49,408 INFO    SenderThread:1550572 [sender.py:send_request_defer():248] send defer: 1
+2021-02-23 11:22:49,408 DEBUG   HandlerThread:1550572 [handler.py:handle_request():94] handle_request: defer
+2021-02-23 11:22:49,408 INFO    HandlerThread:1550572 [handler.py:handle_request_defer():108] handle defer: 1
+2021-02-23 11:22:49,509 DEBUG   SenderThread:1550572 [sender.py:send():117] send: request
+2021-02-23 11:22:49,509 DEBUG   SenderThread:1550572 [sender.py:send_request():126] send_request: defer
+2021-02-23 11:22:49,510 INFO    SenderThread:1550572 [sender.py:send_request_defer():212] handle sender defer: 1
+2021-02-23 11:22:49,510 INFO    SenderThread:1550572 [sender.py:send_request_defer():248] send defer: 2
+2021-02-23 11:22:49,510 DEBUG   SenderThread:1550572 [sender.py:send():117] send: stats
+2021-02-23 11:22:49,510 DEBUG   HandlerThread:1550572 [handler.py:handle_request():94] handle_request: defer
+2021-02-23 11:22:49,510 INFO    HandlerThread:1550572 [handler.py:handle_request_defer():108] handle defer: 2
+2021-02-23 11:22:55,826 DEBUG   HandlerThread:1550572 [handler.py:handle_request():94] handle_request: poll_exit
+2021-02-23 11:22:55,826 DEBUG   SenderThread:1550572 [sender.py:send():117] send: request
+2021-02-23 11:22:55,826 DEBUG   SenderThread:1550572 [sender.py:send_request():126] send_request: defer
+2021-02-23 11:22:55,826 INFO    SenderThread:1550572 [sender.py:send_request_defer():212] handle sender defer: 2
+2021-02-23 11:22:55,826 INFO    SenderThread:1550572 [sender.py:send_request_defer():248] send defer: 3
+2021-02-23 11:22:55,827 DEBUG   SenderThread:1550572 [sender.py:send():117] send: request
+2021-02-23 11:22:55,827 DEBUG   SenderThread:1550572 [sender.py:send_request():126] send_request: poll_exit
+2021-02-23 11:22:55,827 DEBUG   HandlerThread:1550572 [handler.py:handle_request():94] handle_request: defer
+2021-02-23 11:22:55,827 INFO    HandlerThread:1550572 [handler.py:handle_request_defer():108] handle defer: 3
+2021-02-23 11:22:55,827 DEBUG   SenderThread:1550572 [sender.py:send():117] send: summary
+2021-02-23 11:22:55,829 INFO    SenderThread:1550572 [sender.py:_save_file():682] saving file wandb-summary.json with policy end
+2021-02-23 11:22:55,829 DEBUG   SenderThread:1550572 [sender.py:send():117] send: request
+2021-02-23 11:22:55,829 DEBUG   SenderThread:1550572 [sender.py:send_request():126] send_request: defer
+2021-02-23 11:22:55,829 INFO    SenderThread:1550572 [sender.py:send_request_defer():212] handle sender defer: 3
+2021-02-23 11:22:55,829 INFO    SenderThread:1550572 [sender.py:send_request_defer():248] send defer: 4
+2021-02-23 11:22:55,830 DEBUG   HandlerThread:1550572 [handler.py:handle_request():94] handle_request: defer
+2021-02-23 11:22:55,830 INFO    HandlerThread:1550572 [handler.py:handle_request_defer():108] handle defer: 4
+2021-02-23 11:22:55,830 DEBUG   SenderThread:1550572 [sender.py:send():117] send: request
+2021-02-23 11:22:55,830 DEBUG   SenderThread:1550572 [sender.py:send_request():126] send_request: defer
+2021-02-23 11:22:55,830 INFO    SenderThread:1550572 [sender.py:send_request_defer():212] handle sender defer: 4
+2021-02-23 11:22:55,918 INFO    SenderThread:1550572 [sender.py:send_request_defer():248] send defer: 5
+2021-02-23 11:22:55,922 DEBUG   HandlerThread:1550572 [handler.py:handle_request():94] handle_request: defer
+2021-02-23 11:22:55,922 INFO    HandlerThread:1550572 [handler.py:handle_request_defer():108] handle defer: 5
+2021-02-23 11:22:55,922 DEBUG   SenderThread:1550572 [sender.py:send():117] send: request
+2021-02-23 11:22:55,922 DEBUG   SenderThread:1550572 [sender.py:send_request():126] send_request: defer
+2021-02-23 11:22:55,922 INFO    SenderThread:1550572 [sender.py:send_request_defer():212] handle sender defer: 5
+2021-02-23 11:22:55,922 INFO    SenderThread:1550572 [sender.py:send_request_defer():248] send defer: 6
+2021-02-23 11:22:55,922 DEBUG   HandlerThread:1550572 [handler.py:handle_request():94] handle_request: defer
+2021-02-23 11:22:55,923 INFO    HandlerThread:1550572 [handler.py:handle_request_defer():108] handle defer: 6
+2021-02-23 11:22:55,923 DEBUG   SenderThread:1550572 [sender.py:send():117] send: request
+2021-02-23 11:22:55,923 DEBUG   SenderThread:1550572 [sender.py:send_request():126] send_request: defer
+2021-02-23 11:22:55,923 INFO    SenderThread:1550572 [sender.py:send_request_defer():212] handle sender defer: 6
+2021-02-23 11:22:56,083 INFO    SenderThread:1550572 [sender.py:send_request_defer():248] send defer: 7
+2021-02-23 11:22:56,083 DEBUG   HandlerThread:1550572 [handler.py:handle_request():94] handle_request: defer
+2021-02-23 11:22:56,083 INFO    HandlerThread:1550572 [handler.py:handle_request_defer():108] handle defer: 7
+2021-02-23 11:22:56,083 DEBUG   SenderThread:1550572 [sender.py:send():117] send: request
+2021-02-23 11:22:56,083 DEBUG   SenderThread:1550572 [sender.py:send_request():126] send_request: defer
+2021-02-23 11:22:56,083 INFO    SenderThread:1550572 [sender.py:send_request_defer():212] handle sender defer: 7
+2021-02-23 11:22:56,084 INFO    SenderThread:1550572 [sender.py:send_request_defer():248] send defer: 8
+2021-02-23 11:22:56,084 DEBUG   SenderThread:1550572 [sender.py:send():117] send: final
+2021-02-23 11:22:56,084 DEBUG   SenderThread:1550572 [sender.py:send():117] send: footer
+2021-02-23 11:22:56,085 DEBUG   HandlerThread:1550572 [handler.py:handle_request():94] handle_request: defer
+2021-02-23 11:22:56,085 INFO    HandlerThread:1550572 [handler.py:handle_request_defer():108] handle defer: 8
+2021-02-23 11:22:56,085 DEBUG   SenderThread:1550572 [sender.py:send():117] send: request
+2021-02-23 11:22:56,085 DEBUG   SenderThread:1550572 [sender.py:send_request():126] send_request: defer
+2021-02-23 11:22:56,085 INFO    SenderThread:1550572 [sender.py:send_request_defer():212] handle sender defer: 8
+2021-02-23 11:22:57,830 DEBUG   HandlerThread:1550572 [handler.py:handle_request():94] handle_request: poll_exit
+2021-02-23 11:22:57,830 DEBUG   SenderThread:1550572 [sender.py:send():117] send: request
+2021-02-23 11:22:57,830 DEBUG   SenderThread:1550572 [sender.py:send_request():126] send_request: poll_exit
+2021-02-23 11:22:57,831 DEBUG   HandlerThread:1550572 [handler.py:handle_request():94] handle_request: get_summary
+2021-02-23 11:22:57,832 DEBUG   HandlerThread:1550572 [handler.py:handle_request():94] handle_request: sampled_history
+2021-02-23 11:22:57,832 DEBUG   HandlerThread:1550572 [handler.py:handle_request():94] handle_request: shutdown
+2021-02-23 11:22:57,832 INFO    HandlerThread:1550572 [handler.py:finish():333] shutting down handler
+2021-02-23 11:22:58,084 INFO    WriterThread:1550572 [datastore.py:close():258] close: /home/astro/Documents/UvA/Block 4 - NLP2/Multimodal NLP/Multimodal-NLP/wandb/run-20210223_112242-2uoof2pv/run-2uoof2pv.wandb
+2021-02-23 11:22:58,831 INFO    SenderThread:1550572 [sender.py:finish():766] shutting down sender
+2021-02-23 11:22:58,831 INFO    MainThread:1550572 [internal.py:handle_exit():78] Internal process exited
diff --git a/wandb/run-20210223_112242-2uoof2pv/logs/debug.log b/wandb/run-20210223_112242-2uoof2pv/logs/debug.log
new file mode 100644
index 0000000..b4f1945
--- /dev/null
+++ b/wandb/run-20210223_112242-2uoof2pv/logs/debug.log
@@ -0,0 +1,78 @@
+2021-02-23 11:22:42,304 INFO    MainThread:1550511 [wandb_setup.py:_flush():70] setting env: {}
+2021-02-23 11:22:42,304 INFO    MainThread:1550511 [wandb_setup.py:_flush():70] multiprocessing start_methods=fork,spawn,forkserver, using: spawn
+2021-02-23 11:22:42,304 INFO    MainThread:1550511 [wandb_setup.py:_flush():70] setting login settings: {}
+2021-02-23 11:22:42,304 INFO    MainThread:1550511 [wandb_init.py:_log_setup():319] Logging user logs to /home/astro/Documents/UvA/Block 4 - NLP2/Multimodal NLP/Multimodal-NLP/wandb/run-20210223_112242-2uoof2pv/logs/debug.log
+2021-02-23 11:22:42,305 INFO    MainThread:1550511 [wandb_init.py:_log_setup():320] Logging internal logs to /home/astro/Documents/UvA/Block 4 - NLP2/Multimodal NLP/Multimodal-NLP/wandb/run-20210223_112242-2uoof2pv/logs/debug-internal.log
+2021-02-23 11:22:42,305 INFO    MainThread:1550511 [wandb_init.py:init():351] calling init triggers
+2021-02-23 11:22:42,305 INFO    MainThread:1550511 [wandb_init.py:init():358] wandb.init called with sweep_config: {}
+config: {}
+2021-02-23 11:22:42,305 INFO    MainThread:1550511 [wandb_init.py:init():404] starting backend
+2021-02-23 11:22:42,313 INFO    MainThread:1550511 [backend.py:ensure_launched():81] starting backend process...
+2021-02-23 11:22:42,318 INFO    MainThread:1550511 [backend.py:ensure_launched():86] started backend process with pid: 1550572
+2021-02-23 11:22:42,319 INFO    MainThread:1550511 [wandb_init.py:init():413] backend started and connected
+2021-02-23 11:22:42,320 INFO    MainThread:1550511 [wandb_init.py:init():436] updated telemetry
+2021-02-23 11:22:42,320 INFO    MainThread:1550511 [wandb_init.py:init():459] communicating current version
+2021-02-23 11:22:42,700 INFO    MainThread:1550511 [wandb_init.py:init():464] got version response upgrade_message: "wandb version 0.10.20 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
+
+2021-02-23 11:22:42,700 INFO    MainThread:1550511 [wandb_init.py:init():472] communicating run to backend with 30 second timeout
+2021-02-23 11:22:42,904 INFO    MainThread:1550511 [wandb_init.py:init():495] starting run threads in backend
+2021-02-23 11:22:43,813 INFO    MainThread:1550511 [wandb_run.py:_console_start():1411] atexit reg
+2021-02-23 11:22:43,813 INFO    MainThread:1550511 [wandb_run.py:_redirect():1274] redirect: SettingsConsole.REDIRECT
+2021-02-23 11:22:43,813 INFO    MainThread:1550511 [wandb_run.py:_redirect():1279] Redirecting console.
+2021-02-23 11:22:43,813 INFO    MainThread:1550511 [redirect.py:install():213] install start
+2021-02-23 11:22:43,814 INFO    MainThread:1550511 [redirect.py:install():228] install stop
+2021-02-23 11:22:43,814 INFO    MainThread:1550511 [redirect.py:install():213] install start
+2021-02-23 11:22:43,814 INFO    MainThread:1550511 [redirect.py:install():228] install stop
+2021-02-23 11:22:43,814 INFO    MainThread:1550511 [wandb_run.py:_redirect():1325] Redirects installed.
+2021-02-23 11:22:43,814 INFO    MainThread:1550511 [wandb_init.py:init():518] run started, returning control to user process
+2021-02-23 11:22:43,818 INFO    MainThread:1550511 [wandb_run.py:_config_callback():663] config_cb None None {'data_path': './dataset', 'model_path': './model_checkpoints', 'vis_path': './vis_checkpoints', 'model_save_name': 'meme.pt', 'no_model_checkpoints': False, 'remove_checkpoints': False, 'config': 'config/uniter-base.json', 'feature_path': './dataset/own_features', 'pretrained_model_file': 'uniter-base.pt', 'max_txt_len': 60, 'max_bb': 100, 'min_bb': 10, 'num_bb': 36, 'optimizer': 'adam', 'loss_func': 'bce_logits', 'optimize_for': 'aucroc', 'scheduler': 'warmup_cosine', 'beta1': 0.9, 'beta2': 0.999, 'batch_size': 16, 'num_workers': 0, 'gradient_accumulation': 2, 'max_grad_norm': 5, 'pos_wt': 1.0, 'lr': 3e-05, 'warmup_steps': 500, 'weight_decay': 0.001, 'max_epoch': 30, 'lr_decay_step': 3, 'lr_decay_factor': 0.8, 'patience': 5.0, 'early_stop_thresh': 0.001, 'seed': 43, 'log_every': 2000, 'fc_dim': 64, 'dropout': 0.2, 'filter_text': False, 'no_normalize_img': True, 'train_filename': 'train.jsonl', 'upsample_multiplier': 0, 'note': ''}
+2021-02-23 11:22:43,820 INFO    MainThread:1550511 [wandb_run.py:_tensorboard_callback():734] tensorboard callback: ./vis_checkpoints, None
+2021-02-23 11:22:49,239 INFO    MainThread:1550511 [wandb_run.py:_atexit_cleanup():1381] got exitcode: 1
+2021-02-23 11:22:49,239 INFO    MainThread:1550511 [wandb_run.py:_restore():1353] restore
+2021-02-23 11:22:49,239 INFO    MainThread:1550511 [redirect.py:uninstall():232] uninstall start
+2021-02-23 11:22:49,239 INFO    MainThread:1550511 [redirect.py:_stop():287] _stop: stdout
+2021-02-23 11:22:49,239 INFO    stdout    :1550511 [redirect.py:_pipe_relay():129] relay done saw last write: stdout
+2021-02-23 11:22:49,239 INFO    MainThread:1550511 [redirect.py:_stop():293] _stop closed: stdout
+2021-02-23 11:22:49,240 INFO    stdout    :1550511 [redirect.py:_pipe_relay():145] relay done done: stdout
+2021-02-23 11:22:49,240 INFO    MainThread:1550511 [redirect.py:_stop():299] _stop joined: stdout
+2021-02-23 11:22:49,240 INFO    MainThread:1550511 [redirect.py:_stop():301] _stop rd closed: stdout
+2021-02-23 11:22:49,240 INFO    MainThread:1550511 [redirect.py:uninstall():236] uninstall done
+2021-02-23 11:22:49,240 INFO    MainThread:1550511 [redirect.py:uninstall():232] uninstall start
+2021-02-23 11:22:49,240 INFO    MainThread:1550511 [redirect.py:_stop():287] _stop: stderr
+2021-02-23 11:22:49,240 INFO    MainThread:1550511 [redirect.py:_stop():293] _stop closed: stderr
+2021-02-23 11:22:49,241 INFO    stderr    :1550511 [redirect.py:_pipe_relay():129] relay done saw last write: stderr
+2021-02-23 11:22:49,241 INFO    stderr    :1550511 [redirect.py:_pipe_relay():145] relay done done: stderr
+2021-02-23 11:22:49,241 INFO    MainThread:1550511 [redirect.py:_stop():299] _stop joined: stderr
+2021-02-23 11:22:49,241 INFO    MainThread:1550511 [redirect.py:_stop():301] _stop rd closed: stderr
+2021-02-23 11:22:49,241 INFO    MainThread:1550511 [redirect.py:uninstall():236] uninstall done
+2021-02-23 11:22:49,407 INFO    MainThread:1550511 [wandb_run.py:_wait_for_finish():1504] got exit ret: file_counts {
+  wandb_count: 2
+  other_count: 2
+}
+pusher_stats {
+  uploaded_bytes: 36892
+  total_bytes: 36892
+}
+
+2021-02-23 11:22:55,827 INFO    MainThread:1550511 [wandb_run.py:_wait_for_finish():1504] got exit ret: file_counts {
+  wandb_count: 2
+  other_count: 2
+}
+pusher_stats {
+  uploaded_bytes: 36892
+  total_bytes: 36892
+}
+
+2021-02-23 11:22:57,831 INFO    MainThread:1550511 [wandb_run.py:_wait_for_finish():1504] got exit ret: done: true
+exit_result {
+}
+file_counts {
+  wandb_count: 6
+  other_count: 2
+}
+pusher_stats {
+  uploaded_bytes: 44267
+  total_bytes: 44267
+}
+
+2021-02-23 11:22:59,072 INFO    MainThread:1550511 [wandb_run.py:_show_files():1726] logging synced files
diff --git a/wandb/run-20210223_112242-2uoof2pv/run-2uoof2pv.wandb b/wandb/run-20210223_112242-2uoof2pv/run-2uoof2pv.wandb
new file mode 100644
index 0000000..71f7c62
Binary files /dev/null and b/wandb/run-20210223_112242-2uoof2pv/run-2uoof2pv.wandb differ
diff --git a/wandb/run-20210223_112430-3c31apdq/files/code/train_uniter.py b/wandb/run-20210223_112430-3c31apdq/files/code/train_uniter.py
new file mode 100644
index 0000000..44d832c
--- /dev/null
+++ b/wandb/run-20210223_112430-3c31apdq/files/code/train_uniter.py
@@ -0,0 +1,654 @@
+import wandb
+import argparse
+import os
+import time
+import datetime
+import shutil
+import random
+import sys
+import os
+import json
+import re
+import numpy as np
+from statistics import mean, stdev
+import torch
+from torch.utils.tensorboard import SummaryWriter
+import torch.nn as nn
+import torch.nn.functional as F
+from sklearn.metrics import classification_report
+from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup
+from collections import defaultdict
+from functools import partial
+from torch.utils import data
+from transformers import BertTokenizer
+
+from utils.metrics import standard_metrics, find_optimal_threshold
+from utils.optim_utils import get_optimizer
+from utils.utils import calc_elapsed_time, print_stats, print_test_stats, log_tensorboard, set_seed, get_device, get_gather_index, get_attention_mask
+from utils.save import ModelSaver
+from model.meme_uniter import MemeUniter
+from model.pretrain import UniterForPretraining
+from utils.logger import LOGGER
+from data.meme_dataset import MemeDataset
+from model.model import UniterModel, UniterConfig
+from utils.const import IMG_DIM, IMG_LABEL_DIM
+
+
+class TrainerUniter():
+
+    def __init__(self, config):
+        self.preds_list, self.probs_list, self.labels_list, self.loss_list, self.short_loss_list, self.id_list = [], [], [], [], [], []
+        self.best_val_metrics, self.train_metrics = defaultdict(int), {}
+        self.best_auc = 0
+        self.not_improved = 0
+        self.best_val_loss = 1000
+        self.total_iters = 0
+        self.terminate_training = False
+        self.model_file = os.path.join(
+            config['model_path'], config['model_save_name'])
+        self.pretrained_model_file = None
+        if config['pretrained_model_file'] is not None:
+            self.pretrained_model_file = os.path.join(
+                config['model_path'], config['pretrained_model_file'])
+        self.start_epoch = 1
+        self.config = config
+        self.device = get_device()
+
+        if not isinstance(self.config['test_loader'], list):
+            self.config['test_loader'] = [self.config['test_loader']]
+
+        # Initialize the model, optimizer and loss function
+        self.init_training_params()
+
+    def init_training_params(self):
+        self.init_model()
+        wandb.watch(self.model)
+        self.model_saver = ModelSaver(self.model_file)
+
+        self.init_optimizer()
+        self.init_scheduler()
+
+        if self.config['loss_func'] == 'bce_logits':
+            self.criterion = nn.BCEWithLogitsLoss(
+                pos_weight=torch.tensor([self.config['pos_wt']]).to(self.device))
+        elif self.config['loss_func'] == 'bce':
+            self.criterion = nn.BCELoss()
+        else:
+            self.criterion = nn.CrossEntropyLoss()
+
+    def init_scheduler(self):
+        if self.config['scheduler'] == 'step':
+            self.scheduler = torch.optim.lr_scheduler.StepLR(
+                self.optimizer, step_size=self.config['lr_decay_step'], gamma=self.config['lr_decay_factor'])
+        elif self.config['scheduler'] == 'multi_step':
+            self.scheduler = torch.optim.lr_scheduler.MultiStepLR(
+                self.optimizer, milestones=[5, 10, 15, 25, 40], gamma=self.config['lr_decay_factor'])
+        elif self.config['scheduler'] == 'warmup':
+            self.scheduler = get_linear_schedule_with_warmup(self.optimizer, num_warmup_steps=self.config['warmup_steps'],
+                                                             num_training_steps=len(self.config['train_loader']) * self.config['max_epoch'])
+        elif self.config['scheduler'] == 'warmup_cosine':
+            self.scheduler = get_cosine_schedule_with_warmup(self.optimizer, num_warmup_steps=self.config['warmup_steps'],
+                                                             num_training_steps=len(self.config['train_loader']) * self.config['max_epoch'])
+
+    def init_optimizer(self):
+        self.optimizer = get_optimizer(self.model, self.config)
+
+    def init_model(self):
+        # pretrained model file is the original pretrained model - load and use this to fine-tune.
+        # If this argument is False, it will load the model file saved by you after fine-tuning
+        if self.pretrained_model_file:
+            checkpoint = torch.load(self.pretrained_model_file)
+            LOGGER.info('Using pretrained UNITER base model {}'.format(
+                self.pretrained_model_file))
+            base_model = UniterForPretraining.from_pretrained(self.config['config'],
+                                                              state_dict=checkpoint['model_state_dict'],
+                                                              img_dim=IMG_DIM,
+                                                              img_label_dim=IMG_LABEL_DIM)
+            self.model = MemeUniter(uniter_model=base_model.uniter,
+                                    hidden_size=base_model.uniter.config.hidden_size,
+                                    n_classes=self.config['n_classes'])
+        else:
+            self.load_model()
+        print("MemeUniter")
+        print(self.model)
+
+    def load_model(self):
+        # Load pretrained model
+        if self.model_file:
+            checkpoint = torch.load(self.model_file)
+            LOGGER.info('Using UNITER model {}'.format(self.model_file))
+        else:
+            checkpoint = {}
+
+        uniter_config = UniterConfig.from_json_file(self.config['config'])
+        uniter_model = UniterModel(uniter_config, img_dim=IMG_DIM)
+
+        self.model = MemeUniter(uniter_model=uniter_model,
+                                hidden_size=uniter_model.config.hidden_size,
+                                n_classes=self.config['n_classes'])
+        self.model.load_state_dict(checkpoint['model_state_dict'])
+
+    def average_gradients(self, steps):
+        # Used when grad_accumulation > 1
+        for param in self.model.parameters():
+            if param.requires_grad and param.grad is not None:
+                param.grad = param.grad / steps
+
+    def calculate_loss(self, preds, batch_label, grad_step):
+        if self.config['loss_func'] == 'bce':
+            preds = torch.sigmoid(preds)
+        preds = preds.squeeze(1).to(
+            self.device) if self.config['loss_func'] == 'bce_logits' else preds.to(self.device)
+        loss = self.criterion(preds, batch_label.to(
+            self.device) if self.config['loss_func'] == 'ce' else batch_label.float().to(self.device))
+
+        if grad_step and self.iters % self.config['gradient_accumulation'] == 0:
+            loss.backward()
+            self.average_gradients(steps=self.config['gradient_accumulation'])
+            torch.nn.utils.clip_grad_norm_(
+                self.model.parameters(), self.config['max_grad_norm'])
+            self.optimizer.step()
+            self.scheduler.step()
+            self.optimizer.zero_grad()
+        elif grad_step:
+            loss.backward()
+
+        if self.config['loss_func'] == 'bce':
+            probs = preds
+            preds = (preds > 0.5).type(torch.FloatTensor)
+        elif self.config['loss_func'] == 'ce':
+            probs = F.softmax(preds, dim=1)
+            preds = torch.argmax(probs, dim=1)
+        elif self.config['loss_func'] == 'bce_logits':
+            probs = torch.sigmoid(preds)
+            preds = (probs > 0.5).type(torch.FloatTensor)
+
+        self.probs_list.append(probs.cpu().detach().numpy())
+        self.preds_list.append(preds.cpu().detach().numpy())
+        self.labels_list.append(batch_label.cpu().detach().numpy())
+        self.loss_list.append(loss.detach().item())
+        if grad_step:
+            self.short_loss_list.append(loss.detach().item())
+
+    def eval_model(self, test=False, test_idx=0):
+        self.model.eval()
+        self.preds_list, self.probs_list, self.labels_list, self.loss_list, self.id_list = [], [], [], [], []
+        batch_loader = self.config['val_loader'] if not test else self.config['test_loader'][test_idx]
+        with torch.no_grad():
+            for iters, batch in enumerate(batch_loader):
+                batch = self.batch_to_device(batch)
+                if batch_loader.dataset.return_ids:
+                    self.id_list.append(batch['ids'])
+                self.eval_iter_step(iters, batch, test=test)
+
+            self.probs_list = [
+                prob for batch_prob in self.probs_list for prob in batch_prob]
+            self.preds_list = [
+                pred for batch_pred in self.preds_list for pred in batch_pred]
+            self.labels_list = [
+                label for batch_labels in self.labels_list for label in batch_labels]
+            self.id_list = [
+                data_id for batch_id in self.id_list for data_id in batch_id]
+
+            val_loss = sum(self.loss_list)/len(self.loss_list)
+            eval_metrics = standard_metrics(torch.tensor(
+                self.probs_list), torch.tensor(self.labels_list), add_optimal_acc=True)
+            # if test:
+            # 	print(classification_report(np.array(self.labels_list), np.array(self.preds_list)))
+        return eval_metrics, val_loss
+
+    @torch.no_grad()
+    def export_test_predictions(self, test_idx=0, threshold=0.5):
+        self.model.eval()
+
+        # Step 2: Run model on the test set (no loss!)
+        # Ensure that ids are actually returned
+        assert self.config['test_loader'][test_idx].dataset.return_ids, "Can only export test results if the IDs are returned in the test dataset."
+        test_name = self.config['test_loader'][test_idx].dataset.name
+
+        prob_list = []
+        id_list = []
+        for iters, batch in enumerate(self.config['test_loader'][test_idx]):
+            batch = self.batch_to_device(batch)
+            id_list.append(batch['ids'].cpu())
+            probs = self.test_iter_step(batch)
+            if self.config['loss_func'] == 'bce_logits':
+                probs = torch.sigmoid(probs)
+            prob_list.append(probs.detach().cpu())
+
+        probs = torch.cat(prob_list, dim=0)
+        ids = torch.cat(id_list, dim=0)
+        preds = (probs > threshold).long()
+
+        # Step 3: Export predictions
+        self._export_preds(ids, probs, preds,
+                           file_postfix="_%s_preds.csv" % test_name)
+
+        LOGGER.info("Finished export of test predictions")
+
+    @torch.no_grad()
+    def export_val_predictions(self, test=False, test_idx=0, threshold=0.5):
+        batch_loader = self.config['val_loader'] if not test else self.config['test_loader'][test_idx]
+        test_name = batch_loader.dataset.name
+        LOGGER.info("Exporting %s predictions..." % (test_name))
+        self.model.eval()
+
+        # Step 1: Find the optimal threshold on validation set
+        _, _ = self.eval_model(test=test, test_idx=test_idx)
+        val_probs = torch.tensor(self.probs_list)
+        val_labels = torch.tensor(self.labels_list)
+        if len(self.id_list) != 0:
+            val_ids = torch.tensor(self.id_list)
+        else:
+            val_ids = torch.zeros_like(val_labels)-1
+        val_preds = (val_probs > threshold).long()
+
+        self._export_preds(val_ids, val_probs, val_preds,
+                           labels=val_labels, file_postfix="_%s_preds.csv" % test_name)
+
+        LOGGER.info("Finished export of %s predictions" % test_name)
+
+    def _export_preds(self, ids, probs, preds, labels=None, file_postfix="_preds.csv"):
+        file_string = "id,proba,label%s\n" % (
+            ",gt" if labels is not None else "")
+        for i in range(ids.shape[0]):
+            file_string += "%i,%f,%i" % (ids[i].item(),
+                                         probs[i].item(), preds[i].item())
+            if labels is not None:
+                file_string += ",%i" % labels[i].item()
+            file_string += "\n"
+        filepath = os.path.join(
+            self.config['model_path'], self.config['model_save_name'].rsplit(".", 1)[0] + file_postfix)
+        with open(filepath, "w") as f:
+            f.write(file_string)
+        wandb.save(filepath) #Upload file to wandb
+
+    def check_early_stopping(self):
+        self.this_metric = self.val_loss if self.config[
+            'optimize_for'] == 'loss' else self.val_metrics[self.config['optimize_for']]
+        self.current_best = self.best_val_loss if self.config[
+            'optimize_for'] == 'loss' else self.best_val_metrics[self.config['optimize_for']]
+
+        new_best = self.this_metric < self.current_best if self.config[
+            'optimize_for'] == 'loss' else self.this_metric > self.current_best
+        if new_best:
+            LOGGER.info("New High Score! Saving model...")
+            self.best_val_metrics = self.val_metrics
+            self.best_val_loss = self.val_loss
+            wandb.log({'Best val metrics': self.best_val_metrics,
+                       'Best val loss': self.best_val_loss})
+
+            if not self.config["no_model_checkpoints"]:
+                self.model_saver.save(self.model)
+
+        ### Stopping Criteria based on patience and change-in-metric-threshold ###
+        diff = self.current_best - \
+            self.this_metric if self.config['optimize_for'] == 'loss' else self.this_metric - \
+            self.current_best
+        if diff < self.config['early_stop_thresh']:
+            self.not_improved += 1
+            if self.not_improved >= self.config['patience']:
+                self.terminate_training = True
+        else:
+            self.not_improved = 0
+        LOGGER.info("current patience: {}".format(self.not_improved))
+
+    def train_epoch_step(self):
+        self.model.train()
+        lr = self.scheduler.get_last_lr()
+        self.total_iters += self.iters + 1
+        self.probs_list = [
+            pred for batch_pred in self.probs_list for pred in batch_pred]
+        self.labels_list = [
+            label for batch_labels in self.labels_list for label in batch_labels]
+
+        # Evaluate on train set
+        self.train_metrics = standard_metrics(torch.tensor(
+            self.probs_list), torch.tensor(self.labels_list), add_optimal_acc=True)
+        log_tensorboard(self.config, self.config['writer'], self.model, self.epoch, self.iters, self.total_iters,
+                        self.loss_list, self.train_metrics, lr[0], loss_only=False, val=False)
+        self.train_loss = self.loss_list[:]
+
+        # Evaluate on dev set
+        val_time = time.time()
+        self.val_metrics, self.val_loss = self.eval_model()
+        self.config['writer'].add_scalar(
+            "Stats/time_validation", time.time() - val_time, self.total_iters)
+
+        # print stats
+        print_stats(self.config, self.epoch, self.train_metrics,
+                    self.train_loss, self.val_metrics, self.val_loss, self.start, lr[0])
+
+        # log validation stats in tensorboard
+        log_tensorboard(self.config, self.config['writer'], self.model, self.epoch, self.iters,
+                        self.total_iters, self.val_loss, self.val_metrics, lr[0], loss_only=False, val=True)
+
+        # Check for early stopping criteria
+        self.check_early_stopping()
+        self.probs_list = []
+        self.preds_list = []
+        self.labels_list = []
+        self.loss_list = []
+        self.id_list = []
+
+        self.train_loss = sum(self.train_loss)/len(self.train_loss)
+        del self.val_metrics
+        del self.val_loss
+
+    def end_training(self):
+        # Termination message
+        print("\n" + "-"*100)
+        if self.terminate_training:
+            LOGGER.info("Training terminated early because the Validation {} did not improve for  {}  epochs" .format(
+                self.config['optimize_for'], self.config['patience']))
+        else:
+            LOGGER.info("Maximum epochs of {} reached. Finished training !!".format(
+                self.config['max_epoch']))
+
+        print_test_stats(self.best_val_metrics, test=False)
+
+        print("-"*50 + "\n\t\tEvaluating on test set\n" + "-"*50)
+        if not self.config["no_model_checkpoints"]:
+            if os.path.isfile(self.model_file):
+                self.load_model()
+                self.model.to(self.device)
+            else:
+                raise ValueError("No Saved model state_dict found for the chosen model...!!! \nAborting evaluation on test set...".format(
+                    self.config['model_name']))
+
+            self.export_val_predictions()  # Runs evaluation, no need to run it again here
+            val_probs = torch.tensor(self.probs_list)
+            val_labels = torch.tensor(self.labels_list)
+            threshold = 0.5  # the default threshelod for binary classification
+            # Uncomment below line if you have implemented this optional feature
+            # threshold = find_optimal_threshold(val_probs, val_labels, metric="accuracy")
+            best_val_metrics = standard_metrics(
+                val_probs, val_labels, threshold=threshold, add_aucroc=False)
+            LOGGER.info("Optimal threshold on validation dataset: %.4f (accuracy=%4.2f%%)" % (
+                threshold, 100.0*best_val_metrics["accuracy"]))
+
+            # Testing is in the standard form not possible, as we do not have any labels (gives an error in standard_metrics)
+            # Instead, we should write out the predictions in the form of the leaderboard
+            self.test_metrics = dict()
+            for test_idx in range(len(self.config['test_loader'])):
+                test_name = self.config['test_loader'][test_idx].dataset.name
+                LOGGER.info("Export and testing on %s..." % test_name)
+                if hasattr(self.config['test_loader'][test_idx].dataset, "data") and \
+                   hasattr(self.config['test_loader'][test_idx].dataset.data, "labels") and \
+                   self.config['test_loader'][test_idx].dataset.data.labels[0] == -1:  # Step 1: Find the optimal threshold on validation set
+                    self.export_test_predictions(
+                        test_idx=test_idx, threshold=threshold)
+                    self.test_metrics[test_name] = dict()
+                else:
+                    test_idx_metrics, _ = self.eval_model(
+                        test=True, test_idx=test_idx)
+                    self.test_metrics[test_name] = test_idx_metrics
+                    print_test_stats(test_idx_metrics, test=True)
+                    self.export_val_predictions(
+                        test=True, test_idx=test_idx, threshold=threshold)
+        else:
+            LOGGER.info(
+                "No model checkpoints were saved. Hence, testing will be skipped.")
+            self.test_metrics = dict()
+
+        self.export_metrics()
+
+        self.config['writer'].close()
+
+        if self.config['remove_checkpoints']:
+            LOGGER.info("Removing checkpoint %s..." % self.model_file)
+            os.remove(self.model_file)
+
+    def export_metrics(self):
+        metric_export_file = os.path.join(
+            self.config['model_path'], self.config['model_save_name'].rsplit(".", 1)[0] + "_metrics.json")
+        metric_dict = {}
+        metric_dict["dev"] = self.best_val_metrics
+        metric_dict["dev"]["loss"] = self.best_val_loss
+        metric_dict["train"] = self.train_metrics
+        metric_dict["train"]["loss"] = sum(self.train_loss)/len(
+            self.train_loss) if isinstance(self.train_loss, list) else self.train_loss
+        if hasattr(self, "test_metrics") and len(self.test_metrics) > 0:
+            metric_dict["test"] = self.test_metrics
+
+        with open(metric_export_file, "w") as f:
+            json.dump(metric_dict, f, indent=4)
+
+    def train_main(self, cache=False):
+        print("\n\n" + "="*100 + "\n\t\t\t\t\t Training Network\n" + "="*100)
+
+        self.start = time.time()
+        print("\nBeginning training at:  {} \n".format(datetime.datetime.now()))
+
+        self.model.to(self.device)
+
+        for self.epoch in range(self.start_epoch, self.config['max_epoch']+1):
+            train_times = []
+            for self.iters, self.batch in enumerate(self.config['train_loader']):
+                self.model.train()
+
+                iter_time = time.time()
+                self.batch = self.batch_to_device(self.batch)
+                self.train_iter_step()
+                train_times.append(time.time() - iter_time)
+
+                # Loss only logging
+                if (self.total_iters+self.iters+1) % self.config['log_every'] == 0:
+                    log_tensorboard(self.config, self.config['writer'], self.model, self.epoch,
+                                    self.iters, self.total_iters, self.short_loss_list, loss_only=True, val=False)
+                    self.config['writer'].add_scalar(
+                        'Stats/time_per_train_iter', mean(train_times), (self.iters+self.total_iters+1))
+                    self.config['writer'].add_scalar(
+                        'Stats/learning_rate', self.scheduler.get_last_lr()[0], (self.iters+self.total_iters+1))
+                    train_times = []
+                    self.short_loss_list = []
+            self.train_epoch_step()
+
+            if self.terminate_training:
+                break
+
+        self.end_training()
+        return self.best_val_metrics, self.test_metrics
+
+    def batch_to_device(self, batch):
+        batch = {k: (v.to(self.device) if isinstance(v, torch.Tensor) else v)
+                 for k, v in batch.items()}
+        return batch
+
+    def eval_iter_step(self, iters, batch, test):
+        # Forward pass
+        preds = self.model(img_feat=batch['img_feat'], img_pos_feat=batch['img_pos_feat'], input_ids=batch['input_ids'],
+                           position_ids=batch['position_ids'], attention_mask=batch['attn_mask'], gather_index=batch['gather_index'],
+                           output_all_encoded_layers=False, gender_race_probs=batch['gender_race_probs'])
+        self.calculate_loss(preds, batch['labels'], grad_step=False)
+
+    def train_iter_step(self):
+        # Forward pass
+        self.preds = self.model(img_feat=self.batch['img_feat'], img_pos_feat=self.batch['img_pos_feat'], input_ids=self.batch['input_ids'],
+                                position_ids=self.batch['position_ids'], attention_mask=self.batch[
+                                    'attn_mask'], gather_index=self.batch['gather_index'],
+                                output_all_encoded_layers=False, gender_race_probs=self.batch['gender_race_probs'])
+        self.calculate_loss(self.preds, self.batch['labels'], grad_step=True)
+
+    def test_iter_step(self, batch):
+        # Forward pass
+        preds = self.model(img_feat=batch['img_feat'], img_pos_feat=batch['img_pos_feat'], input_ids=batch['input_ids'],
+                           position_ids=batch['position_ids'], attention_mask=batch['attn_mask'], gather_index=batch['gather_index'],
+                           output_all_encoded_layers=False, gender_race_probs=batch['gender_race_probs'])
+        return preds.squeeze()
+
+
+if __name__ == '__main__':
+    wandb.init(project="multimodal-nlp2")
+    wandb.tensorboard.patch(root_logdir='./vis_checkpoints',
+                            pytorch=True, tensorboardX=False)
+    parser = argparse.ArgumentParser()
+    defaults = dict()
+
+    # Required Paths
+    parser.add_argument('--data_path', type=str, default='./dataset',
+                        help='path to dataset folder that contains the processed data files')
+    parser.add_argument('--model_path', type=str, default='./model_checkpoints',
+                        help='Directory for saving trained model checkpoints')
+    parser.add_argument('--vis_path', type=str, default='./vis_checkpoints',
+                        help='Directory for saving tensorboard checkpoints')
+    parser.add_argument("--model_save_name", type=str, default='best_model.pt',
+                        help='saved model name')
+    parser.add_argument("--no_model_checkpoints", action="store_true",
+                        help='If selected, no model checkpoints will be created, and no testing performed (for gridsearches etc.)')
+    parser.add_argument("--remove_checkpoints", action="store_true",
+                        help='If selected, model checkpoints will be deleted after finishing testing.')
+    parser.add_argument('--config', type=str, default='./config/uniter-base.json',
+                        help='JSON config file')
+    parser.add_argument('--feature_path', type=str, default='./dataset/img_feats',
+                        help='Path to image features')
+
+    # Load pretrained model
+    parser.add_argument('--pretrained_model_file', type=str,
+                        help='Name of the original pretrained model')
+
+    #### Pre-processing Params ####
+    parser.add_argument('--max_txt_len', type=int, default=60,
+                        help='max number of tokens in text (BERT BPE)')
+    parser.add_argument('--max_bb', type=int, default=100,
+                        help='max number of bounding boxes')
+    parser.add_argument('--min_bb', type=int, default=10,
+                        help='min number of bounding boxes')
+    parser.add_argument('--num_bb', type=int, default=36,
+                        help='static number of bounding boxes')
+
+    #### Training Params ####
+    # Named parameters
+    parser.add_argument('--optimizer', type=str, default=defaults.get('optimizer', 'adam'),
+                        help='Optimizer to use for training: adam / adamx / adamw')
+    parser.add_argument('--loss_func', type=str, default=defaults.get('loss_func', 'bce_logits'),
+                        help='Loss function to use for optimization: bce / bce_logits / ce')
+    parser.add_argument('--optimize_for', type=str, default=defaults.get('optimize_for', 'aucroc'),
+                        help='Optimize for what measure during training and early stopping: loss / F1 / aucroc / accuracy')
+    parser.add_argument('--scheduler', type=str, default=defaults.get('scheduler', 'warmup_cosine'),
+                        help='The type of lr scheduler to use anneal learning rate: step/multi_step/warmup/warmp_cosine')
+
+    # Numerical parameters
+    parser.add_argument('--beta1', type=float, default=defaults.get('beta1', 0.9),
+                        help='beta1 parameter in Adam optimizer')
+    parser.add_argument('--beta2', type=float, default=defaults.get('beta2', 0.999),
+                        help='beta2 parameter in Adam optimizer')
+    parser.add_argument('--batch_size', type=int, default=defaults.get('batch_size', 8),
+                        help='batch size for training')
+    parser.add_argument('--num_workers', type=int, default=defaults.get('num_workers', 0),
+                        help='Number of workers to start per dataset')
+    parser.add_argument('--gradient_accumulation', type=int, default=defaults.get('gradient_accumulation', 1),
+                        help='No. of update steps to accumulate before performing backward pass')
+    parser.add_argument('--max_grad_norm', type=int, default=defaults.get('max_grad_norm', 5),
+                        help='max gradient norm for gradient clipping')
+    parser.add_argument('--pos_wt', type=float, default=defaults.get('pos_wt', 1),
+                        help='Loss reweighting for the positive class to deal with class imbalance')
+    parser.add_argument('--lr', type=float, default=defaults.get('lr', 1e-4),
+                        help='Learning rate for training')
+    parser.add_argument('--warmup_steps', type=int, default=defaults.get('warmup_steps', 50),
+                        help='No. of steps to perform linear lr warmup for')
+    parser.add_argument('--weight_decay', type=float, default=defaults.get('weight_decay', 1e-3),
+                        help='weight decay for optimizer')
+    parser.add_argument('--max_epoch', type=int, default=defaults.get('max_epoch', 20),
+                        help='Max epochs to train for')
+    parser.add_argument('--lr_decay_step', type=float, default=defaults.get('lr_decay_step', 3),
+                        help='No. of epochs after which learning rate should be decreased')
+    parser.add_argument('--lr_decay_factor', type=float, default=defaults.get('lr_decay_factor', 0.8),
+                        help='Decay the learning rate of the optimizer by this multiplicative amount')
+    parser.add_argument('--patience', type=float, default=defaults.get('patience', 5),
+                        help='Patience no. of epochs for early stopping')
+    parser.add_argument('--early_stop_thresh', type=float, default=defaults.get('early_stop_thresh', 1e-3),
+                        help='Patience no. of epochs for early stopping')
+    parser.add_argument('--seed', type=int, default=defaults.get('seed', 42),
+                        help='set seed for reproducability')
+    parser.add_argument('--log_every', type=int, default=defaults.get('log_every', 2000),
+                        help='Log stats in Tensorboard every x iterations (not epochs) of training')
+    parser.add_argument('--fc_dim', type=int, default=64,
+                        help='dimen of FC layer"')
+    parser.add_argument('--dropout', type=float, default=0.2,
+                        help='Standard dropout regularization')
+
+    # New parameters by team
+    parser.add_argument('--filter_text', action='store_true',
+                        help='Filter out bounding boxes around text')
+    parser.add_argument('--no_normalize_img', action='store_false',
+                        help='Normalize images by dividing them by their height and width. Default=True')
+    parser.add_argument('--train_filename', type=str, default='train.jsonl',
+                        help='The name of the trainin json file to load.')
+    parser.add_argument('--upsample_multiplier', type=int, default=0,
+                        help='Multiplier used to increase the amount of confounders in training data')
+    parser.add_argument('--note', type=str, default='',
+                        help='Add a note that can be seen in wandb')
+    args, unparsed = parser.parse_known_args()
+    config = args.__dict__
+    wandb.config.update(config)
+    config['device'] = get_device()
+    config['n_classes'] = 2 if config['loss_func'] == 'ce' else 1
+
+    # Check all provided paths:
+    if not os.path.exists(config['data_path']):
+        raise ValueError("[!] ERROR: Dataset path does not exist")
+    else:
+        LOGGER.info("Data path checked..")
+    if not os.path.exists(config['model_path']):
+        LOGGER.warning("Creating checkpoint path for saved models at:  {}\n".format(
+            config['model_path']))
+        os.makedirs(config['model_path'])
+    else:
+        LOGGER.info("Model save path checked..")
+    if 'config' in config:
+        if not os.path.isfile(config['config']):
+            raise ValueError("[!] ERROR: config JSON path does not exist")
+        else:
+            LOGGER.info("config JSON path checked..")
+    if not os.path.exists(config['vis_path']):
+        LOGGER.warning("Creating checkpoint path for Tensorboard visualizations at:  {}\n".format(
+            config['vis_path']))
+        os.makedirs(config['vis_path'])
+    else:
+        LOGGER.info("Tensorboard Visualization path checked..")
+        LOGGER.info(
+            "Cleaning Visualization path of older tensorboard files...\n")
+        # shutil.rmtree(config['vis_path'])
+
+    # Print args
+    print("\n" + "x"*50 + "\n\nRunning training with the following parameters: \n")
+    for key, value in config.items():
+        if not key.endswith('transf'):
+            print(key + ' : ' + str(value))
+    print("\n" + "x"*50)
+
+    config['writer'] = SummaryWriter(config['vis_path'])
+
+    set_seed(config['seed'])
+
+    # Tokenize
+    tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
+    tokenizer_func = partial(tokenizer, max_length=config['max_txt_len'], padding='max_length',
+                             truncation=True, return_tensors='pt', return_length=True)
+
+    # Prepare the datasets and dataloaders for training and evaluation
+    train_dataset = MemeDataset(filepath=os.path.join(config['data_path'], config['train_filename']),
+                                feature_dir=config['feature_path'], text_padding=tokenizer_func, filter_text=config["filter_text"],
+                                upsample_multiplier=config["upsample_multiplier"])
+    val_dataset = MemeDataset(filepath=os.path.join(config['data_path'], 'dev_seen.jsonl'),
+                              feature_dir=config['feature_path'], text_padding=tokenizer_func, filter_text=config["filter_text"])
+    test_dataset = MemeDataset(filepath=os.path.join(config['data_path'], 'test_seen.jsonl'),
+                               feature_dir=config['feature_path'], text_padding=tokenizer_func, filter_text=config["filter_text"])
+
+    config['train_loader'] = data.DataLoader(train_dataset, batch_size=config['batch_size'],
+                                             num_workers=config['num_workers'], collate_fn=train_dataset.get_collate_fn(), shuffle=True, pin_memory=True)
+    config['val_loader'] = data.DataLoader(val_dataset, batch_size=config['batch_size'],
+                                           num_workers=config['num_workers'], collate_fn=val_dataset.get_collate_fn())
+    config['test_loader'] = data.DataLoader(test_dataset, batch_size=config['batch_size'],
+                                            num_workers=config['num_workers'], collate_fn=test_dataset.get_collate_fn())
+
+    try:
+        trainer = TrainerUniter(config)
+        trainer.train_main()
+        wandb.save('vis_checkpoints/*', base_path="vis_checkpoints/")
+        wandb.finish()
+    except KeyboardInterrupt:
+        LOGGER.warning(
+            "Keyboard interrupt by user detected...\nClosing the tensorboard writer!")
+        config['writer'].close()
diff --git a/wandb/run-20210223_112430-3c31apdq/files/config.yaml b/wandb/run-20210223_112430-3c31apdq/files/config.yaml
new file mode 100644
index 0000000..674a8a3
--- /dev/null
+++ b/wandb/run-20210223_112430-3c31apdq/files/config.yaml
@@ -0,0 +1,151 @@
+wandb_version: 1
+
+_wandb:
+  desc: null
+  value:
+    cli_version: 0.10.19
+    code_path: code/train_uniter.py
+    framework: huggingface
+    huggingface_version: 3.2.0
+    is_jupyter_run: false
+    is_kaggle_kernel: false
+    python_version: 3.7.5
+    t:
+      1:
+      - 1
+      - 3
+      - 5
+      - 11
+      2:
+      - 1
+      - 3
+      - 5
+      - 11
+      3:
+      - 1
+      4: 3.7.5
+      5: 0.10.19
+      6: 3.2.0
+batch_size:
+  desc: null
+  value: 16
+beta1:
+  desc: null
+  value: 0.9
+beta2:
+  desc: null
+  value: 0.999
+config:
+  desc: null
+  value: config/uniter-base.json
+data_path:
+  desc: null
+  value: ./dataset
+dropout:
+  desc: null
+  value: 0.2
+early_stop_thresh:
+  desc: null
+  value: 0.001
+fc_dim:
+  desc: null
+  value: 64
+feature_path:
+  desc: null
+  value: ./dataset/own_features
+filter_text:
+  desc: null
+  value: false
+gradient_accumulation:
+  desc: null
+  value: 2
+log_every:
+  desc: null
+  value: 2000
+loss_func:
+  desc: null
+  value: bce_logits
+lr:
+  desc: null
+  value: 3.0e-05
+lr_decay_factor:
+  desc: null
+  value: 0.8
+lr_decay_step:
+  desc: null
+  value: 3
+max_bb:
+  desc: null
+  value: 100
+max_epoch:
+  desc: null
+  value: 30
+max_grad_norm:
+  desc: null
+  value: 5
+max_txt_len:
+  desc: null
+  value: 60
+min_bb:
+  desc: null
+  value: 10
+model_path:
+  desc: null
+  value: ./model_checkpoints
+model_save_name:
+  desc: null
+  value: meme.pt
+no_model_checkpoints:
+  desc: null
+  value: false
+no_normalize_img:
+  desc: null
+  value: true
+note:
+  desc: null
+  value: ''
+num_bb:
+  desc: null
+  value: 36
+num_workers:
+  desc: null
+  value: 0
+optimize_for:
+  desc: null
+  value: aucroc
+optimizer:
+  desc: null
+  value: adam
+patience:
+  desc: null
+  value: 5.0
+pos_wt:
+  desc: null
+  value: 1.0
+pretrained_model_file:
+  desc: null
+  value: uniter-base.pt
+remove_checkpoints:
+  desc: null
+  value: false
+scheduler:
+  desc: null
+  value: warmup_cosine
+seed:
+  desc: null
+  value: 43
+train_filename:
+  desc: null
+  value: train.jsonl
+upsample_multiplier:
+  desc: null
+  value: 0
+vis_path:
+  desc: null
+  value: ./vis_checkpoints
+warmup_steps:
+  desc: null
+  value: 500
+weight_decay:
+  desc: null
+  value: 0.001
diff --git a/wandb/run-20210223_112430-3c31apdq/files/diff.patch b/wandb/run-20210223_112430-3c31apdq/files/diff.patch
new file mode 100644
index 0000000..5353f25
--- /dev/null
+++ b/wandb/run-20210223_112430-3c31apdq/files/diff.patch
@@ -0,0 +1,49 @@
+diff --git a/data/meme_dataset.py b/data/meme_dataset.py
+index 3e9ee53..4df775c 100644
+--- a/data/meme_dataset.py
++++ b/data/meme_dataset.py
+@@ -201,7 +201,7 @@ class MemeDataset(data.Dataset):
+                               for json_dict in f.readlines()]
+         print("Loaded dataset contains ", str(len(self.json_list)), "samples")
+         self._load_dataset()
+-        self._load_gender_race_preds()
++        self._load_gender_race_probs()
+     
+ 
+     def _load_dataset(self):
+@@ -242,7 +242,7 @@ class MemeDataset(data.Dataset):
+             self.data.text = self.text_preprocess(self.data.text)
+ 
+     def _load_gender_race_probs(self):
+-        with open(f'../dataset/gender_race_preds/{self.name}_gender_race_probs.pickle', 'rb') as f:
++        with open(f'dataset/gender_race_probs/{self.name}_gender_race_probs.pickle', 'rb') as f:
+             self.data.gender_race_probs = pickle.load(f)
+ 
+     def __len__(self):
+diff --git a/wandb/debug-internal.log b/wandb/debug-internal.log
+index f09c7de..a3774a1 120000
+--- a/wandb/debug-internal.log
++++ b/wandb/debug-internal.log
+@@ -1 +1 @@
+-run-20210222_215454-1602o824/logs/debug-internal.log
+\ No newline at end of file
++run-20210223_112430-3c31apdq/logs/debug-internal.log
+\ No newline at end of file
+diff --git a/wandb/debug.log b/wandb/debug.log
+index 9d13059..b42a293 120000
+--- a/wandb/debug.log
++++ b/wandb/debug.log
+@@ -1 +1 @@
+-run-20210222_215454-1602o824/logs/debug.log
+\ No newline at end of file
++run-20210223_112430-3c31apdq/logs/debug.log
+\ No newline at end of file
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 15abb24..897ad3d 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20210222_215454-1602o824
+\ No newline at end of file
++run-20210223_112430-3c31apdq
+\ No newline at end of file
diff --git a/wandb/run-20210223_112430-3c31apdq/files/events.out.tfevents.1614075871.astro.1551076.0 b/wandb/run-20210223_112430-3c31apdq/files/events.out.tfevents.1614075871.astro.1551076.0
new file mode 120000
index 0000000..9744c40
--- /dev/null
+++ b/wandb/run-20210223_112430-3c31apdq/files/events.out.tfevents.1614075871.astro.1551076.0
@@ -0,0 +1 @@
+/home/astro/Documents/UvA/Block 4 - NLP2/Multimodal NLP/Multimodal-NLP/vis_checkpoints/events.out.tfevents.1614075871.astro.1551076.0
\ No newline at end of file
diff --git a/wandb/run-20210223_112430-3c31apdq/files/output.log b/wandb/run-20210223_112430-3c31apdq/files/output.log
new file mode 100644
index 0000000..0350c0d
--- /dev/null
+++ b/wandb/run-20210223_112430-3c31apdq/files/output.log
@@ -0,0 +1,407 @@
+23/02/2021 11:24:31 AM : INFO - Data path checked..
+23/02/2021 11:24:31 AM : INFO - Model save path checked..
+23/02/2021 11:24:31 AM : INFO - config JSON path checked..
+23/02/2021 11:24:31 AM : INFO - Tensorboard Visualization path checked..
+23/02/2021 11:24:31 AM : INFO - Cleaning Visualization path of older tensorboard files...
+
+
+xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
+
+Running training with the following parameters: 
+
+data_path : ./dataset
+model_path : ./model_checkpoints
+vis_path : ./vis_checkpoints
+model_save_name : meme.pt
+no_model_checkpoints : False
+remove_checkpoints : False
+config : config/uniter-base.json
+feature_path : ./dataset/own_features
+pretrained_model_file : uniter-base.pt
+max_txt_len : 60
+max_bb : 100
+min_bb : 10
+num_bb : 36
+optimizer : adam
+loss_func : bce_logits
+optimize_for : aucroc
+scheduler : warmup_cosine
+beta1 : 0.9
+beta2 : 0.999
+batch_size : 16
+num_workers : 0
+gradient_accumulation : 2
+max_grad_norm : 5
+pos_wt : 1.0
+lr : 3e-05
+warmup_steps : 500
+weight_decay : 0.001
+max_epoch : 30
+lr_decay_step : 3
+lr_decay_factor : 0.8
+patience : 5.0
+early_stop_thresh : 0.001
+seed : 43
+log_every : 2000
+fc_dim : 64
+dropout : 0.2
+filter_text : False
+no_normalize_img : True
+train_filename : train.jsonl
+upsample_multiplier : 0
+note : 
+device : cuda
+n_classes : 1
+
+xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
+filter text False
+Loaded dataset contains  8500 samples
+filter text False
+Loaded dataset contains  500 samples
+filter text False
+Loaded dataset contains  1000 samples
+23/02/2021 11:24:39 AM : INFO - Using pretrained UNITER base model ./model_checkpoints/uniter-base.pt
+23/02/2021 11:24:39 AM : INFO - Model config {
+  "attention_probs_dropout_prob": 0.1,
+  "hidden_act": "gelu",
+  "hidden_dropout_prob": 0.1,
+  "hidden_size": 768,
+  "initializer_range": 0.02,
+  "intermediate_size": 3072,
+  "max_position_embeddings": 512,
+  "num_attention_heads": 12,
+  "num_hidden_layers": 12,
+  "type_vocab_size": 2,
+  "vocab_size": 28996
+}
+
+MemeUniter
+MemeUniter(
+  (uniter_model): UniterModel(
+    (embeddings): UniterTextEmbeddings(
+      (word_embeddings): Embedding(28996, 768, padding_idx=0)
+      (position_embeddings): Embedding(512, 768)
+      (token_type_embeddings): Embedding(2, 768)
+      (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+      (dropout): Dropout(p=0.1, inplace=False)
+    )
+    (img_embeddings): UniterImageEmbeddings(
+      (img_linear): Linear(in_features=2048, out_features=768, bias=True)
+      (img_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+      (pos_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+      (pos_linear): Linear(in_features=7, out_features=768, bias=True)
+      (mask_embedding): Embedding(2, 2048, padding_idx=0)
+      (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+      (dropout): Dropout(p=0.1, inplace=False)
+    )
+    (encoder): UniterEncoder(
+      (layer): ModuleList(
+        (0): BertLayer(
+          (attention): BertAttention(
+            (self): BertSelfAttention(
+              (query): Linear(in_features=768, out_features=768, bias=True)
+              (key): Linear(in_features=768, out_features=768, bias=True)
+              (value): Linear(in_features=768, out_features=768, bias=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+            (output): BertSelfOutput(
+              (dense): Linear(in_features=768, out_features=768, bias=True)
+              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+          )
+          (intermediate): BertIntermediate(
+            (dense): Linear(in_features=768, out_features=3072, bias=True)
+          )
+          (output): BertOutput(
+            (dense): Linear(in_features=3072, out_features=768, bias=True)
+            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+            (dropout): Dropout(p=0.1, inplace=False)
+          )
+        )
+        (1): BertLayer(
+          (attention): BertAttention(
+            (self): BertSelfAttention(
+              (query): Linear(in_features=768, out_features=768, bias=True)
+              (key): Linear(in_features=768, out_features=768, bias=True)
+              (value): Linear(in_features=768, out_features=768, bias=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+            (output): BertSelfOutput(
+              (dense): Linear(in_features=768, out_features=768, bias=True)
+              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+          )
+          (intermediate): BertIntermediate(
+            (dense): Linear(in_features=768, out_features=3072, bias=True)
+          )
+          (output): BertOutput(
+            (dense): Linear(in_features=3072, out_features=768, bias=True)
+            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+            (dropout): Dropout(p=0.1, inplace=False)
+          )
+        )
+        (2): BertLayer(
+          (attention): BertAttention(
+            (self): BertSelfAttention(
+              (query): Linear(in_features=768, out_features=768, bias=True)
+              (key): Linear(in_features=768, out_features=768, bias=True)
+              (value): Linear(in_features=768, out_features=768, bias=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+            (output): BertSelfOutput(
+              (dense): Linear(in_features=768, out_features=768, bias=True)
+              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+          )
+          (intermediate): BertIntermediate(
+            (dense): Linear(in_features=768, out_features=3072, bias=True)
+          )
+          (output): BertOutput(
+            (dense): Linear(in_features=3072, out_features=768, bias=True)
+            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+            (dropout): Dropout(p=0.1, inplace=False)
+          )
+        )
+        (3): BertLayer(
+          (attention): BertAttention(
+            (self): BertSelfAttention(
+              (query): Linear(in_features=768, out_features=768, bias=True)
+              (key): Linear(in_features=768, out_features=768, bias=True)
+              (value): Linear(in_features=768, out_features=768, bias=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+            (output): BertSelfOutput(
+              (dense): Linear(in_features=768, out_features=768, bias=True)
+              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+          )
+          (intermediate): BertIntermediate(
+            (dense): Linear(in_features=768, out_features=3072, bias=True)
+          )
+          (output): BertOutput(
+            (dense): Linear(in_features=3072, out_features=768, bias=True)
+            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+            (dropout): Dropout(p=0.1, inplace=False)
+          )
+        )
+        (4): BertLayer(
+          (attention): BertAttention(
+            (self): BertSelfAttention(
+              (query): Linear(in_features=768, out_features=768, bias=True)
+              (key): Linear(in_features=768, out_features=768, bias=True)
+              (value): Linear(in_features=768, out_features=768, bias=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+            (output): BertSelfOutput(
+              (dense): Linear(in_features=768, out_features=768, bias=True)
+              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+          )
+          (intermediate): BertIntermediate(
+            (dense): Linear(in_features=768, out_features=3072, bias=True)
+          )
+          (output): BertOutput(
+            (dense): Linear(in_features=3072, out_features=768, bias=True)
+            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+            (dropout): Dropout(p=0.1, inplace=False)
+          )
+        )
+        (5): BertLayer(
+          (attention): BertAttention(
+            (self): BertSelfAttention(
+              (query): Linear(in_features=768, out_features=768, bias=True)
+              (key): Linear(in_features=768, out_features=768, bias=True)
+              (value): Linear(in_features=768, out_features=768, bias=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+            (output): BertSelfOutput(
+              (dense): Linear(in_features=768, out_features=768, bias=True)
+              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+          )
+          (intermediate): BertIntermediate(
+            (dense): Linear(in_features=768, out_features=3072, bias=True)
+          )
+          (output): BertOutput(
+            (dense): Linear(in_features=3072, out_features=768, bias=True)
+            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+            (dropout): Dropout(p=0.1, inplace=False)
+          )
+        )
+        (6): BertLayer(
+          (attention): BertAttention(
+            (self): BertSelfAttention(
+              (query): Linear(in_features=768, out_features=768, bias=True)
+              (key): Linear(in_features=768, out_features=768, bias=True)
+              (value): Linear(in_features=768, out_features=768, bias=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+            (output): BertSelfOutput(
+              (dense): Linear(in_features=768, out_features=768, bias=True)
+              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+          )
+          (intermediate): BertIntermediate(
+            (dense): Linear(in_features=768, out_features=3072, bias=True)
+          )
+          (output): BertOutput(
+            (dense): Linear(in_features=3072, out_features=768, bias=True)
+            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+            (dropout): Dropout(p=0.1, inplace=False)
+          )
+        )
+        (7): BertLayer(
+          (attention): BertAttention(
+            (self): BertSelfAttention(
+              (query): Linear(in_features=768, out_features=768, bias=True)
+              (key): Linear(in_features=768, out_features=768, bias=True)
+              (value): Linear(in_features=768, out_features=768, bias=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+            (output): BertSelfOutput(
+              (dense): Linear(in_features=768, out_features=768, bias=True)
+              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+          )
+          (intermediate): BertIntermediate(
+            (dense): Linear(in_features=768, out_features=3072, bias=True)
+          )
+          (output): BertOutput(
+            (dense): Linear(in_features=3072, out_features=768, bias=True)
+            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+            (dropout): Dropout(p=0.1, inplace=False)
+          )
+        )
+        (8): BertLayer(
+          (attention): BertAttention(
+            (self): BertSelfAttention(
+              (query): Linear(in_features=768, out_features=768, bias=True)
+              (key): Linear(in_features=768, out_features=768, bias=True)
+              (value): Linear(in_features=768, out_features=768, bias=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+            (output): BertSelfOutput(
+              (dense): Linear(in_features=768, out_features=768, bias=True)
+              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+          )
+          (intermediate): BertIntermediate(
+            (dense): Linear(in_features=768, out_features=3072, bias=True)
+          )
+          (output): BertOutput(
+            (dense): Linear(in_features=3072, out_features=768, bias=True)
+            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+            (dropout): Dropout(p=0.1, inplace=False)
+          )
+        )
+        (9): BertLayer(
+          (attention): BertAttention(
+            (self): BertSelfAttention(
+              (query): Linear(in_features=768, out_features=768, bias=True)
+              (key): Linear(in_features=768, out_features=768, bias=True)
+              (value): Linear(in_features=768, out_features=768, bias=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+            (output): BertSelfOutput(
+              (dense): Linear(in_features=768, out_features=768, bias=True)
+              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+          )
+          (intermediate): BertIntermediate(
+            (dense): Linear(in_features=768, out_features=3072, bias=True)
+          )
+          (output): BertOutput(
+            (dense): Linear(in_features=3072, out_features=768, bias=True)
+            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+            (dropout): Dropout(p=0.1, inplace=False)
+          )
+        )
+        (10): BertLayer(
+          (attention): BertAttention(
+            (self): BertSelfAttention(
+              (query): Linear(in_features=768, out_features=768, bias=True)
+              (key): Linear(in_features=768, out_features=768, bias=True)
+              (value): Linear(in_features=768, out_features=768, bias=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+            (output): BertSelfOutput(
+              (dense): Linear(in_features=768, out_features=768, bias=True)
+              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+          )
+          (intermediate): BertIntermediate(
+            (dense): Linear(in_features=768, out_features=3072, bias=True)
+          )
+          (output): BertOutput(
+            (dense): Linear(in_features=3072, out_features=768, bias=True)
+            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+            (dropout): Dropout(p=0.1, inplace=False)
+          )
+        )
+        (11): BertLayer(
+          (attention): BertAttention(
+            (self): BertSelfAttention(
+              (query): Linear(in_features=768, out_features=768, bias=True)
+              (key): Linear(in_features=768, out_features=768, bias=True)
+              (value): Linear(in_features=768, out_features=768, bias=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+            (output): BertSelfOutput(
+              (dense): Linear(in_features=768, out_features=768, bias=True)
+              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+          )
+          (intermediate): BertIntermediate(
+            (dense): Linear(in_features=768, out_features=3072, bias=True)
+          )
+          (output): BertOutput(
+            (dense): Linear(in_features=3072, out_features=768, bias=True)
+            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+            (dropout): Dropout(p=0.1, inplace=False)
+          )
+        )
+      )
+    )
+    (pooler): BertPooler(
+      (dense): Linear(in_features=768, out_features=768, bias=True)
+      (activation): Tanh()
+    )
+  )
+  (linear_1): Linear(in_features=768, out_features=384, bias=True)
+  (activation_1): ReLU()
+  (linear_2): Linear(in_features=384, out_features=1, bias=True)
+)
+
+
+====================================================================================================
+					 Training Network
+====================================================================================================
+
+Beginning training at:  2021-02-23 11:24:42.986757 
+
+Traceback (most recent call last):
+  File "train_uniter.py", line 648, in <module>
+    trainer.train_main()
+  File "train_uniter.py", line 432, in train_main
+    self.train_iter_step()
+  File "train_uniter.py", line 470, in train_iter_step
+    output_all_encoded_layers=False, gender_race_probs=self.batch['gender_race_probs'])
+  File "/home/astro/anaconda3/envs/nlp2-multimodal/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
+    result = self.forward(*input, **kwargs)
+  File "/home/astro/Documents/UvA/Block 4 - NLP2/Multimodal NLP/Multimodal-NLP/model/meme_uniter.py", line 25, in forward
+    out = self.uniter_model(**kwargs)
+  File "/home/astro/anaconda3/envs/nlp2-multimodal/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
+    result = self.forward(*input, **kwargs)
+TypeError: forward() got an unexpected keyword argument 'gender_race_probs'
diff --git a/wandb/run-20210223_112430-3c31apdq/files/requirements.txt b/wandb/run-20210223_112430-3c31apdq/files/requirements.txt
new file mode 100644
index 0000000..0d8abf6
--- /dev/null
+++ b/wandb/run-20210223_112430-3c31apdq/files/requirements.txt
@@ -0,0 +1,176 @@
+absl-py==0.10.0
+aiohttp==3.6.2
+apex==0.1
+argon2-cffi==20.1.0
+astor==0.8.1
+astunparse==1.6.3
+async-generator==1.10
+async-timeout==3.0.1
+attrs==20.2.0
+autopep8==1.5.5
+backcall==0.2.0
+beautifulsoup4==4.9.2
+bleach==3.2.1
+boto3==1.15.6
+botocore==1.18.6
+cached-property==1.5.2
+cachetools==4.1.1
+certifi==2020.6.20
+cffi==1.14.3
+chardet==3.0.4
+click==7.1.2
+configparser==5.0.1
+cycler==0.10.0
+dbfread==2.0.4
+deap==1.3.1
+decorator==4.4.2
+defusedxml==0.6.0
+demjson==2.2.4
+dlib==19.21.1
+docker-pycreds==0.4.0
+editdistance==0.5.3
+entrypoints==0.3
+fasttext==0.9.1
+filelock==3.0.12
+flask==1.1.2
+flatbuffers==1.12
+future==0.18.2
+gast==0.3.3
+gdown==3.12.2
+gitdb==4.0.5
+gitpython==3.1.0
+google-auth-oauthlib==0.4.1
+google-auth==1.22.0
+google-pasta==0.2.0
+gql==2.0.0
+graphql-core==2.3.2
+grpcio==1.32.0
+h5py==2.10.0
+idna==2.10
+ijson==2.6.1
+imagehash==4.2.0
+importlib-metadata==2.0.0
+ipykernel==5.3.4
+ipython-genutils==0.2.0
+ipython==7.20.0
+ipywidgets==7.5.1
+itsdangerous==1.1.0
+jedi==0.17.2
+jinja2==2.11.2
+jmespath==0.10.0
+joblib==0.16.0
+jsonschema==3.2.0
+jupyter-client==6.1.7
+jupyter-core==4.6.3
+jupyterlab-pygments==0.1.2
+keras-applications==1.0.8
+keras-preprocessing==1.1.2
+keras==2.4.3
+kiwisolver==1.2.0
+lightgbm==3.1.1
+lmdb==0.98
+markdown==3.2.2
+markupsafe==1.1.1
+matplotlib==3.3.2
+mdbtools==0.3.14
+meza==0.42.5
+mistune==0.8.4
+mtcnn==0.1.0
+multidict==4.7.6
+nbclient==0.5.0
+nbconvert==6.0.7
+nbformat==5.0.7
+nest-asyncio==1.4.1
+nltk==3.4.5
+notebook==6.1.4
+numpy==1.19.2
+nvidia-ml-py3==7.352.0
+oauthlib==3.1.0
+omegaconf==2.0.1rc4
+opencv-contrib-python==4.5.1.48
+opencv-python==4.5.1.48
+opt-einsum==3.3.0
+packaging==20.4
+pandas==1.1.2
+pandocfilters==1.4.2
+parso==0.7.1
+pathtools==0.1.2
+pexpect==4.8.0
+pickleshare==0.7.5
+pillow==7.2.0
+pip==20.3.3
+prometheus-client==0.8.0
+promise==2.3
+prompt-toolkit==3.0.7
+protobuf==3.13.0
+psutil==5.8.0
+ptyprocess==0.6.0
+pyasn1-modules==0.2.8
+pyasn1==0.4.8
+pybind11==2.6.2
+pycodestyle==2.6.0
+pycparser==2.20
+pygments==2.7.1
+pygogo==0.13.2
+pymongo==3.11.0
+pyparsing==2.4.7
+pyrsistent==0.17.3
+pysocks==1.7.1
+python-dateutil==2.8.1
+python-slugify==1.2.6
+pytz==2020.1
+pywavelets==1.1.1
+pyyaml==5.3.1
+pyzmq==19.0.2
+regex==2020.9.27
+requests-oauthlib==1.3.0
+requests==2.23.0
+rsa==4.6
+rx==1.6.1
+s3transfer==0.3.3
+sacremoses==0.0.43
+scikit-learn==0.23.2
+scipy==1.5.2
+seaborn==0.11.0
+send2trash==1.5.0
+sentencepiece==0.1.91
+sentry-sdk==0.20.3
+setuptools==52.0.0.post20210125
+shortuuid==1.0.1
+six==1.15.0
+sklearn==0.0
+smmap==3.0.4
+soupsieve==2.0.1
+subprocess32==3.5.4
+tensorboard-plugin-wit==1.7.0
+tensorboard==2.4.1
+tensorflow-estimator==2.4.0
+tensorflow==2.4.1
+termcolor==1.1.0
+terminado==0.9.1
+testpath==0.4.4
+threadpoolctl==2.1.0
+tokenizers==0.8.1rc2
+toml==0.10.2
+toolz==0.11.1
+torch==1.6.0+cu101
+torchtext==0.5.0
+torchvision==0.7.0+cu101
+tornado==6.0.4
+tqdm==4.50.0
+traitlets==5.0.5
+transformers==3.2.0
+typing-extensions==3.7.4.3
+unidecode==1.1.1
+urllib3==1.25.10
+wandb==0.10.19
+watchdog==2.0.1
+wcwidth==0.2.5
+webencodings==0.5.1
+werkzeug==1.0.1
+wheel==0.36.2
+widgetsnbextension==3.5.1
+wrapt==1.12.1
+xlrd==1.2.0
+yarl==1.6.0
+zipp==3.2.0
\ No newline at end of file
diff --git a/wandb/run-20210223_112430-3c31apdq/files/wandb-metadata.json b/wandb/run-20210223_112430-3c31apdq/files/wandb-metadata.json
new file mode 100644
index 0000000..2a04b50
--- /dev/null
+++ b/wandb/run-20210223_112430-3c31apdq/files/wandb-metadata.json
@@ -0,0 +1,55 @@
+{
+    "os": "Linux-5.8.0-43-generic-x86_64-with-debian-bullseye-sid",
+    "python": "3.7.5",
+    "heartbeatAt": "2021-02-23T10:24:31.681674",
+    "startedAt": "2021-02-23T10:24:30.280871",
+    "docker": null,
+    "gpu": "GeForce RTX 2060 SUPER",
+    "gpu_count": 1,
+    "cpu_count": 24,
+    "cuda": "11.0.228",
+    "args": [
+        "--config",
+        "config/uniter-base.json",
+        "--data_path",
+        "./dataset",
+        "--model_path",
+        "./model_checkpoints",
+        "--pretrained_model_file",
+        "uniter-base.pt",
+        "--feature_path",
+        "./dataset/own_features",
+        "--lr",
+        "3e-5",
+        "--scheduler",
+        "warmup_cosine",
+        "--warmup_steps",
+        "500",
+        "--max_epoch",
+        "30",
+        "--batch_size",
+        "16",
+        "--patience",
+        "5",
+        "--gradient_accumulation",
+        "2",
+        "--model_save_name",
+        "meme.pt",
+        "--seed",
+        "43",
+        "--pos_wt",
+        "1"
+    ],
+    "state": "running",
+    "codePath": "train_uniter.py",
+    "program": "train_uniter.py",
+    "git": {
+        "remote": "https://github.com/Noixas/Multimodal-NLP.git",
+        "commit": "83666a1f2f64375554a1633e19e39a7c7cef964b"
+    },
+    "email": "rodrigo.mulsa@outlook.com",
+    "root": "/home/astro/Documents/UvA/Block 4 - NLP2/Multimodal NLP/Multimodal-NLP",
+    "host": "astro",
+    "username": "astro",
+    "executable": "/home/astro/anaconda3/envs/nlp2-multimodal/bin/python"
+}
diff --git a/wandb/run-20210223_112430-3c31apdq/files/wandb-summary.json b/wandb/run-20210223_112430-3c31apdq/files/wandb-summary.json
new file mode 100644
index 0000000..9e26dfe
--- /dev/null
+++ b/wandb/run-20210223_112430-3c31apdq/files/wandb-summary.json
@@ -0,0 +1 @@
+{}
\ No newline at end of file
diff --git a/wandb/run-20210223_112430-3c31apdq/logs/debug-internal.log b/wandb/run-20210223_112430-3c31apdq/logs/debug-internal.log
new file mode 100644
index 0000000..1a13b5e
--- /dev/null
+++ b/wandb/run-20210223_112430-3c31apdq/logs/debug-internal.log
@@ -0,0 +1,121 @@
+2021-02-23 11:24:30,654 INFO    MainThread:1551135 [internal.py:wandb_internal():91] W&B internal server running at pid: 1551135, started at: 2021-02-23 11:24:30.654421
+2021-02-23 11:24:30,656 DEBUG   HandlerThread:1551135 [handler.py:handle_request():94] handle_request: check_version
+2021-02-23 11:24:30,656 INFO    WriterThread:1551135 [datastore.py:open_for_write():77] open: /home/astro/Documents/UvA/Block 4 - NLP2/Multimodal NLP/Multimodal-NLP/wandb/run-20210223_112430-3c31apdq/run-3c31apdq.wandb
+2021-02-23 11:24:30,657 DEBUG   SenderThread:1551135 [sender.py:send():117] send: header
+2021-02-23 11:24:30,657 DEBUG   SenderThread:1551135 [sender.py:send():117] send: request
+2021-02-23 11:24:30,657 DEBUG   SenderThread:1551135 [sender.py:send_request():126] send_request: check_version
+2021-02-23 11:24:30,690 DEBUG   SenderThread:1551135 [sender.py:send():117] send: run
+2021-02-23 11:24:30,858 INFO    SenderThread:1551135 [sender.py:_start_run_threads():596] run started: 3c31apdq with start time 1614075870
+2021-02-23 11:24:30,858 DEBUG   SenderThread:1551135 [sender.py:send():117] send: summary
+2021-02-23 11:24:30,859 DEBUG   HandlerThread:1551135 [handler.py:handle_request():94] handle_request: run_start
+2021-02-23 11:24:30,859 INFO    SenderThread:1551135 [sender.py:_save_file():682] saving file wandb-summary.json with policy end
+2021-02-23 11:24:31,681 DEBUG   HandlerThread:1551135 [meta.py:__init__():34] meta init
+2021-02-23 11:24:31,681 DEBUG   HandlerThread:1551135 [meta.py:__init__():48] meta init done
+2021-02-23 11:24:31,681 DEBUG   HandlerThread:1551135 [meta.py:probe():190] probe
+2021-02-23 11:24:31,687 DEBUG   HandlerThread:1551135 [meta.py:_setup_git():180] setup git
+2021-02-23 11:24:31,708 DEBUG   HandlerThread:1551135 [meta.py:_setup_git():187] setup git done
+2021-02-23 11:24:31,708 DEBUG   HandlerThread:1551135 [meta.py:_save_code():69] save code
+2021-02-23 11:24:31,717 DEBUG   HandlerThread:1551135 [meta.py:_save_code():90] save code done
+2021-02-23 11:24:31,717 DEBUG   HandlerThread:1551135 [meta.py:_save_patches():107] save patches
+2021-02-23 11:24:31,821 DEBUG   HandlerThread:1551135 [meta.py:_save_patches():149] save patches done
+2021-02-23 11:24:31,821 DEBUG   HandlerThread:1551135 [meta.py:_save_pip():52] save pip
+2021-02-23 11:24:31,821 DEBUG   HandlerThread:1551135 [meta.py:_save_pip():66] save pip done
+2021-02-23 11:24:31,821 DEBUG   HandlerThread:1551135 [meta.py:probe():231] probe done
+2021-02-23 11:24:31,823 DEBUG   SenderThread:1551135 [sender.py:send():117] send: files
+2021-02-23 11:24:31,824 INFO    SenderThread:1551135 [sender.py:_save_file():682] saving file wandb-metadata.json with policy now
+2021-02-23 11:24:31,824 INFO    SenderThread:1551135 [sender.py:_save_file():682] saving file code/train_uniter.py with policy now
+2021-02-23 11:24:31,824 INFO    SenderThread:1551135 [sender.py:_save_file():682] saving file diff.patch with policy now
+2021-02-23 11:24:31,830 DEBUG   HandlerThread:1551135 [handler.py:handle_request():94] handle_request: status
+2021-02-23 11:24:31,830 DEBUG   SenderThread:1551135 [sender.py:send():117] send: request
+2021-02-23 11:24:31,831 DEBUG   SenderThread:1551135 [sender.py:send_request():126] send_request: status
+2021-02-23 11:24:31,834 INFO    HandlerThread:1551135 [handler.py:handle_tbrecord():307] handling tbrecord: tbrecord {
+  log_dir: "./vis_checkpoints"
+  save: true
+  root_dir: "./vis_checkpoints"
+}
+
+2021-02-23 11:24:31,835 DEBUG   HandlerThread:1551135 [config_util.py:dict_from_config_file():99] no default config file found in config-defaults.yaml
+2021-02-23 11:24:31,972 DEBUG   SenderThread:1551135 [sender.py:send():117] send: config
+2021-02-23 11:24:32,143 DEBUG   SenderThread:1551135 [sender.py:send():117] send: tbrecord
+2021-02-23 11:24:32,143 DEBUG   SenderThread:1551135 [sender.py:send():117] send: files
+2021-02-23 11:24:32,143 INFO    SenderThread:1551135 [sender.py:_save_file():682] saving file events.out.tfevents.1614075871.astro.1551076.0 with policy live
+2021-02-23 11:24:35,076 DEBUG   SenderThread:1551135 [sender.py:send():117] send: stats
+2021-02-23 11:24:39,556 DEBUG   SenderThread:1551135 [sender.py:send():117] send: stats
+2021-02-23 11:24:43,380 DEBUG   SenderThread:1551135 [sender.py:send():117] send: telemetry
+2021-02-23 11:24:43,380 DEBUG   HandlerThread:1551135 [handler.py:handle_request():94] handle_request: poll_exit
+2021-02-23 11:24:43,548 DEBUG   SenderThread:1551135 [sender.py:send():117] send: exit
+2021-02-23 11:24:43,548 INFO    SenderThread:1551135 [sender.py:send_exit():195] handling exit code: 1
+2021-02-23 11:24:43,548 INFO    SenderThread:1551135 [sender.py:send_exit():203] send defer
+2021-02-23 11:24:43,549 DEBUG   SenderThread:1551135 [sender.py:send():117] send: request
+2021-02-23 11:24:43,549 DEBUG   SenderThread:1551135 [sender.py:send_request():126] send_request: poll_exit
+2021-02-23 11:24:43,549 DEBUG   HandlerThread:1551135 [handler.py:handle_request():94] handle_request: defer
+2021-02-23 11:24:43,549 INFO    HandlerThread:1551135 [handler.py:handle_request_defer():108] handle defer: 0
+2021-02-23 11:24:43,549 DEBUG   SenderThread:1551135 [sender.py:send():117] send: request
+2021-02-23 11:24:43,549 DEBUG   SenderThread:1551135 [sender.py:send_request():126] send_request: defer
+2021-02-23 11:24:43,549 INFO    SenderThread:1551135 [sender.py:send_request_defer():212] handle sender defer: 0
+2021-02-23 11:24:43,549 INFO    SenderThread:1551135 [sender.py:send_request_defer():248] send defer: 1
+2021-02-23 11:24:43,550 DEBUG   HandlerThread:1551135 [handler.py:handle_request():94] handle_request: defer
+2021-02-23 11:24:43,550 INFO    HandlerThread:1551135 [handler.py:handle_request_defer():108] handle defer: 1
+2021-02-23 11:24:43,624 DEBUG   SenderThread:1551135 [sender.py:send():117] send: request
+2021-02-23 11:24:43,624 DEBUG   SenderThread:1551135 [sender.py:send_request():126] send_request: defer
+2021-02-23 11:24:43,624 INFO    SenderThread:1551135 [sender.py:send_request_defer():212] handle sender defer: 1
+2021-02-23 11:24:43,624 INFO    SenderThread:1551135 [sender.py:send_request_defer():248] send defer: 2
+2021-02-23 11:24:43,624 DEBUG   SenderThread:1551135 [sender.py:send():117] send: stats
+2021-02-23 11:24:43,625 DEBUG   HandlerThread:1551135 [handler.py:handle_request():94] handle_request: defer
+2021-02-23 11:24:43,625 INFO    HandlerThread:1551135 [handler.py:handle_request_defer():108] handle defer: 2
+2021-02-23 11:24:49,838 DEBUG   SenderThread:1551135 [sender.py:send():117] send: request
+2021-02-23 11:24:49,838 DEBUG   HandlerThread:1551135 [handler.py:handle_request():94] handle_request: poll_exit
+2021-02-23 11:24:49,838 DEBUG   SenderThread:1551135 [sender.py:send_request():126] send_request: defer
+2021-02-23 11:24:49,838 INFO    SenderThread:1551135 [sender.py:send_request_defer():212] handle sender defer: 2
+2021-02-23 11:24:49,838 INFO    SenderThread:1551135 [sender.py:send_request_defer():248] send defer: 3
+2021-02-23 11:24:49,839 DEBUG   SenderThread:1551135 [sender.py:send():117] send: request
+2021-02-23 11:24:49,839 DEBUG   SenderThread:1551135 [sender.py:send_request():126] send_request: poll_exit
+2021-02-23 11:24:49,839 DEBUG   HandlerThread:1551135 [handler.py:handle_request():94] handle_request: defer
+2021-02-23 11:24:49,839 INFO    HandlerThread:1551135 [handler.py:handle_request_defer():108] handle defer: 3
+2021-02-23 11:24:49,839 DEBUG   SenderThread:1551135 [sender.py:send():117] send: summary
+2021-02-23 11:24:49,841 INFO    SenderThread:1551135 [sender.py:_save_file():682] saving file wandb-summary.json with policy end
+2021-02-23 11:24:49,842 DEBUG   SenderThread:1551135 [sender.py:send():117] send: request
+2021-02-23 11:24:49,842 DEBUG   SenderThread:1551135 [sender.py:send_request():126] send_request: defer
+2021-02-23 11:24:49,842 INFO    SenderThread:1551135 [sender.py:send_request_defer():212] handle sender defer: 3
+2021-02-23 11:24:49,842 INFO    SenderThread:1551135 [sender.py:send_request_defer():248] send defer: 4
+2021-02-23 11:24:49,842 DEBUG   HandlerThread:1551135 [handler.py:handle_request():94] handle_request: defer
+2021-02-23 11:24:49,842 INFO    HandlerThread:1551135 [handler.py:handle_request_defer():108] handle defer: 4
+2021-02-23 11:24:49,842 DEBUG   SenderThread:1551135 [sender.py:send():117] send: request
+2021-02-23 11:24:49,842 DEBUG   SenderThread:1551135 [sender.py:send_request():126] send_request: defer
+2021-02-23 11:24:49,842 INFO    SenderThread:1551135 [sender.py:send_request_defer():212] handle sender defer: 4
+2021-02-23 11:24:49,872 INFO    SenderThread:1551135 [sender.py:send_request_defer():248] send defer: 5
+2021-02-23 11:24:49,876 DEBUG   HandlerThread:1551135 [handler.py:handle_request():94] handle_request: defer
+2021-02-23 11:24:49,881 INFO    HandlerThread:1551135 [handler.py:handle_request_defer():108] handle defer: 5
+2021-02-23 11:24:49,881 DEBUG   SenderThread:1551135 [sender.py:send():117] send: request
+2021-02-23 11:24:49,882 DEBUG   SenderThread:1551135 [sender.py:send_request():126] send_request: defer
+2021-02-23 11:24:49,882 INFO    SenderThread:1551135 [sender.py:send_request_defer():212] handle sender defer: 5
+2021-02-23 11:24:49,882 INFO    SenderThread:1551135 [sender.py:send_request_defer():248] send defer: 6
+2021-02-23 11:24:49,882 DEBUG   HandlerThread:1551135 [handler.py:handle_request():94] handle_request: defer
+2021-02-23 11:24:49,882 INFO    HandlerThread:1551135 [handler.py:handle_request_defer():108] handle defer: 6
+2021-02-23 11:24:49,883 DEBUG   SenderThread:1551135 [sender.py:send():117] send: request
+2021-02-23 11:24:49,883 DEBUG   SenderThread:1551135 [sender.py:send_request():126] send_request: defer
+2021-02-23 11:24:49,883 INFO    SenderThread:1551135 [sender.py:send_request_defer():212] handle sender defer: 6
+2021-02-23 11:24:50,095 INFO    SenderThread:1551135 [sender.py:send_request_defer():248] send defer: 7
+2021-02-23 11:24:50,095 DEBUG   HandlerThread:1551135 [handler.py:handle_request():94] handle_request: defer
+2021-02-23 11:24:50,095 INFO    HandlerThread:1551135 [handler.py:handle_request_defer():108] handle defer: 7
+2021-02-23 11:24:50,095 DEBUG   SenderThread:1551135 [sender.py:send():117] send: request
+2021-02-23 11:24:50,095 DEBUG   SenderThread:1551135 [sender.py:send_request():126] send_request: defer
+2021-02-23 11:24:50,095 INFO    SenderThread:1551135 [sender.py:send_request_defer():212] handle sender defer: 7
+2021-02-23 11:24:50,095 INFO    SenderThread:1551135 [sender.py:send_request_defer():248] send defer: 8
+2021-02-23 11:24:50,096 DEBUG   SenderThread:1551135 [sender.py:send():117] send: final
+2021-02-23 11:24:50,096 DEBUG   SenderThread:1551135 [sender.py:send():117] send: footer
+2021-02-23 11:24:50,096 DEBUG   HandlerThread:1551135 [handler.py:handle_request():94] handle_request: defer
+2021-02-23 11:24:50,096 INFO    HandlerThread:1551135 [handler.py:handle_request_defer():108] handle defer: 8
+2021-02-23 11:24:50,096 DEBUG   SenderThread:1551135 [sender.py:send():117] send: request
+2021-02-23 11:24:50,096 DEBUG   SenderThread:1551135 [sender.py:send_request():126] send_request: defer
+2021-02-23 11:24:50,096 INFO    SenderThread:1551135 [sender.py:send_request_defer():212] handle sender defer: 8
+2021-02-23 11:24:51,842 DEBUG   HandlerThread:1551135 [handler.py:handle_request():94] handle_request: poll_exit
+2021-02-23 11:24:51,842 DEBUG   SenderThread:1551135 [sender.py:send():117] send: request
+2021-02-23 11:24:51,842 DEBUG   SenderThread:1551135 [sender.py:send_request():126] send_request: poll_exit
+2021-02-23 11:24:51,843 DEBUG   HandlerThread:1551135 [handler.py:handle_request():94] handle_request: get_summary
+2021-02-23 11:24:51,844 DEBUG   HandlerThread:1551135 [handler.py:handle_request():94] handle_request: sampled_history
+2021-02-23 11:24:51,844 DEBUG   HandlerThread:1551135 [handler.py:handle_request():94] handle_request: shutdown
+2021-02-23 11:24:51,844 INFO    HandlerThread:1551135 [handler.py:finish():333] shutting down handler
+2021-02-23 11:24:52,096 INFO    WriterThread:1551135 [datastore.py:close():258] close: /home/astro/Documents/UvA/Block 4 - NLP2/Multimodal NLP/Multimodal-NLP/wandb/run-20210223_112430-3c31apdq/run-3c31apdq.wandb
+2021-02-23 11:24:52,843 INFO    SenderThread:1551135 [sender.py:finish():766] shutting down sender
+2021-02-23 11:24:52,844 INFO    MainThread:1551135 [internal.py:handle_exit():78] Internal process exited
diff --git a/wandb/run-20210223_112430-3c31apdq/logs/debug.log b/wandb/run-20210223_112430-3c31apdq/logs/debug.log
new file mode 100644
index 0000000..5f08840
--- /dev/null
+++ b/wandb/run-20210223_112430-3c31apdq/logs/debug.log
@@ -0,0 +1,79 @@
+2021-02-23 11:24:30,282 INFO    MainThread:1551076 [wandb_setup.py:_flush():70] setting env: {}
+2021-02-23 11:24:30,282 INFO    MainThread:1551076 [wandb_setup.py:_flush():70] multiprocessing start_methods=fork,spawn,forkserver, using: spawn
+2021-02-23 11:24:30,282 INFO    MainThread:1551076 [wandb_setup.py:_flush():70] setting login settings: {}
+2021-02-23 11:24:30,282 INFO    MainThread:1551076 [wandb_init.py:_log_setup():319] Logging user logs to /home/astro/Documents/UvA/Block 4 - NLP2/Multimodal NLP/Multimodal-NLP/wandb/run-20210223_112430-3c31apdq/logs/debug.log
+2021-02-23 11:24:30,282 INFO    MainThread:1551076 [wandb_init.py:_log_setup():320] Logging internal logs to /home/astro/Documents/UvA/Block 4 - NLP2/Multimodal NLP/Multimodal-NLP/wandb/run-20210223_112430-3c31apdq/logs/debug-internal.log
+2021-02-23 11:24:30,283 INFO    MainThread:1551076 [wandb_init.py:init():351] calling init triggers
+2021-02-23 11:24:30,283 INFO    MainThread:1551076 [wandb_init.py:init():358] wandb.init called with sweep_config: {}
+config: {}
+2021-02-23 11:24:30,283 INFO    MainThread:1551076 [wandb_init.py:init():404] starting backend
+2021-02-23 11:24:30,293 INFO    MainThread:1551076 [backend.py:ensure_launched():81] starting backend process...
+2021-02-23 11:24:30,299 INFO    MainThread:1551076 [backend.py:ensure_launched():86] started backend process with pid: 1551135
+2021-02-23 11:24:30,300 INFO    MainThread:1551076 [wandb_init.py:init():413] backend started and connected
+2021-02-23 11:24:30,302 INFO    MainThread:1551076 [wandb_init.py:init():436] updated telemetry
+2021-02-23 11:24:30,302 INFO    MainThread:1551076 [wandb_init.py:init():459] communicating current version
+2021-02-23 11:24:30,689 INFO    MainThread:1551076 [wandb_init.py:init():464] got version response upgrade_message: "wandb version 0.10.20 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
+
+2021-02-23 11:24:30,689 INFO    MainThread:1551076 [wandb_init.py:init():472] communicating run to backend with 30 second timeout
+2021-02-23 11:24:30,858 INFO    MainThread:1551076 [wandb_init.py:init():495] starting run threads in backend
+2021-02-23 11:24:31,826 INFO    MainThread:1551076 [wandb_run.py:_console_start():1411] atexit reg
+2021-02-23 11:24:31,827 INFO    MainThread:1551076 [wandb_run.py:_redirect():1274] redirect: SettingsConsole.REDIRECT
+2021-02-23 11:24:31,827 INFO    MainThread:1551076 [wandb_run.py:_redirect():1279] Redirecting console.
+2021-02-23 11:24:31,827 INFO    MainThread:1551076 [redirect.py:install():213] install start
+2021-02-23 11:24:31,827 INFO    MainThread:1551076 [redirect.py:install():228] install stop
+2021-02-23 11:24:31,827 INFO    MainThread:1551076 [redirect.py:install():213] install start
+2021-02-23 11:24:31,828 INFO    MainThread:1551076 [redirect.py:install():228] install stop
+2021-02-23 11:24:31,828 INFO    MainThread:1551076 [wandb_run.py:_redirect():1325] Redirects installed.
+2021-02-23 11:24:31,828 INFO    MainThread:1551076 [wandb_init.py:init():518] run started, returning control to user process
+2021-02-23 11:24:31,831 INFO    MainThread:1551076 [wandb_run.py:_config_callback():663] config_cb None None {'data_path': './dataset', 'model_path': './model_checkpoints', 'vis_path': './vis_checkpoints', 'model_save_name': 'meme.pt', 'no_model_checkpoints': False, 'remove_checkpoints': False, 'config': 'config/uniter-base.json', 'feature_path': './dataset/own_features', 'pretrained_model_file': 'uniter-base.pt', 'max_txt_len': 60, 'max_bb': 100, 'min_bb': 10, 'num_bb': 36, 'optimizer': 'adam', 'loss_func': 'bce_logits', 'optimize_for': 'aucroc', 'scheduler': 'warmup_cosine', 'beta1': 0.9, 'beta2': 0.999, 'batch_size': 16, 'num_workers': 0, 'gradient_accumulation': 2, 'max_grad_norm': 5, 'pos_wt': 1.0, 'lr': 3e-05, 'warmup_steps': 500, 'weight_decay': 0.001, 'max_epoch': 30, 'lr_decay_step': 3, 'lr_decay_factor': 0.8, 'patience': 5.0, 'early_stop_thresh': 0.001, 'seed': 43, 'log_every': 2000, 'fc_dim': 64, 'dropout': 0.2, 'filter_text': False, 'no_normalize_img': True, 'train_filename': 'train.jsonl', 'upsample_multiplier': 0, 'note': ''}
+2021-02-23 11:24:31,833 INFO    MainThread:1551076 [wandb_run.py:_tensorboard_callback():734] tensorboard callback: ./vis_checkpoints, None
+2021-02-23 11:24:41,300 INFO    MainThread:1551076 [wandb_watch.py:watch():39] Watching
+2021-02-23 11:24:43,378 INFO    MainThread:1551076 [wandb_run.py:_atexit_cleanup():1381] got exitcode: 1
+2021-02-23 11:24:43,378 INFO    MainThread:1551076 [wandb_run.py:_restore():1353] restore
+2021-02-23 11:24:43,378 INFO    MainThread:1551076 [redirect.py:uninstall():232] uninstall start
+2021-02-23 11:24:43,378 INFO    MainThread:1551076 [redirect.py:_stop():287] _stop: stdout
+2021-02-23 11:24:43,379 INFO    MainThread:1551076 [redirect.py:_stop():293] _stop closed: stdout
+2021-02-23 11:24:43,379 INFO    stdout    :1551076 [redirect.py:_pipe_relay():129] relay done saw last write: stdout
+2021-02-23 11:24:43,379 INFO    stdout    :1551076 [redirect.py:_pipe_relay():145] relay done done: stdout
+2021-02-23 11:24:43,379 INFO    MainThread:1551076 [redirect.py:_stop():299] _stop joined: stdout
+2021-02-23 11:24:43,379 INFO    MainThread:1551076 [redirect.py:_stop():301] _stop rd closed: stdout
+2021-02-23 11:24:43,379 INFO    MainThread:1551076 [redirect.py:uninstall():236] uninstall done
+2021-02-23 11:24:43,379 INFO    MainThread:1551076 [redirect.py:uninstall():232] uninstall start
+2021-02-23 11:24:43,379 INFO    MainThread:1551076 [redirect.py:_stop():287] _stop: stderr
+2021-02-23 11:24:43,379 INFO    MainThread:1551076 [redirect.py:_stop():293] _stop closed: stderr
+2021-02-23 11:24:43,379 INFO    stderr    :1551076 [redirect.py:_pipe_relay():129] relay done saw last write: stderr
+2021-02-23 11:24:43,379 INFO    stderr    :1551076 [redirect.py:_pipe_relay():145] relay done done: stderr
+2021-02-23 11:24:43,379 INFO    MainThread:1551076 [redirect.py:_stop():299] _stop joined: stderr
+2021-02-23 11:24:43,379 INFO    MainThread:1551076 [redirect.py:_stop():301] _stop rd closed: stderr
+2021-02-23 11:24:43,380 INFO    MainThread:1551076 [redirect.py:uninstall():236] uninstall done
+2021-02-23 11:24:43,549 INFO    MainThread:1551076 [wandb_run.py:_wait_for_finish():1504] got exit ret: file_counts {
+  wandb_count: 2
+  other_count: 2
+}
+pusher_stats {
+  uploaded_bytes: 36889
+  total_bytes: 36889
+}
+
+2021-02-23 11:24:49,839 INFO    MainThread:1551076 [wandb_run.py:_wait_for_finish():1504] got exit ret: file_counts {
+  wandb_count: 2
+  other_count: 2
+}
+pusher_stats {
+  uploaded_bytes: 36889
+  total_bytes: 36889
+}
+
+2021-02-23 11:24:51,843 INFO    MainThread:1551076 [wandb_run.py:_wait_for_finish():1504] got exit ret: done: true
+exit_result {
+}
+file_counts {
+  wandb_count: 6
+  other_count: 2
+}
+pusher_stats {
+  uploaded_bytes: 59728
+  total_bytes: 59728
+}
+
+2021-02-23 11:24:53,129 INFO    MainThread:1551076 [wandb_run.py:_show_files():1726] logging synced files
diff --git a/wandb/run-20210223_112430-3c31apdq/run-3c31apdq.wandb b/wandb/run-20210223_112430-3c31apdq/run-3c31apdq.wandb
new file mode 100644
index 0000000..655fd1d
Binary files /dev/null and b/wandb/run-20210223_112430-3c31apdq/run-3c31apdq.wandb differ
diff --git a/wandb/run-20210223_112549-3ck5aive/files/code/train_uniter.py b/wandb/run-20210223_112549-3ck5aive/files/code/train_uniter.py
new file mode 100644
index 0000000..44d832c
--- /dev/null
+++ b/wandb/run-20210223_112549-3ck5aive/files/code/train_uniter.py
@@ -0,0 +1,654 @@
+import wandb
+import argparse
+import os
+import time
+import datetime
+import shutil
+import random
+import sys
+import os
+import json
+import re
+import numpy as np
+from statistics import mean, stdev
+import torch
+from torch.utils.tensorboard import SummaryWriter
+import torch.nn as nn
+import torch.nn.functional as F
+from sklearn.metrics import classification_report
+from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup
+from collections import defaultdict
+from functools import partial
+from torch.utils import data
+from transformers import BertTokenizer
+
+from utils.metrics import standard_metrics, find_optimal_threshold
+from utils.optim_utils import get_optimizer
+from utils.utils import calc_elapsed_time, print_stats, print_test_stats, log_tensorboard, set_seed, get_device, get_gather_index, get_attention_mask
+from utils.save import ModelSaver
+from model.meme_uniter import MemeUniter
+from model.pretrain import UniterForPretraining
+from utils.logger import LOGGER
+from data.meme_dataset import MemeDataset
+from model.model import UniterModel, UniterConfig
+from utils.const import IMG_DIM, IMG_LABEL_DIM
+
+
+class TrainerUniter():
+
+    def __init__(self, config):
+        self.preds_list, self.probs_list, self.labels_list, self.loss_list, self.short_loss_list, self.id_list = [], [], [], [], [], []
+        self.best_val_metrics, self.train_metrics = defaultdict(int), {}
+        self.best_auc = 0
+        self.not_improved = 0
+        self.best_val_loss = 1000
+        self.total_iters = 0
+        self.terminate_training = False
+        self.model_file = os.path.join(
+            config['model_path'], config['model_save_name'])
+        self.pretrained_model_file = None
+        if config['pretrained_model_file'] is not None:
+            self.pretrained_model_file = os.path.join(
+                config['model_path'], config['pretrained_model_file'])
+        self.start_epoch = 1
+        self.config = config
+        self.device = get_device()
+
+        if not isinstance(self.config['test_loader'], list):
+            self.config['test_loader'] = [self.config['test_loader']]
+
+        # Initialize the model, optimizer and loss function
+        self.init_training_params()
+
+    def init_training_params(self):
+        self.init_model()
+        wandb.watch(self.model)
+        self.model_saver = ModelSaver(self.model_file)
+
+        self.init_optimizer()
+        self.init_scheduler()
+
+        if self.config['loss_func'] == 'bce_logits':
+            self.criterion = nn.BCEWithLogitsLoss(
+                pos_weight=torch.tensor([self.config['pos_wt']]).to(self.device))
+        elif self.config['loss_func'] == 'bce':
+            self.criterion = nn.BCELoss()
+        else:
+            self.criterion = nn.CrossEntropyLoss()
+
+    def init_scheduler(self):
+        if self.config['scheduler'] == 'step':
+            self.scheduler = torch.optim.lr_scheduler.StepLR(
+                self.optimizer, step_size=self.config['lr_decay_step'], gamma=self.config['lr_decay_factor'])
+        elif self.config['scheduler'] == 'multi_step':
+            self.scheduler = torch.optim.lr_scheduler.MultiStepLR(
+                self.optimizer, milestones=[5, 10, 15, 25, 40], gamma=self.config['lr_decay_factor'])
+        elif self.config['scheduler'] == 'warmup':
+            self.scheduler = get_linear_schedule_with_warmup(self.optimizer, num_warmup_steps=self.config['warmup_steps'],
+                                                             num_training_steps=len(self.config['train_loader']) * self.config['max_epoch'])
+        elif self.config['scheduler'] == 'warmup_cosine':
+            self.scheduler = get_cosine_schedule_with_warmup(self.optimizer, num_warmup_steps=self.config['warmup_steps'],
+                                                             num_training_steps=len(self.config['train_loader']) * self.config['max_epoch'])
+
+    def init_optimizer(self):
+        self.optimizer = get_optimizer(self.model, self.config)
+
+    def init_model(self):
+        # pretrained model file is the original pretrained model - load and use this to fine-tune.
+        # If this argument is False, it will load the model file saved by you after fine-tuning
+        if self.pretrained_model_file:
+            checkpoint = torch.load(self.pretrained_model_file)
+            LOGGER.info('Using pretrained UNITER base model {}'.format(
+                self.pretrained_model_file))
+            base_model = UniterForPretraining.from_pretrained(self.config['config'],
+                                                              state_dict=checkpoint['model_state_dict'],
+                                                              img_dim=IMG_DIM,
+                                                              img_label_dim=IMG_LABEL_DIM)
+            self.model = MemeUniter(uniter_model=base_model.uniter,
+                                    hidden_size=base_model.uniter.config.hidden_size,
+                                    n_classes=self.config['n_classes'])
+        else:
+            self.load_model()
+        print("MemeUniter")
+        print(self.model)
+
+    def load_model(self):
+        # Load pretrained model
+        if self.model_file:
+            checkpoint = torch.load(self.model_file)
+            LOGGER.info('Using UNITER model {}'.format(self.model_file))
+        else:
+            checkpoint = {}
+
+        uniter_config = UniterConfig.from_json_file(self.config['config'])
+        uniter_model = UniterModel(uniter_config, img_dim=IMG_DIM)
+
+        self.model = MemeUniter(uniter_model=uniter_model,
+                                hidden_size=uniter_model.config.hidden_size,
+                                n_classes=self.config['n_classes'])
+        self.model.load_state_dict(checkpoint['model_state_dict'])
+
+    def average_gradients(self, steps):
+        # Used when grad_accumulation > 1
+        for param in self.model.parameters():
+            if param.requires_grad and param.grad is not None:
+                param.grad = param.grad / steps
+
+    def calculate_loss(self, preds, batch_label, grad_step):
+        if self.config['loss_func'] == 'bce':
+            preds = torch.sigmoid(preds)
+        preds = preds.squeeze(1).to(
+            self.device) if self.config['loss_func'] == 'bce_logits' else preds.to(self.device)
+        loss = self.criterion(preds, batch_label.to(
+            self.device) if self.config['loss_func'] == 'ce' else batch_label.float().to(self.device))
+
+        if grad_step and self.iters % self.config['gradient_accumulation'] == 0:
+            loss.backward()
+            self.average_gradients(steps=self.config['gradient_accumulation'])
+            torch.nn.utils.clip_grad_norm_(
+                self.model.parameters(), self.config['max_grad_norm'])
+            self.optimizer.step()
+            self.scheduler.step()
+            self.optimizer.zero_grad()
+        elif grad_step:
+            loss.backward()
+
+        if self.config['loss_func'] == 'bce':
+            probs = preds
+            preds = (preds > 0.5).type(torch.FloatTensor)
+        elif self.config['loss_func'] == 'ce':
+            probs = F.softmax(preds, dim=1)
+            preds = torch.argmax(probs, dim=1)
+        elif self.config['loss_func'] == 'bce_logits':
+            probs = torch.sigmoid(preds)
+            preds = (probs > 0.5).type(torch.FloatTensor)
+
+        self.probs_list.append(probs.cpu().detach().numpy())
+        self.preds_list.append(preds.cpu().detach().numpy())
+        self.labels_list.append(batch_label.cpu().detach().numpy())
+        self.loss_list.append(loss.detach().item())
+        if grad_step:
+            self.short_loss_list.append(loss.detach().item())
+
+    def eval_model(self, test=False, test_idx=0):
+        self.model.eval()
+        self.preds_list, self.probs_list, self.labels_list, self.loss_list, self.id_list = [], [], [], [], []
+        batch_loader = self.config['val_loader'] if not test else self.config['test_loader'][test_idx]
+        with torch.no_grad():
+            for iters, batch in enumerate(batch_loader):
+                batch = self.batch_to_device(batch)
+                if batch_loader.dataset.return_ids:
+                    self.id_list.append(batch['ids'])
+                self.eval_iter_step(iters, batch, test=test)
+
+            self.probs_list = [
+                prob for batch_prob in self.probs_list for prob in batch_prob]
+            self.preds_list = [
+                pred for batch_pred in self.preds_list for pred in batch_pred]
+            self.labels_list = [
+                label for batch_labels in self.labels_list for label in batch_labels]
+            self.id_list = [
+                data_id for batch_id in self.id_list for data_id in batch_id]
+
+            val_loss = sum(self.loss_list)/len(self.loss_list)
+            eval_metrics = standard_metrics(torch.tensor(
+                self.probs_list), torch.tensor(self.labels_list), add_optimal_acc=True)
+            # if test:
+            # 	print(classification_report(np.array(self.labels_list), np.array(self.preds_list)))
+        return eval_metrics, val_loss
+
+    @torch.no_grad()
+    def export_test_predictions(self, test_idx=0, threshold=0.5):
+        self.model.eval()
+
+        # Step 2: Run model on the test set (no loss!)
+        # Ensure that ids are actually returned
+        assert self.config['test_loader'][test_idx].dataset.return_ids, "Can only export test results if the IDs are returned in the test dataset."
+        test_name = self.config['test_loader'][test_idx].dataset.name
+
+        prob_list = []
+        id_list = []
+        for iters, batch in enumerate(self.config['test_loader'][test_idx]):
+            batch = self.batch_to_device(batch)
+            id_list.append(batch['ids'].cpu())
+            probs = self.test_iter_step(batch)
+            if self.config['loss_func'] == 'bce_logits':
+                probs = torch.sigmoid(probs)
+            prob_list.append(probs.detach().cpu())
+
+        probs = torch.cat(prob_list, dim=0)
+        ids = torch.cat(id_list, dim=0)
+        preds = (probs > threshold).long()
+
+        # Step 3: Export predictions
+        self._export_preds(ids, probs, preds,
+                           file_postfix="_%s_preds.csv" % test_name)
+
+        LOGGER.info("Finished export of test predictions")
+
+    @torch.no_grad()
+    def export_val_predictions(self, test=False, test_idx=0, threshold=0.5):
+        batch_loader = self.config['val_loader'] if not test else self.config['test_loader'][test_idx]
+        test_name = batch_loader.dataset.name
+        LOGGER.info("Exporting %s predictions..." % (test_name))
+        self.model.eval()
+
+        # Step 1: Find the optimal threshold on validation set
+        _, _ = self.eval_model(test=test, test_idx=test_idx)
+        val_probs = torch.tensor(self.probs_list)
+        val_labels = torch.tensor(self.labels_list)
+        if len(self.id_list) != 0:
+            val_ids = torch.tensor(self.id_list)
+        else:
+            val_ids = torch.zeros_like(val_labels)-1
+        val_preds = (val_probs > threshold).long()
+
+        self._export_preds(val_ids, val_probs, val_preds,
+                           labels=val_labels, file_postfix="_%s_preds.csv" % test_name)
+
+        LOGGER.info("Finished export of %s predictions" % test_name)
+
+    def _export_preds(self, ids, probs, preds, labels=None, file_postfix="_preds.csv"):
+        file_string = "id,proba,label%s\n" % (
+            ",gt" if labels is not None else "")
+        for i in range(ids.shape[0]):
+            file_string += "%i,%f,%i" % (ids[i].item(),
+                                         probs[i].item(), preds[i].item())
+            if labels is not None:
+                file_string += ",%i" % labels[i].item()
+            file_string += "\n"
+        filepath = os.path.join(
+            self.config['model_path'], self.config['model_save_name'].rsplit(".", 1)[0] + file_postfix)
+        with open(filepath, "w") as f:
+            f.write(file_string)
+        wandb.save(filepath) #Upload file to wandb
+
+    def check_early_stopping(self):
+        self.this_metric = self.val_loss if self.config[
+            'optimize_for'] == 'loss' else self.val_metrics[self.config['optimize_for']]
+        self.current_best = self.best_val_loss if self.config[
+            'optimize_for'] == 'loss' else self.best_val_metrics[self.config['optimize_for']]
+
+        new_best = self.this_metric < self.current_best if self.config[
+            'optimize_for'] == 'loss' else self.this_metric > self.current_best
+        if new_best:
+            LOGGER.info("New High Score! Saving model...")
+            self.best_val_metrics = self.val_metrics
+            self.best_val_loss = self.val_loss
+            wandb.log({'Best val metrics': self.best_val_metrics,
+                       'Best val loss': self.best_val_loss})
+
+            if not self.config["no_model_checkpoints"]:
+                self.model_saver.save(self.model)
+
+        ### Stopping Criteria based on patience and change-in-metric-threshold ###
+        diff = self.current_best - \
+            self.this_metric if self.config['optimize_for'] == 'loss' else self.this_metric - \
+            self.current_best
+        if diff < self.config['early_stop_thresh']:
+            self.not_improved += 1
+            if self.not_improved >= self.config['patience']:
+                self.terminate_training = True
+        else:
+            self.not_improved = 0
+        LOGGER.info("current patience: {}".format(self.not_improved))
+
+    def train_epoch_step(self):
+        self.model.train()
+        lr = self.scheduler.get_last_lr()
+        self.total_iters += self.iters + 1
+        self.probs_list = [
+            pred for batch_pred in self.probs_list for pred in batch_pred]
+        self.labels_list = [
+            label for batch_labels in self.labels_list for label in batch_labels]
+
+        # Evaluate on train set
+        self.train_metrics = standard_metrics(torch.tensor(
+            self.probs_list), torch.tensor(self.labels_list), add_optimal_acc=True)
+        log_tensorboard(self.config, self.config['writer'], self.model, self.epoch, self.iters, self.total_iters,
+                        self.loss_list, self.train_metrics, lr[0], loss_only=False, val=False)
+        self.train_loss = self.loss_list[:]
+
+        # Evaluate on dev set
+        val_time = time.time()
+        self.val_metrics, self.val_loss = self.eval_model()
+        self.config['writer'].add_scalar(
+            "Stats/time_validation", time.time() - val_time, self.total_iters)
+
+        # print stats
+        print_stats(self.config, self.epoch, self.train_metrics,
+                    self.train_loss, self.val_metrics, self.val_loss, self.start, lr[0])
+
+        # log validation stats in tensorboard
+        log_tensorboard(self.config, self.config['writer'], self.model, self.epoch, self.iters,
+                        self.total_iters, self.val_loss, self.val_metrics, lr[0], loss_only=False, val=True)
+
+        # Check for early stopping criteria
+        self.check_early_stopping()
+        self.probs_list = []
+        self.preds_list = []
+        self.labels_list = []
+        self.loss_list = []
+        self.id_list = []
+
+        self.train_loss = sum(self.train_loss)/len(self.train_loss)
+        del self.val_metrics
+        del self.val_loss
+
+    def end_training(self):
+        # Termination message
+        print("\n" + "-"*100)
+        if self.terminate_training:
+            LOGGER.info("Training terminated early because the Validation {} did not improve for  {}  epochs" .format(
+                self.config['optimize_for'], self.config['patience']))
+        else:
+            LOGGER.info("Maximum epochs of {} reached. Finished training !!".format(
+                self.config['max_epoch']))
+
+        print_test_stats(self.best_val_metrics, test=False)
+
+        print("-"*50 + "\n\t\tEvaluating on test set\n" + "-"*50)
+        if not self.config["no_model_checkpoints"]:
+            if os.path.isfile(self.model_file):
+                self.load_model()
+                self.model.to(self.device)
+            else:
+                raise ValueError("No Saved model state_dict found for the chosen model...!!! \nAborting evaluation on test set...".format(
+                    self.config['model_name']))
+
+            self.export_val_predictions()  # Runs evaluation, no need to run it again here
+            val_probs = torch.tensor(self.probs_list)
+            val_labels = torch.tensor(self.labels_list)
+            threshold = 0.5  # the default threshelod for binary classification
+            # Uncomment below line if you have implemented this optional feature
+            # threshold = find_optimal_threshold(val_probs, val_labels, metric="accuracy")
+            best_val_metrics = standard_metrics(
+                val_probs, val_labels, threshold=threshold, add_aucroc=False)
+            LOGGER.info("Optimal threshold on validation dataset: %.4f (accuracy=%4.2f%%)" % (
+                threshold, 100.0*best_val_metrics["accuracy"]))
+
+            # Testing is in the standard form not possible, as we do not have any labels (gives an error in standard_metrics)
+            # Instead, we should write out the predictions in the form of the leaderboard
+            self.test_metrics = dict()
+            for test_idx in range(len(self.config['test_loader'])):
+                test_name = self.config['test_loader'][test_idx].dataset.name
+                LOGGER.info("Export and testing on %s..." % test_name)
+                if hasattr(self.config['test_loader'][test_idx].dataset, "data") and \
+                   hasattr(self.config['test_loader'][test_idx].dataset.data, "labels") and \
+                   self.config['test_loader'][test_idx].dataset.data.labels[0] == -1:  # Step 1: Find the optimal threshold on validation set
+                    self.export_test_predictions(
+                        test_idx=test_idx, threshold=threshold)
+                    self.test_metrics[test_name] = dict()
+                else:
+                    test_idx_metrics, _ = self.eval_model(
+                        test=True, test_idx=test_idx)
+                    self.test_metrics[test_name] = test_idx_metrics
+                    print_test_stats(test_idx_metrics, test=True)
+                    self.export_val_predictions(
+                        test=True, test_idx=test_idx, threshold=threshold)
+        else:
+            LOGGER.info(
+                "No model checkpoints were saved. Hence, testing will be skipped.")
+            self.test_metrics = dict()
+
+        self.export_metrics()
+
+        self.config['writer'].close()
+
+        if self.config['remove_checkpoints']:
+            LOGGER.info("Removing checkpoint %s..." % self.model_file)
+            os.remove(self.model_file)
+
+    def export_metrics(self):
+        metric_export_file = os.path.join(
+            self.config['model_path'], self.config['model_save_name'].rsplit(".", 1)[0] + "_metrics.json")
+        metric_dict = {}
+        metric_dict["dev"] = self.best_val_metrics
+        metric_dict["dev"]["loss"] = self.best_val_loss
+        metric_dict["train"] = self.train_metrics
+        metric_dict["train"]["loss"] = sum(self.train_loss)/len(
+            self.train_loss) if isinstance(self.train_loss, list) else self.train_loss
+        if hasattr(self, "test_metrics") and len(self.test_metrics) > 0:
+            metric_dict["test"] = self.test_metrics
+
+        with open(metric_export_file, "w") as f:
+            json.dump(metric_dict, f, indent=4)
+
+    def train_main(self, cache=False):
+        print("\n\n" + "="*100 + "\n\t\t\t\t\t Training Network\n" + "="*100)
+
+        self.start = time.time()
+        print("\nBeginning training at:  {} \n".format(datetime.datetime.now()))
+
+        self.model.to(self.device)
+
+        for self.epoch in range(self.start_epoch, self.config['max_epoch']+1):
+            train_times = []
+            for self.iters, self.batch in enumerate(self.config['train_loader']):
+                self.model.train()
+
+                iter_time = time.time()
+                self.batch = self.batch_to_device(self.batch)
+                self.train_iter_step()
+                train_times.append(time.time() - iter_time)
+
+                # Loss only logging
+                if (self.total_iters+self.iters+1) % self.config['log_every'] == 0:
+                    log_tensorboard(self.config, self.config['writer'], self.model, self.epoch,
+                                    self.iters, self.total_iters, self.short_loss_list, loss_only=True, val=False)
+                    self.config['writer'].add_scalar(
+                        'Stats/time_per_train_iter', mean(train_times), (self.iters+self.total_iters+1))
+                    self.config['writer'].add_scalar(
+                        'Stats/learning_rate', self.scheduler.get_last_lr()[0], (self.iters+self.total_iters+1))
+                    train_times = []
+                    self.short_loss_list = []
+            self.train_epoch_step()
+
+            if self.terminate_training:
+                break
+
+        self.end_training()
+        return self.best_val_metrics, self.test_metrics
+
+    def batch_to_device(self, batch):
+        batch = {k: (v.to(self.device) if isinstance(v, torch.Tensor) else v)
+                 for k, v in batch.items()}
+        return batch
+
+    def eval_iter_step(self, iters, batch, test):
+        # Forward pass
+        preds = self.model(img_feat=batch['img_feat'], img_pos_feat=batch['img_pos_feat'], input_ids=batch['input_ids'],
+                           position_ids=batch['position_ids'], attention_mask=batch['attn_mask'], gather_index=batch['gather_index'],
+                           output_all_encoded_layers=False, gender_race_probs=batch['gender_race_probs'])
+        self.calculate_loss(preds, batch['labels'], grad_step=False)
+
+    def train_iter_step(self):
+        # Forward pass
+        self.preds = self.model(img_feat=self.batch['img_feat'], img_pos_feat=self.batch['img_pos_feat'], input_ids=self.batch['input_ids'],
+                                position_ids=self.batch['position_ids'], attention_mask=self.batch[
+                                    'attn_mask'], gather_index=self.batch['gather_index'],
+                                output_all_encoded_layers=False, gender_race_probs=self.batch['gender_race_probs'])
+        self.calculate_loss(self.preds, self.batch['labels'], grad_step=True)
+
+    def test_iter_step(self, batch):
+        # Forward pass
+        preds = self.model(img_feat=batch['img_feat'], img_pos_feat=batch['img_pos_feat'], input_ids=batch['input_ids'],
+                           position_ids=batch['position_ids'], attention_mask=batch['attn_mask'], gather_index=batch['gather_index'],
+                           output_all_encoded_layers=False, gender_race_probs=batch['gender_race_probs'])
+        return preds.squeeze()
+
+
+if __name__ == '__main__':
+    wandb.init(project="multimodal-nlp2")
+    wandb.tensorboard.patch(root_logdir='./vis_checkpoints',
+                            pytorch=True, tensorboardX=False)
+    parser = argparse.ArgumentParser()
+    defaults = dict()
+
+    # Required Paths
+    parser.add_argument('--data_path', type=str, default='./dataset',
+                        help='path to dataset folder that contains the processed data files')
+    parser.add_argument('--model_path', type=str, default='./model_checkpoints',
+                        help='Directory for saving trained model checkpoints')
+    parser.add_argument('--vis_path', type=str, default='./vis_checkpoints',
+                        help='Directory for saving tensorboard checkpoints')
+    parser.add_argument("--model_save_name", type=str, default='best_model.pt',
+                        help='saved model name')
+    parser.add_argument("--no_model_checkpoints", action="store_true",
+                        help='If selected, no model checkpoints will be created, and no testing performed (for gridsearches etc.)')
+    parser.add_argument("--remove_checkpoints", action="store_true",
+                        help='If selected, model checkpoints will be deleted after finishing testing.')
+    parser.add_argument('--config', type=str, default='./config/uniter-base.json',
+                        help='JSON config file')
+    parser.add_argument('--feature_path', type=str, default='./dataset/img_feats',
+                        help='Path to image features')
+
+    # Load pretrained model
+    parser.add_argument('--pretrained_model_file', type=str,
+                        help='Name of the original pretrained model')
+
+    #### Pre-processing Params ####
+    parser.add_argument('--max_txt_len', type=int, default=60,
+                        help='max number of tokens in text (BERT BPE)')
+    parser.add_argument('--max_bb', type=int, default=100,
+                        help='max number of bounding boxes')
+    parser.add_argument('--min_bb', type=int, default=10,
+                        help='min number of bounding boxes')
+    parser.add_argument('--num_bb', type=int, default=36,
+                        help='static number of bounding boxes')
+
+    #### Training Params ####
+    # Named parameters
+    parser.add_argument('--optimizer', type=str, default=defaults.get('optimizer', 'adam'),
+                        help='Optimizer to use for training: adam / adamx / adamw')
+    parser.add_argument('--loss_func', type=str, default=defaults.get('loss_func', 'bce_logits'),
+                        help='Loss function to use for optimization: bce / bce_logits / ce')
+    parser.add_argument('--optimize_for', type=str, default=defaults.get('optimize_for', 'aucroc'),
+                        help='Optimize for what measure during training and early stopping: loss / F1 / aucroc / accuracy')
+    parser.add_argument('--scheduler', type=str, default=defaults.get('scheduler', 'warmup_cosine'),
+                        help='The type of lr scheduler to use anneal learning rate: step/multi_step/warmup/warmp_cosine')
+
+    # Numerical parameters
+    parser.add_argument('--beta1', type=float, default=defaults.get('beta1', 0.9),
+                        help='beta1 parameter in Adam optimizer')
+    parser.add_argument('--beta2', type=float, default=defaults.get('beta2', 0.999),
+                        help='beta2 parameter in Adam optimizer')
+    parser.add_argument('--batch_size', type=int, default=defaults.get('batch_size', 8),
+                        help='batch size for training')
+    parser.add_argument('--num_workers', type=int, default=defaults.get('num_workers', 0),
+                        help='Number of workers to start per dataset')
+    parser.add_argument('--gradient_accumulation', type=int, default=defaults.get('gradient_accumulation', 1),
+                        help='No. of update steps to accumulate before performing backward pass')
+    parser.add_argument('--max_grad_norm', type=int, default=defaults.get('max_grad_norm', 5),
+                        help='max gradient norm for gradient clipping')
+    parser.add_argument('--pos_wt', type=float, default=defaults.get('pos_wt', 1),
+                        help='Loss reweighting for the positive class to deal with class imbalance')
+    parser.add_argument('--lr', type=float, default=defaults.get('lr', 1e-4),
+                        help='Learning rate for training')
+    parser.add_argument('--warmup_steps', type=int, default=defaults.get('warmup_steps', 50),
+                        help='No. of steps to perform linear lr warmup for')
+    parser.add_argument('--weight_decay', type=float, default=defaults.get('weight_decay', 1e-3),
+                        help='weight decay for optimizer')
+    parser.add_argument('--max_epoch', type=int, default=defaults.get('max_epoch', 20),
+                        help='Max epochs to train for')
+    parser.add_argument('--lr_decay_step', type=float, default=defaults.get('lr_decay_step', 3),
+                        help='No. of epochs after which learning rate should be decreased')
+    parser.add_argument('--lr_decay_factor', type=float, default=defaults.get('lr_decay_factor', 0.8),
+                        help='Decay the learning rate of the optimizer by this multiplicative amount')
+    parser.add_argument('--patience', type=float, default=defaults.get('patience', 5),
+                        help='Patience no. of epochs for early stopping')
+    parser.add_argument('--early_stop_thresh', type=float, default=defaults.get('early_stop_thresh', 1e-3),
+                        help='Patience no. of epochs for early stopping')
+    parser.add_argument('--seed', type=int, default=defaults.get('seed', 42),
+                        help='set seed for reproducability')
+    parser.add_argument('--log_every', type=int, default=defaults.get('log_every', 2000),
+                        help='Log stats in Tensorboard every x iterations (not epochs) of training')
+    parser.add_argument('--fc_dim', type=int, default=64,
+                        help='dimen of FC layer"')
+    parser.add_argument('--dropout', type=float, default=0.2,
+                        help='Standard dropout regularization')
+
+    # New parameters by team
+    parser.add_argument('--filter_text', action='store_true',
+                        help='Filter out bounding boxes around text')
+    parser.add_argument('--no_normalize_img', action='store_false',
+                        help='Normalize images by dividing them by their height and width. Default=True')
+    parser.add_argument('--train_filename', type=str, default='train.jsonl',
+                        help='The name of the trainin json file to load.')
+    parser.add_argument('--upsample_multiplier', type=int, default=0,
+                        help='Multiplier used to increase the amount of confounders in training data')
+    parser.add_argument('--note', type=str, default='',
+                        help='Add a note that can be seen in wandb')
+    args, unparsed = parser.parse_known_args()
+    config = args.__dict__
+    wandb.config.update(config)
+    config['device'] = get_device()
+    config['n_classes'] = 2 if config['loss_func'] == 'ce' else 1
+
+    # Check all provided paths:
+    if not os.path.exists(config['data_path']):
+        raise ValueError("[!] ERROR: Dataset path does not exist")
+    else:
+        LOGGER.info("Data path checked..")
+    if not os.path.exists(config['model_path']):
+        LOGGER.warning("Creating checkpoint path for saved models at:  {}\n".format(
+            config['model_path']))
+        os.makedirs(config['model_path'])
+    else:
+        LOGGER.info("Model save path checked..")
+    if 'config' in config:
+        if not os.path.isfile(config['config']):
+            raise ValueError("[!] ERROR: config JSON path does not exist")
+        else:
+            LOGGER.info("config JSON path checked..")
+    if not os.path.exists(config['vis_path']):
+        LOGGER.warning("Creating checkpoint path for Tensorboard visualizations at:  {}\n".format(
+            config['vis_path']))
+        os.makedirs(config['vis_path'])
+    else:
+        LOGGER.info("Tensorboard Visualization path checked..")
+        LOGGER.info(
+            "Cleaning Visualization path of older tensorboard files...\n")
+        # shutil.rmtree(config['vis_path'])
+
+    # Print args
+    print("\n" + "x"*50 + "\n\nRunning training with the following parameters: \n")
+    for key, value in config.items():
+        if not key.endswith('transf'):
+            print(key + ' : ' + str(value))
+    print("\n" + "x"*50)
+
+    config['writer'] = SummaryWriter(config['vis_path'])
+
+    set_seed(config['seed'])
+
+    # Tokenize
+    tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
+    tokenizer_func = partial(tokenizer, max_length=config['max_txt_len'], padding='max_length',
+                             truncation=True, return_tensors='pt', return_length=True)
+
+    # Prepare the datasets and dataloaders for training and evaluation
+    train_dataset = MemeDataset(filepath=os.path.join(config['data_path'], config['train_filename']),
+                                feature_dir=config['feature_path'], text_padding=tokenizer_func, filter_text=config["filter_text"],
+                                upsample_multiplier=config["upsample_multiplier"])
+    val_dataset = MemeDataset(filepath=os.path.join(config['data_path'], 'dev_seen.jsonl'),
+                              feature_dir=config['feature_path'], text_padding=tokenizer_func, filter_text=config["filter_text"])
+    test_dataset = MemeDataset(filepath=os.path.join(config['data_path'], 'test_seen.jsonl'),
+                               feature_dir=config['feature_path'], text_padding=tokenizer_func, filter_text=config["filter_text"])
+
+    config['train_loader'] = data.DataLoader(train_dataset, batch_size=config['batch_size'],
+                                             num_workers=config['num_workers'], collate_fn=train_dataset.get_collate_fn(), shuffle=True, pin_memory=True)
+    config['val_loader'] = data.DataLoader(val_dataset, batch_size=config['batch_size'],
+                                           num_workers=config['num_workers'], collate_fn=val_dataset.get_collate_fn())
+    config['test_loader'] = data.DataLoader(test_dataset, batch_size=config['batch_size'],
+                                            num_workers=config['num_workers'], collate_fn=test_dataset.get_collate_fn())
+
+    try:
+        trainer = TrainerUniter(config)
+        trainer.train_main()
+        wandb.save('vis_checkpoints/*', base_path="vis_checkpoints/")
+        wandb.finish()
+    except KeyboardInterrupt:
+        LOGGER.warning(
+            "Keyboard interrupt by user detected...\nClosing the tensorboard writer!")
+        config['writer'].close()
diff --git a/wandb/run-20210223_112549-3ck5aive/files/config.yaml b/wandb/run-20210223_112549-3ck5aive/files/config.yaml
new file mode 100644
index 0000000..674a8a3
--- /dev/null
+++ b/wandb/run-20210223_112549-3ck5aive/files/config.yaml
@@ -0,0 +1,151 @@
+wandb_version: 1
+
+_wandb:
+  desc: null
+  value:
+    cli_version: 0.10.19
+    code_path: code/train_uniter.py
+    framework: huggingface
+    huggingface_version: 3.2.0
+    is_jupyter_run: false
+    is_kaggle_kernel: false
+    python_version: 3.7.5
+    t:
+      1:
+      - 1
+      - 3
+      - 5
+      - 11
+      2:
+      - 1
+      - 3
+      - 5
+      - 11
+      3:
+      - 1
+      4: 3.7.5
+      5: 0.10.19
+      6: 3.2.0
+batch_size:
+  desc: null
+  value: 16
+beta1:
+  desc: null
+  value: 0.9
+beta2:
+  desc: null
+  value: 0.999
+config:
+  desc: null
+  value: config/uniter-base.json
+data_path:
+  desc: null
+  value: ./dataset
+dropout:
+  desc: null
+  value: 0.2
+early_stop_thresh:
+  desc: null
+  value: 0.001
+fc_dim:
+  desc: null
+  value: 64
+feature_path:
+  desc: null
+  value: ./dataset/own_features
+filter_text:
+  desc: null
+  value: false
+gradient_accumulation:
+  desc: null
+  value: 2
+log_every:
+  desc: null
+  value: 2000
+loss_func:
+  desc: null
+  value: bce_logits
+lr:
+  desc: null
+  value: 3.0e-05
+lr_decay_factor:
+  desc: null
+  value: 0.8
+lr_decay_step:
+  desc: null
+  value: 3
+max_bb:
+  desc: null
+  value: 100
+max_epoch:
+  desc: null
+  value: 30
+max_grad_norm:
+  desc: null
+  value: 5
+max_txt_len:
+  desc: null
+  value: 60
+min_bb:
+  desc: null
+  value: 10
+model_path:
+  desc: null
+  value: ./model_checkpoints
+model_save_name:
+  desc: null
+  value: meme.pt
+no_model_checkpoints:
+  desc: null
+  value: false
+no_normalize_img:
+  desc: null
+  value: true
+note:
+  desc: null
+  value: ''
+num_bb:
+  desc: null
+  value: 36
+num_workers:
+  desc: null
+  value: 0
+optimize_for:
+  desc: null
+  value: aucroc
+optimizer:
+  desc: null
+  value: adam
+patience:
+  desc: null
+  value: 5.0
+pos_wt:
+  desc: null
+  value: 1.0
+pretrained_model_file:
+  desc: null
+  value: uniter-base.pt
+remove_checkpoints:
+  desc: null
+  value: false
+scheduler:
+  desc: null
+  value: warmup_cosine
+seed:
+  desc: null
+  value: 43
+train_filename:
+  desc: null
+  value: train.jsonl
+upsample_multiplier:
+  desc: null
+  value: 0
+vis_path:
+  desc: null
+  value: ./vis_checkpoints
+warmup_steps:
+  desc: null
+  value: 500
+weight_decay:
+  desc: null
+  value: 0.001
diff --git a/wandb/run-20210223_112549-3ck5aive/files/diff.patch b/wandb/run-20210223_112549-3ck5aive/files/diff.patch
new file mode 100644
index 0000000..bc81c12
--- /dev/null
+++ b/wandb/run-20210223_112549-3ck5aive/files/diff.patch
@@ -0,0 +1,61 @@
+diff --git a/data/meme_dataset.py b/data/meme_dataset.py
+index 3e9ee53..4df775c 100644
+--- a/data/meme_dataset.py
++++ b/data/meme_dataset.py
+@@ -201,7 +201,7 @@ class MemeDataset(data.Dataset):
+                               for json_dict in f.readlines()]
+         print("Loaded dataset contains ", str(len(self.json_list)), "samples")
+         self._load_dataset()
+-        self._load_gender_race_preds()
++        self._load_gender_race_probs()
+     
+ 
+     def _load_dataset(self):
+@@ -242,7 +242,7 @@ class MemeDataset(data.Dataset):
+             self.data.text = self.text_preprocess(self.data.text)
+ 
+     def _load_gender_race_probs(self):
+-        with open(f'../dataset/gender_race_preds/{self.name}_gender_race_probs.pickle', 'rb') as f:
++        with open(f'dataset/gender_race_probs/{self.name}_gender_race_probs.pickle', 'rb') as f:
+             self.data.gender_race_probs = pickle.load(f)
+ 
+     def __len__(self):
+diff --git a/model/meme_uniter.py b/model/meme_uniter.py
+index 7844280..ffb05a0 100644
+--- a/model/meme_uniter.py
++++ b/model/meme_uniter.py
+@@ -24,6 +24,7 @@ class MemeUniter(nn.Module):
+     def forward(self, **kwargs):
+         out = self.uniter_model(**kwargs)
+         out = self.uniter_model.pooler(out)
++        print(kwargs)
+         gender_race_probs = kwargs["gender_race_probs"]
+         out = torch.cat((out, gender_race_probs), 1) # concatenate the uniter output with gender and race probabilities
+         out = self.linear_1(out)
+diff --git a/wandb/debug-internal.log b/wandb/debug-internal.log
+index f09c7de..a14f5e5 120000
+--- a/wandb/debug-internal.log
++++ b/wandb/debug-internal.log
+@@ -1 +1 @@
+-run-20210222_215454-1602o824/logs/debug-internal.log
+\ No newline at end of file
++run-20210223_112549-3ck5aive/logs/debug-internal.log
+\ No newline at end of file
+diff --git a/wandb/debug.log b/wandb/debug.log
+index 9d13059..4c296df 120000
+--- a/wandb/debug.log
++++ b/wandb/debug.log
+@@ -1 +1 @@
+-run-20210222_215454-1602o824/logs/debug.log
+\ No newline at end of file
++run-20210223_112549-3ck5aive/logs/debug.log
+\ No newline at end of file
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 15abb24..b3cdce7 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20210222_215454-1602o824
+\ No newline at end of file
++run-20210223_112549-3ck5aive
+\ No newline at end of file
diff --git a/wandb/run-20210223_112549-3ck5aive/files/events.out.tfevents.1614075950.astro.1551487.0 b/wandb/run-20210223_112549-3ck5aive/files/events.out.tfevents.1614075950.astro.1551487.0
new file mode 120000
index 0000000..3ec1c26
--- /dev/null
+++ b/wandb/run-20210223_112549-3ck5aive/files/events.out.tfevents.1614075950.astro.1551487.0
@@ -0,0 +1 @@
+/home/astro/Documents/UvA/Block 4 - NLP2/Multimodal NLP/Multimodal-NLP/vis_checkpoints/events.out.tfevents.1614075950.astro.1551487.0
\ No newline at end of file
diff --git a/wandb/run-20210223_112549-3ck5aive/files/output.log b/wandb/run-20210223_112549-3ck5aive/files/output.log
new file mode 100644
index 0000000..717523d
--- /dev/null
+++ b/wandb/run-20210223_112549-3ck5aive/files/output.log
@@ -0,0 +1,407 @@
+23/02/2021 11:25:50 AM : INFO - Data path checked..
+
+xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
+
+Running training with the following parameters: 
+23/02/2021 11:25:50 AM : INFO - Model save path checked..
+23/02/2021 11:25:50 AM : INFO - config JSON path checked..
+23/02/2021 11:25:50 AM : INFO - Tensorboard Visualization path checked..
+23/02/2021 11:25:50 AM : INFO - Cleaning Visualization path of older tensorboard files...
+
+
+data_path : ./dataset
+model_path : ./model_checkpoints
+vis_path : ./vis_checkpoints
+model_save_name : meme.pt
+no_model_checkpoints : False
+remove_checkpoints : False
+config : config/uniter-base.json
+feature_path : ./dataset/own_features
+pretrained_model_file : uniter-base.pt
+max_txt_len : 60
+max_bb : 100
+min_bb : 10
+num_bb : 36
+optimizer : adam
+loss_func : bce_logits
+optimize_for : aucroc
+scheduler : warmup_cosine
+beta1 : 0.9
+beta2 : 0.999
+batch_size : 16
+num_workers : 0
+gradient_accumulation : 2
+max_grad_norm : 5
+pos_wt : 1.0
+lr : 3e-05
+warmup_steps : 500
+weight_decay : 0.001
+max_epoch : 30
+lr_decay_step : 3
+lr_decay_factor : 0.8
+patience : 5.0
+early_stop_thresh : 0.001
+seed : 43
+log_every : 2000
+fc_dim : 64
+dropout : 0.2
+filter_text : False
+no_normalize_img : True
+train_filename : train.jsonl
+upsample_multiplier : 0
+note : 
+device : cuda
+n_classes : 1
+
+xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
+filter text False
+Loaded dataset contains  8500 samples
+filter text False
+Loaded dataset contains  500 samples
+filter text False
+Loaded dataset contains  1000 samples
+23/02/2021 11:25:57 AM : INFO - Using pretrained UNITER base model ./model_checkpoints/uniter-base.pt
+23/02/2021 11:25:57 AM : INFO - Model config {
+  "attention_probs_dropout_prob": 0.1,
+  "hidden_act": "gelu",
+  "hidden_dropout_prob": 0.1,
+  "hidden_size": 768,
+  "initializer_range": 0.02,
+  "intermediate_size": 3072,
+  "max_position_embeddings": 512,
+  "num_attention_heads": 12,
+  "num_hidden_layers": 12,
+  "type_vocab_size": 2,
+  "vocab_size": 28996
+}
+
+MemeUniter
+MemeUniter(
+  (uniter_model): UniterModel(
+    (embeddings): UniterTextEmbeddings(
+      (word_embeddings): Embedding(28996, 768, padding_idx=0)
+      (position_embeddings): Embedding(512, 768)
+      (token_type_embeddings): Embedding(2, 768)
+      (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+      (dropout): Dropout(p=0.1, inplace=False)
+    )
+    (img_embeddings): UniterImageEmbeddings(
+      (img_linear): Linear(in_features=2048, out_features=768, bias=True)
+      (img_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+      (pos_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+      (pos_linear): Linear(in_features=7, out_features=768, bias=True)
+      (mask_embedding): Embedding(2, 2048, padding_idx=0)
+      (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+      (dropout): Dropout(p=0.1, inplace=False)
+    )
+    (encoder): UniterEncoder(
+      (layer): ModuleList(
+        (0): BertLayer(
+          (attention): BertAttention(
+            (self): BertSelfAttention(
+              (query): Linear(in_features=768, out_features=768, bias=True)
+              (key): Linear(in_features=768, out_features=768, bias=True)
+              (value): Linear(in_features=768, out_features=768, bias=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+            (output): BertSelfOutput(
+              (dense): Linear(in_features=768, out_features=768, bias=True)
+              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+          )
+          (intermediate): BertIntermediate(
+            (dense): Linear(in_features=768, out_features=3072, bias=True)
+          )
+          (output): BertOutput(
+            (dense): Linear(in_features=3072, out_features=768, bias=True)
+            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+            (dropout): Dropout(p=0.1, inplace=False)
+          )
+        )
+        (1): BertLayer(
+          (attention): BertAttention(
+            (self): BertSelfAttention(
+              (query): Linear(in_features=768, out_features=768, bias=True)
+              (key): Linear(in_features=768, out_features=768, bias=True)
+              (value): Linear(in_features=768, out_features=768, bias=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+            (output): BertSelfOutput(
+              (dense): Linear(in_features=768, out_features=768, bias=True)
+              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+          )
+          (intermediate): BertIntermediate(
+            (dense): Linear(in_features=768, out_features=3072, bias=True)
+          )
+          (output): BertOutput(
+            (dense): Linear(in_features=3072, out_features=768, bias=True)
+            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+            (dropout): Dropout(p=0.1, inplace=False)
+          )
+        )
+        (2): BertLayer(
+          (attention): BertAttention(
+            (self): BertSelfAttention(
+              (query): Linear(in_features=768, out_features=768, bias=True)
+              (key): Linear(in_features=768, out_features=768, bias=True)
+              (value): Linear(in_features=768, out_features=768, bias=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+            (output): BertSelfOutput(
+              (dense): Linear(in_features=768, out_features=768, bias=True)
+              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+          )
+          (intermediate): BertIntermediate(
+            (dense): Linear(in_features=768, out_features=3072, bias=True)
+          )
+          (output): BertOutput(
+            (dense): Linear(in_features=3072, out_features=768, bias=True)
+            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+            (dropout): Dropout(p=0.1, inplace=False)
+          )
+        )
+        (3): BertLayer(
+          (attention): BertAttention(
+            (self): BertSelfAttention(
+              (query): Linear(in_features=768, out_features=768, bias=True)
+              (key): Linear(in_features=768, out_features=768, bias=True)
+              (value): Linear(in_features=768, out_features=768, bias=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+            (output): BertSelfOutput(
+              (dense): Linear(in_features=768, out_features=768, bias=True)
+              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+          )
+          (intermediate): BertIntermediate(
+            (dense): Linear(in_features=768, out_features=3072, bias=True)
+          )
+          (output): BertOutput(
+            (dense): Linear(in_features=3072, out_features=768, bias=True)
+            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+            (dropout): Dropout(p=0.1, inplace=False)
+          )
+        )
+        (4): BertLayer(
+          (attention): BertAttention(
+            (self): BertSelfAttention(
+              (query): Linear(in_features=768, out_features=768, bias=True)
+              (key): Linear(in_features=768, out_features=768, bias=True)
+              (value): Linear(in_features=768, out_features=768, bias=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+            (output): BertSelfOutput(
+              (dense): Linear(in_features=768, out_features=768, bias=True)
+              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+          )
+          (intermediate): BertIntermediate(
+            (dense): Linear(in_features=768, out_features=3072, bias=True)
+          )
+          (output): BertOutput(
+            (dense): Linear(in_features=3072, out_features=768, bias=True)
+            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+            (dropout): Dropout(p=0.1, inplace=False)
+          )
+        )
+        (5): BertLayer(
+          (attention): BertAttention(
+            (self): BertSelfAttention(
+              (query): Linear(in_features=768, out_features=768, bias=True)
+              (key): Linear(in_features=768, out_features=768, bias=True)
+              (value): Linear(in_features=768, out_features=768, bias=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+            (output): BertSelfOutput(
+              (dense): Linear(in_features=768, out_features=768, bias=True)
+              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+          )
+          (intermediate): BertIntermediate(
+            (dense): Linear(in_features=768, out_features=3072, bias=True)
+          )
+          (output): BertOutput(
+            (dense): Linear(in_features=3072, out_features=768, bias=True)
+            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+            (dropout): Dropout(p=0.1, inplace=False)
+          )
+        )
+        (6): BertLayer(
+          (attention): BertAttention(
+            (self): BertSelfAttention(
+              (query): Linear(in_features=768, out_features=768, bias=True)
+              (key): Linear(in_features=768, out_features=768, bias=True)
+              (value): Linear(in_features=768, out_features=768, bias=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+            (output): BertSelfOutput(
+              (dense): Linear(in_features=768, out_features=768, bias=True)
+              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+          )
+          (intermediate): BertIntermediate(
+            (dense): Linear(in_features=768, out_features=3072, bias=True)
+          )
+          (output): BertOutput(
+            (dense): Linear(in_features=3072, out_features=768, bias=True)
+            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+            (dropout): Dropout(p=0.1, inplace=False)
+          )
+        )
+        (7): BertLayer(
+          (attention): BertAttention(
+            (self): BertSelfAttention(
+              (query): Linear(in_features=768, out_features=768, bias=True)
+              (key): Linear(in_features=768, out_features=768, bias=True)
+              (value): Linear(in_features=768, out_features=768, bias=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+            (output): BertSelfOutput(
+              (dense): Linear(in_features=768, out_features=768, bias=True)
+              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+          )
+          (intermediate): BertIntermediate(
+            (dense): Linear(in_features=768, out_features=3072, bias=True)
+          )
+          (output): BertOutput(
+            (dense): Linear(in_features=3072, out_features=768, bias=True)
+            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+            (dropout): Dropout(p=0.1, inplace=False)
+          )
+        )
+        (8): BertLayer(
+          (attention): BertAttention(
+            (self): BertSelfAttention(
+              (query): Linear(in_features=768, out_features=768, bias=True)
+              (key): Linear(in_features=768, out_features=768, bias=True)
+              (value): Linear(in_features=768, out_features=768, bias=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+            (output): BertSelfOutput(
+              (dense): Linear(in_features=768, out_features=768, bias=True)
+              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+          )
+          (intermediate): BertIntermediate(
+            (dense): Linear(in_features=768, out_features=3072, bias=True)
+          )
+          (output): BertOutput(
+            (dense): Linear(in_features=3072, out_features=768, bias=True)
+            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+            (dropout): Dropout(p=0.1, inplace=False)
+          )
+        )
+        (9): BertLayer(
+          (attention): BertAttention(
+            (self): BertSelfAttention(
+              (query): Linear(in_features=768, out_features=768, bias=True)
+              (key): Linear(in_features=768, out_features=768, bias=True)
+              (value): Linear(in_features=768, out_features=768, bias=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+            (output): BertSelfOutput(
+              (dense): Linear(in_features=768, out_features=768, bias=True)
+              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+          )
+          (intermediate): BertIntermediate(
+            (dense): Linear(in_features=768, out_features=3072, bias=True)
+          )
+          (output): BertOutput(
+            (dense): Linear(in_features=3072, out_features=768, bias=True)
+            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+            (dropout): Dropout(p=0.1, inplace=False)
+          )
+        )
+        (10): BertLayer(
+          (attention): BertAttention(
+            (self): BertSelfAttention(
+              (query): Linear(in_features=768, out_features=768, bias=True)
+              (key): Linear(in_features=768, out_features=768, bias=True)
+              (value): Linear(in_features=768, out_features=768, bias=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+            (output): BertSelfOutput(
+              (dense): Linear(in_features=768, out_features=768, bias=True)
+              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+          )
+          (intermediate): BertIntermediate(
+            (dense): Linear(in_features=768, out_features=3072, bias=True)
+          )
+          (output): BertOutput(
+            (dense): Linear(in_features=3072, out_features=768, bias=True)
+            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+            (dropout): Dropout(p=0.1, inplace=False)
+          )
+        )
+        (11): BertLayer(
+          (attention): BertAttention(
+            (self): BertSelfAttention(
+              (query): Linear(in_features=768, out_features=768, bias=True)
+              (key): Linear(in_features=768, out_features=768, bias=True)
+              (value): Linear(in_features=768, out_features=768, bias=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+            (output): BertSelfOutput(
+              (dense): Linear(in_features=768, out_features=768, bias=True)
+              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+              (dropout): Dropout(p=0.1, inplace=False)
+            )
+          )
+          (intermediate): BertIntermediate(
+            (dense): Linear(in_features=768, out_features=3072, bias=True)
+          )
+          (output): BertOutput(
+            (dense): Linear(in_features=3072, out_features=768, bias=True)
+            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
+            (dropout): Dropout(p=0.1, inplace=False)
+          )
+        )
+      )
+    )
+    (pooler): BertPooler(
+      (dense): Linear(in_features=768, out_features=768, bias=True)
+      (activation): Tanh()
+    )
+  )
+  (linear_1): Linear(in_features=768, out_features=384, bias=True)
+  (activation_1): ReLU()
+  (linear_2): Linear(in_features=384, out_features=1, bias=True)
+)
+
+
+====================================================================================================
+					 Training Network
+====================================================================================================
+
+Beginning training at:  2021-02-23 11:26:01.262159 
+
+Traceback (most recent call last):
+  File "train_uniter.py", line 648, in <module>
+    trainer.train_main()
+  File "train_uniter.py", line 432, in train_main
+    self.train_iter_step()
+  File "train_uniter.py", line 470, in train_iter_step
+    output_all_encoded_layers=False, gender_race_probs=self.batch['gender_race_probs'])
+  File "/home/astro/anaconda3/envs/nlp2-multimodal/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
+    result = self.forward(*input, **kwargs)
+  File "/home/astro/Documents/UvA/Block 4 - NLP2/Multimodal NLP/Multimodal-NLP/model/meme_uniter.py", line 25, in forward
+    out = self.uniter_model(**kwargs)
+  File "/home/astro/anaconda3/envs/nlp2-multimodal/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
+    result = self.forward(*input, **kwargs)
+TypeError: forward() got an unexpected keyword argument 'gender_race_probs'
diff --git a/wandb/run-20210223_112549-3ck5aive/files/requirements.txt b/wandb/run-20210223_112549-3ck5aive/files/requirements.txt
new file mode 100644
index 0000000..0d8abf6
--- /dev/null
+++ b/wandb/run-20210223_112549-3ck5aive/files/requirements.txt
@@ -0,0 +1,176 @@
+absl-py==0.10.0
+aiohttp==3.6.2
+apex==0.1
+argon2-cffi==20.1.0
+astor==0.8.1
+astunparse==1.6.3
+async-generator==1.10
+async-timeout==3.0.1
+attrs==20.2.0
+autopep8==1.5.5
+backcall==0.2.0
+beautifulsoup4==4.9.2
+bleach==3.2.1
+boto3==1.15.6
+botocore==1.18.6
+cached-property==1.5.2
+cachetools==4.1.1
+certifi==2020.6.20
+cffi==1.14.3
+chardet==3.0.4
+click==7.1.2
+configparser==5.0.1
+cycler==0.10.0
+dbfread==2.0.4
+deap==1.3.1
+decorator==4.4.2
+defusedxml==0.6.0
+demjson==2.2.4
+dlib==19.21.1
+docker-pycreds==0.4.0
+editdistance==0.5.3
+entrypoints==0.3
+fasttext==0.9.1
+filelock==3.0.12
+flask==1.1.2
+flatbuffers==1.12
+future==0.18.2
+gast==0.3.3
+gdown==3.12.2
+gitdb==4.0.5
+gitpython==3.1.0
+google-auth-oauthlib==0.4.1
+google-auth==1.22.0
+google-pasta==0.2.0
+gql==2.0.0
+graphql-core==2.3.2
+grpcio==1.32.0
+h5py==2.10.0
+idna==2.10
+ijson==2.6.1
+imagehash==4.2.0
+importlib-metadata==2.0.0
+ipykernel==5.3.4
+ipython-genutils==0.2.0
+ipython==7.20.0
+ipywidgets==7.5.1
+itsdangerous==1.1.0
+jedi==0.17.2
+jinja2==2.11.2
+jmespath==0.10.0
+joblib==0.16.0
+jsonschema==3.2.0
+jupyter-client==6.1.7
+jupyter-core==4.6.3
+jupyterlab-pygments==0.1.2
+keras-applications==1.0.8
+keras-preprocessing==1.1.2
+keras==2.4.3
+kiwisolver==1.2.0
+lightgbm==3.1.1
+lmdb==0.98
+markdown==3.2.2
+markupsafe==1.1.1
+matplotlib==3.3.2
+mdbtools==0.3.14
+meza==0.42.5
+mistune==0.8.4
+mtcnn==0.1.0
+multidict==4.7.6
+nbclient==0.5.0
+nbconvert==6.0.7
+nbformat==5.0.7
+nest-asyncio==1.4.1
+nltk==3.4.5
+notebook==6.1.4
+numpy==1.19.2
+nvidia-ml-py3==7.352.0
+oauthlib==3.1.0
+omegaconf==2.0.1rc4
+opencv-contrib-python==4.5.1.48
+opencv-python==4.5.1.48
+opt-einsum==3.3.0
+packaging==20.4
+pandas==1.1.2
+pandocfilters==1.4.2
+parso==0.7.1
+pathtools==0.1.2
+pexpect==4.8.0
+pickleshare==0.7.5
+pillow==7.2.0
+pip==20.3.3
+prometheus-client==0.8.0
+promise==2.3
+prompt-toolkit==3.0.7
+protobuf==3.13.0
+psutil==5.8.0
+ptyprocess==0.6.0
+pyasn1-modules==0.2.8
+pyasn1==0.4.8
+pybind11==2.6.2
+pycodestyle==2.6.0
+pycparser==2.20
+pygments==2.7.1
+pygogo==0.13.2
+pymongo==3.11.0
+pyparsing==2.4.7
+pyrsistent==0.17.3
+pysocks==1.7.1
+python-dateutil==2.8.1
+python-slugify==1.2.6
+pytz==2020.1
+pywavelets==1.1.1
+pyyaml==5.3.1
+pyzmq==19.0.2
+regex==2020.9.27
+requests-oauthlib==1.3.0
+requests==2.23.0
+rsa==4.6
+rx==1.6.1
+s3transfer==0.3.3
+sacremoses==0.0.43
+scikit-learn==0.23.2
+scipy==1.5.2
+seaborn==0.11.0
+send2trash==1.5.0
+sentencepiece==0.1.91
+sentry-sdk==0.20.3
+setuptools==52.0.0.post20210125
+shortuuid==1.0.1
+six==1.15.0
+sklearn==0.0
+smmap==3.0.4
+soupsieve==2.0.1
+subprocess32==3.5.4
+tensorboard-plugin-wit==1.7.0
+tensorboard==2.4.1
+tensorflow-estimator==2.4.0
+tensorflow==2.4.1
+termcolor==1.1.0
+terminado==0.9.1
+testpath==0.4.4
+threadpoolctl==2.1.0
+tokenizers==0.8.1rc2
+toml==0.10.2
+toolz==0.11.1
+torch==1.6.0+cu101
+torchtext==0.5.0
+torchvision==0.7.0+cu101
+tornado==6.0.4
+tqdm==4.50.0
+traitlets==5.0.5
+transformers==3.2.0
+typing-extensions==3.7.4.3
+unidecode==1.1.1
+urllib3==1.25.10
+wandb==0.10.19
+watchdog==2.0.1
+wcwidth==0.2.5
+webencodings==0.5.1
+werkzeug==1.0.1
+wheel==0.36.2
+widgetsnbextension==3.5.1
+wrapt==1.12.1
+xlrd==1.2.0
+yarl==1.6.0
+zipp==3.2.0
\ No newline at end of file
diff --git a/wandb/run-20210223_112549-3ck5aive/files/wandb-metadata.json b/wandb/run-20210223_112549-3ck5aive/files/wandb-metadata.json
new file mode 100644
index 0000000..8641f31
--- /dev/null
+++ b/wandb/run-20210223_112549-3ck5aive/files/wandb-metadata.json
@@ -0,0 +1,55 @@
+{
+    "os": "Linux-5.8.0-43-generic-x86_64-with-debian-bullseye-sid",
+    "python": "3.7.5",
+    "heartbeatAt": "2021-02-23T10:25:50.447239",
+    "startedAt": "2021-02-23T10:25:49.037171",
+    "docker": null,
+    "gpu": "GeForce RTX 2060 SUPER",
+    "gpu_count": 1,
+    "cpu_count": 24,
+    "cuda": "11.0.228",
+    "args": [
+        "--config",
+        "config/uniter-base.json",
+        "--data_path",
+        "./dataset",
+        "--model_path",
+        "./model_checkpoints",
+        "--pretrained_model_file",
+        "uniter-base.pt",
+        "--feature_path",
+        "./dataset/own_features",
+        "--lr",
+        "3e-5",
+        "--scheduler",
+        "warmup_cosine",
+        "--warmup_steps",
+        "500",
+        "--max_epoch",
+        "30",
+        "--batch_size",
+        "16",
+        "--patience",
+        "5",
+        "--gradient_accumulation",
+        "2",
+        "--model_save_name",
+        "meme.pt",
+        "--seed",
+        "43",
+        "--pos_wt",
+        "1"
+    ],
+    "state": "running",
+    "codePath": "train_uniter.py",
+    "program": "train_uniter.py",
+    "git": {
+        "remote": "https://github.com/Noixas/Multimodal-NLP.git",
+        "commit": "83666a1f2f64375554a1633e19e39a7c7cef964b"
+    },
+    "email": "rodrigo.mulsa@outlook.com",
+    "root": "/home/astro/Documents/UvA/Block 4 - NLP2/Multimodal NLP/Multimodal-NLP",
+    "host": "astro",
+    "username": "astro",
+    "executable": "/home/astro/anaconda3/envs/nlp2-multimodal/bin/python"
+}
diff --git a/wandb/run-20210223_112549-3ck5aive/files/wandb-summary.json b/wandb/run-20210223_112549-3ck5aive/files/wandb-summary.json
new file mode 100644
index 0000000..9e26dfe
--- /dev/null
+++ b/wandb/run-20210223_112549-3ck5aive/files/wandb-summary.json
@@ -0,0 +1 @@
+{}
\ No newline at end of file
diff --git a/wandb/run-20210223_112549-3ck5aive/logs/debug-internal.log b/wandb/run-20210223_112549-3ck5aive/logs/debug-internal.log
new file mode 100644
index 0000000..a2dfa36
--- /dev/null
+++ b/wandb/run-20210223_112549-3ck5aive/logs/debug-internal.log
@@ -0,0 +1,121 @@
+2021-02-23 11:25:49,412 INFO    MainThread:1551549 [internal.py:wandb_internal():91] W&B internal server running at pid: 1551549, started at: 2021-02-23 11:25:49.412101
+2021-02-23 11:25:49,413 DEBUG   HandlerThread:1551549 [handler.py:handle_request():94] handle_request: check_version
+2021-02-23 11:25:49,413 INFO    WriterThread:1551549 [datastore.py:open_for_write():77] open: /home/astro/Documents/UvA/Block 4 - NLP2/Multimodal NLP/Multimodal-NLP/wandb/run-20210223_112549-3ck5aive/run-3ck5aive.wandb
+2021-02-23 11:25:49,414 DEBUG   SenderThread:1551549 [sender.py:send():117] send: header
+2021-02-23 11:25:49,414 DEBUG   SenderThread:1551549 [sender.py:send():117] send: request
+2021-02-23 11:25:49,414 DEBUG   SenderThread:1551549 [sender.py:send_request():126] send_request: check_version
+2021-02-23 11:25:49,446 DEBUG   SenderThread:1551549 [sender.py:send():117] send: run
+2021-02-23 11:25:49,627 INFO    SenderThread:1551549 [sender.py:_start_run_threads():596] run started: 3ck5aive with start time 1614075949
+2021-02-23 11:25:49,627 DEBUG   SenderThread:1551549 [sender.py:send():117] send: summary
+2021-02-23 11:25:49,627 DEBUG   HandlerThread:1551549 [handler.py:handle_request():94] handle_request: run_start
+2021-02-23 11:25:49,628 INFO    SenderThread:1551549 [sender.py:_save_file():682] saving file wandb-summary.json with policy end
+2021-02-23 11:25:50,447 DEBUG   HandlerThread:1551549 [meta.py:__init__():34] meta init
+2021-02-23 11:25:50,447 DEBUG   HandlerThread:1551549 [meta.py:__init__():48] meta init done
+2021-02-23 11:25:50,447 DEBUG   HandlerThread:1551549 [meta.py:probe():190] probe
+2021-02-23 11:25:50,453 DEBUG   HandlerThread:1551549 [meta.py:_setup_git():180] setup git
+2021-02-23 11:25:50,471 DEBUG   HandlerThread:1551549 [meta.py:_setup_git():187] setup git done
+2021-02-23 11:25:50,471 DEBUG   HandlerThread:1551549 [meta.py:_save_code():69] save code
+2021-02-23 11:25:50,480 DEBUG   HandlerThread:1551549 [meta.py:_save_code():90] save code done
+2021-02-23 11:25:50,480 DEBUG   HandlerThread:1551549 [meta.py:_save_patches():107] save patches
+2021-02-23 11:25:50,587 DEBUG   HandlerThread:1551549 [meta.py:_save_patches():149] save patches done
+2021-02-23 11:25:50,587 DEBUG   HandlerThread:1551549 [meta.py:_save_pip():52] save pip
+2021-02-23 11:25:50,588 DEBUG   HandlerThread:1551549 [meta.py:_save_pip():66] save pip done
+2021-02-23 11:25:50,588 DEBUG   HandlerThread:1551549 [meta.py:probe():231] probe done
+2021-02-23 11:25:50,590 DEBUG   SenderThread:1551549 [sender.py:send():117] send: files
+2021-02-23 11:25:50,591 INFO    SenderThread:1551549 [sender.py:_save_file():682] saving file wandb-metadata.json with policy now
+2021-02-23 11:25:50,591 INFO    SenderThread:1551549 [sender.py:_save_file():682] saving file code/train_uniter.py with policy now
+2021-02-23 11:25:50,591 INFO    SenderThread:1551549 [sender.py:_save_file():682] saving file diff.patch with policy now
+2021-02-23 11:25:50,597 DEBUG   HandlerThread:1551549 [handler.py:handle_request():94] handle_request: status
+2021-02-23 11:25:50,597 DEBUG   SenderThread:1551549 [sender.py:send():117] send: request
+2021-02-23 11:25:50,597 DEBUG   SenderThread:1551549 [sender.py:send_request():126] send_request: status
+2021-02-23 11:25:50,601 INFO    HandlerThread:1551549 [handler.py:handle_tbrecord():307] handling tbrecord: tbrecord {
+  log_dir: "./vis_checkpoints"
+  save: true
+  root_dir: "./vis_checkpoints"
+}
+
+2021-02-23 11:25:50,602 DEBUG   HandlerThread:1551549 [config_util.py:dict_from_config_file():99] no default config file found in config-defaults.yaml
+2021-02-23 11:25:50,738 DEBUG   SenderThread:1551549 [sender.py:send():117] send: config
+2021-02-23 11:25:50,908 DEBUG   SenderThread:1551549 [sender.py:send():117] send: tbrecord
+2021-02-23 11:25:50,908 DEBUG   SenderThread:1551549 [sender.py:send():117] send: files
+2021-02-23 11:25:50,908 INFO    SenderThread:1551549 [sender.py:_save_file():682] saving file events.out.tfevents.1614075950.astro.1551487.0 with policy live
+2021-02-23 11:25:53,854 DEBUG   SenderThread:1551549 [sender.py:send():117] send: stats
+2021-02-23 11:25:58,327 DEBUG   SenderThread:1551549 [sender.py:send():117] send: stats
+2021-02-23 11:26:01,552 DEBUG   SenderThread:1551549 [sender.py:send():117] send: telemetry
+2021-02-23 11:26:01,558 DEBUG   HandlerThread:1551549 [handler.py:handle_request():94] handle_request: poll_exit
+2021-02-23 11:26:01,720 DEBUG   SenderThread:1551549 [sender.py:send():117] send: exit
+2021-02-23 11:26:01,720 INFO    SenderThread:1551549 [sender.py:send_exit():195] handling exit code: 1
+2021-02-23 11:26:01,720 INFO    SenderThread:1551549 [sender.py:send_exit():203] send defer
+2021-02-23 11:26:01,720 DEBUG   SenderThread:1551549 [sender.py:send():117] send: request
+2021-02-23 11:26:01,720 DEBUG   SenderThread:1551549 [sender.py:send_request():126] send_request: poll_exit
+2021-02-23 11:26:01,721 DEBUG   HandlerThread:1551549 [handler.py:handle_request():94] handle_request: defer
+2021-02-23 11:26:01,721 INFO    HandlerThread:1551549 [handler.py:handle_request_defer():108] handle defer: 0
+2021-02-23 11:26:01,721 DEBUG   SenderThread:1551549 [sender.py:send():117] send: request
+2021-02-23 11:26:01,721 DEBUG   SenderThread:1551549 [sender.py:send_request():126] send_request: defer
+2021-02-23 11:26:01,721 INFO    SenderThread:1551549 [sender.py:send_request_defer():212] handle sender defer: 0
+2021-02-23 11:26:01,721 INFO    SenderThread:1551549 [sender.py:send_request_defer():248] send defer: 1
+2021-02-23 11:26:01,721 DEBUG   HandlerThread:1551549 [handler.py:handle_request():94] handle_request: defer
+2021-02-23 11:26:01,721 INFO    HandlerThread:1551549 [handler.py:handle_request_defer():108] handle defer: 1
+2021-02-23 11:26:01,796 DEBUG   SenderThread:1551549 [sender.py:send():117] send: request
+2021-02-23 11:26:01,796 DEBUG   SenderThread:1551549 [sender.py:send_request():126] send_request: defer
+2021-02-23 11:26:01,797 INFO    SenderThread:1551549 [sender.py:send_request_defer():212] handle sender defer: 1
+2021-02-23 11:26:01,797 INFO    SenderThread:1551549 [sender.py:send_request_defer():248] send defer: 2
+2021-02-23 11:26:01,797 DEBUG   SenderThread:1551549 [sender.py:send():117] send: stats
+2021-02-23 11:26:01,797 DEBUG   HandlerThread:1551549 [handler.py:handle_request():94] handle_request: defer
+2021-02-23 11:26:01,797 INFO    HandlerThread:1551549 [handler.py:handle_request_defer():108] handle defer: 2
+2021-02-23 11:26:08,609 DEBUG   HandlerThread:1551549 [handler.py:handle_request():94] handle_request: poll_exit
+2021-02-23 11:26:08,609 DEBUG   SenderThread:1551549 [sender.py:send():117] send: request
+2021-02-23 11:26:08,609 DEBUG   SenderThread:1551549 [sender.py:send_request():126] send_request: defer
+2021-02-23 11:26:08,609 INFO    SenderThread:1551549 [sender.py:send_request_defer():212] handle sender defer: 2
+2021-02-23 11:26:08,609 INFO    SenderThread:1551549 [sender.py:send_request_defer():248] send defer: 3
+2021-02-23 11:26:08,609 DEBUG   SenderThread:1551549 [sender.py:send():117] send: request
+2021-02-23 11:26:08,609 DEBUG   SenderThread:1551549 [sender.py:send_request():126] send_request: poll_exit
+2021-02-23 11:26:08,609 DEBUG   HandlerThread:1551549 [handler.py:handle_request():94] handle_request: defer
+2021-02-23 11:26:08,610 INFO    HandlerThread:1551549 [handler.py:handle_request_defer():108] handle defer: 3
+2021-02-23 11:26:08,610 DEBUG   SenderThread:1551549 [sender.py:send():117] send: summary
+2021-02-23 11:26:08,612 INFO    SenderThread:1551549 [sender.py:_save_file():682] saving file wandb-summary.json with policy end
+2021-02-23 11:26:08,612 DEBUG   SenderThread:1551549 [sender.py:send():117] send: request
+2021-02-23 11:26:08,612 DEBUG   SenderThread:1551549 [sender.py:send_request():126] send_request: defer
+2021-02-23 11:26:08,612 INFO    SenderThread:1551549 [sender.py:send_request_defer():212] handle sender defer: 3
+2021-02-23 11:26:08,612 INFO    SenderThread:1551549 [sender.py:send_request_defer():248] send defer: 4
+2021-02-23 11:26:08,613 DEBUG   HandlerThread:1551549 [handler.py:handle_request():94] handle_request: defer
+2021-02-23 11:26:08,613 INFO    HandlerThread:1551549 [handler.py:handle_request_defer():108] handle defer: 4
+2021-02-23 11:26:08,613 DEBUG   SenderThread:1551549 [sender.py:send():117] send: request
+2021-02-23 11:26:08,613 DEBUG   SenderThread:1551549 [sender.py:send_request():126] send_request: defer
+2021-02-23 11:26:08,613 INFO    SenderThread:1551549 [sender.py:send_request_defer():212] handle sender defer: 4
+2021-02-23 11:26:08,646 INFO    SenderThread:1551549 [sender.py:send_request_defer():248] send defer: 5
+2021-02-23 11:26:08,650 DEBUG   HandlerThread:1551549 [handler.py:handle_request():94] handle_request: defer
+2021-02-23 11:26:08,651 INFO    HandlerThread:1551549 [handler.py:handle_request_defer():108] handle defer: 5
+2021-02-23 11:26:08,651 DEBUG   SenderThread:1551549 [sender.py:send():117] send: request
+2021-02-23 11:26:08,651 DEBUG   SenderThread:1551549 [sender.py:send_request():126] send_request: defer
+2021-02-23 11:26:08,651 INFO    SenderThread:1551549 [sender.py:send_request_defer():212] handle sender defer: 5
+2021-02-23 11:26:08,651 INFO    SenderThread:1551549 [sender.py:send_request_defer():248] send defer: 6
+2021-02-23 11:26:08,651 DEBUG   HandlerThread:1551549 [handler.py:handle_request():94] handle_request: defer
+2021-02-23 11:26:08,652 INFO    HandlerThread:1551549 [handler.py:handle_request_defer():108] handle defer: 6
+2021-02-23 11:26:08,652 DEBUG   SenderThread:1551549 [sender.py:send():117] send: request
+2021-02-23 11:26:08,652 DEBUG   SenderThread:1551549 [sender.py:send_request():126] send_request: defer
+2021-02-23 11:26:08,652 INFO    SenderThread:1551549 [sender.py:send_request_defer():212] handle sender defer: 6
+2021-02-23 11:26:08,864 INFO    SenderThread:1551549 [sender.py:send_request_defer():248] send defer: 7
+2021-02-23 11:26:08,864 DEBUG   HandlerThread:1551549 [handler.py:handle_request():94] handle_request: defer
+2021-02-23 11:26:08,864 INFO    HandlerThread:1551549 [handler.py:handle_request_defer():108] handle defer: 7
+2021-02-23 11:26:08,865 DEBUG   SenderThread:1551549 [sender.py:send():117] send: request
+2021-02-23 11:26:08,865 DEBUG   SenderThread:1551549 [sender.py:send_request():126] send_request: defer
+2021-02-23 11:26:08,865 INFO    SenderThread:1551549 [sender.py:send_request_defer():212] handle sender defer: 7
+2021-02-23 11:26:08,865 INFO    SenderThread:1551549 [sender.py:send_request_defer():248] send defer: 8
+2021-02-23 11:26:08,865 DEBUG   HandlerThread:1551549 [handler.py:handle_request():94] handle_request: defer
+2021-02-23 11:26:08,865 DEBUG   SenderThread:1551549 [sender.py:send():117] send: final
+2021-02-23 11:26:08,865 INFO    HandlerThread:1551549 [handler.py:handle_request_defer():108] handle defer: 8
+2021-02-23 11:26:08,866 DEBUG   SenderThread:1551549 [sender.py:send():117] send: footer
+2021-02-23 11:26:08,866 DEBUG   SenderThread:1551549 [sender.py:send():117] send: request
+2021-02-23 11:26:08,866 DEBUG   SenderThread:1551549 [sender.py:send_request():126] send_request: defer
+2021-02-23 11:26:08,866 INFO    SenderThread:1551549 [sender.py:send_request_defer():212] handle sender defer: 8
+2021-02-23 11:26:10,611 DEBUG   HandlerThread:1551549 [handler.py:handle_request():94] handle_request: poll_exit
+2021-02-23 11:26:10,611 DEBUG   SenderThread:1551549 [sender.py:send():117] send: request
+2021-02-23 11:26:10,612 DEBUG   SenderThread:1551549 [sender.py:send_request():126] send_request: poll_exit
+2021-02-23 11:26:10,612 DEBUG   HandlerThread:1551549 [handler.py:handle_request():94] handle_request: get_summary
+2021-02-23 11:26:10,613 DEBUG   HandlerThread:1551549 [handler.py:handle_request():94] handle_request: sampled_history
+2021-02-23 11:26:10,613 DEBUG   HandlerThread:1551549 [handler.py:handle_request():94] handle_request: shutdown
+2021-02-23 11:26:10,614 INFO    HandlerThread:1551549 [handler.py:finish():333] shutting down handler
+2021-02-23 11:26:10,866 INFO    WriterThread:1551549 [datastore.py:close():258] close: /home/astro/Documents/UvA/Block 4 - NLP2/Multimodal NLP/Multimodal-NLP/wandb/run-20210223_112549-3ck5aive/run-3ck5aive.wandb
+2021-02-23 11:26:11,612 INFO    SenderThread:1551549 [sender.py:finish():766] shutting down sender
+2021-02-23 11:26:11,613 INFO    MainThread:1551549 [internal.py:handle_exit():78] Internal process exited
diff --git a/wandb/run-20210223_112549-3ck5aive/logs/debug.log b/wandb/run-20210223_112549-3ck5aive/logs/debug.log
new file mode 100644
index 0000000..43e5ddb
--- /dev/null
+++ b/wandb/run-20210223_112549-3ck5aive/logs/debug.log
@@ -0,0 +1,79 @@
+2021-02-23 11:25:49,038 INFO    MainThread:1551487 [wandb_setup.py:_flush():70] setting env: {}
+2021-02-23 11:25:49,038 INFO    MainThread:1551487 [wandb_setup.py:_flush():70] multiprocessing start_methods=fork,spawn,forkserver, using: spawn
+2021-02-23 11:25:49,038 INFO    MainThread:1551487 [wandb_setup.py:_flush():70] setting login settings: {}
+2021-02-23 11:25:49,038 INFO    MainThread:1551487 [wandb_init.py:_log_setup():319] Logging user logs to /home/astro/Documents/UvA/Block 4 - NLP2/Multimodal NLP/Multimodal-NLP/wandb/run-20210223_112549-3ck5aive/logs/debug.log
+2021-02-23 11:25:49,038 INFO    MainThread:1551487 [wandb_init.py:_log_setup():320] Logging internal logs to /home/astro/Documents/UvA/Block 4 - NLP2/Multimodal NLP/Multimodal-NLP/wandb/run-20210223_112549-3ck5aive/logs/debug-internal.log
+2021-02-23 11:25:49,038 INFO    MainThread:1551487 [wandb_init.py:init():351] calling init triggers
+2021-02-23 11:25:49,039 INFO    MainThread:1551487 [wandb_init.py:init():358] wandb.init called with sweep_config: {}
+config: {}
+2021-02-23 11:25:49,039 INFO    MainThread:1551487 [wandb_init.py:init():404] starting backend
+2021-02-23 11:25:49,048 INFO    MainThread:1551487 [backend.py:ensure_launched():81] starting backend process...
+2021-02-23 11:25:49,054 INFO    MainThread:1551487 [backend.py:ensure_launched():86] started backend process with pid: 1551549
+2021-02-23 11:25:49,055 INFO    MainThread:1551487 [wandb_init.py:init():413] backend started and connected
+2021-02-23 11:25:49,056 INFO    MainThread:1551487 [wandb_init.py:init():436] updated telemetry
+2021-02-23 11:25:49,057 INFO    MainThread:1551487 [wandb_init.py:init():459] communicating current version
+2021-02-23 11:25:49,445 INFO    MainThread:1551487 [wandb_init.py:init():464] got version response upgrade_message: "wandb version 0.10.20 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
+
+2021-02-23 11:25:49,445 INFO    MainThread:1551487 [wandb_init.py:init():472] communicating run to backend with 30 second timeout
+2021-02-23 11:25:49,627 INFO    MainThread:1551487 [wandb_init.py:init():495] starting run threads in backend
+2021-02-23 11:25:50,594 INFO    MainThread:1551487 [wandb_run.py:_console_start():1411] atexit reg
+2021-02-23 11:25:50,594 INFO    MainThread:1551487 [wandb_run.py:_redirect():1274] redirect: SettingsConsole.REDIRECT
+2021-02-23 11:25:50,595 INFO    MainThread:1551487 [wandb_run.py:_redirect():1279] Redirecting console.
+2021-02-23 11:25:50,595 INFO    MainThread:1551487 [redirect.py:install():213] install start
+2021-02-23 11:25:50,595 INFO    MainThread:1551487 [redirect.py:install():228] install stop
+2021-02-23 11:25:50,595 INFO    MainThread:1551487 [redirect.py:install():213] install start
+2021-02-23 11:25:50,596 INFO    MainThread:1551487 [redirect.py:install():228] install stop
+2021-02-23 11:25:50,596 INFO    MainThread:1551487 [wandb_run.py:_redirect():1325] Redirects installed.
+2021-02-23 11:25:50,596 INFO    MainThread:1551487 [wandb_init.py:init():518] run started, returning control to user process
+2021-02-23 11:25:50,598 INFO    MainThread:1551487 [wandb_run.py:_config_callback():663] config_cb None None {'data_path': './dataset', 'model_path': './model_checkpoints', 'vis_path': './vis_checkpoints', 'model_save_name': 'meme.pt', 'no_model_checkpoints': False, 'remove_checkpoints': False, 'config': 'config/uniter-base.json', 'feature_path': './dataset/own_features', 'pretrained_model_file': 'uniter-base.pt', 'max_txt_len': 60, 'max_bb': 100, 'min_bb': 10, 'num_bb': 36, 'optimizer': 'adam', 'loss_func': 'bce_logits', 'optimize_for': 'aucroc', 'scheduler': 'warmup_cosine', 'beta1': 0.9, 'beta2': 0.999, 'batch_size': 16, 'num_workers': 0, 'gradient_accumulation': 2, 'max_grad_norm': 5, 'pos_wt': 1.0, 'lr': 3e-05, 'warmup_steps': 500, 'weight_decay': 0.001, 'max_epoch': 30, 'lr_decay_step': 3, 'lr_decay_factor': 0.8, 'patience': 5.0, 'early_stop_thresh': 0.001, 'seed': 43, 'log_every': 2000, 'fc_dim': 64, 'dropout': 0.2, 'filter_text': False, 'no_normalize_img': True, 'train_filename': 'train.jsonl', 'upsample_multiplier': 0, 'note': ''}
+2021-02-23 11:25:50,600 INFO    MainThread:1551487 [wandb_run.py:_tensorboard_callback():734] tensorboard callback: ./vis_checkpoints, None
+2021-02-23 11:25:59,501 INFO    MainThread:1551487 [wandb_watch.py:watch():39] Watching
+2021-02-23 11:26:01,550 INFO    MainThread:1551487 [wandb_run.py:_atexit_cleanup():1381] got exitcode: 1
+2021-02-23 11:26:01,551 INFO    MainThread:1551487 [wandb_run.py:_restore():1353] restore
+2021-02-23 11:26:01,551 INFO    MainThread:1551487 [redirect.py:uninstall():232] uninstall start
+2021-02-23 11:26:01,551 INFO    MainThread:1551487 [redirect.py:_stop():287] _stop: stdout
+2021-02-23 11:26:01,551 INFO    stdout    :1551487 [redirect.py:_pipe_relay():129] relay done saw last write: stdout
+2021-02-23 11:26:01,551 INFO    stdout    :1551487 [redirect.py:_pipe_relay():145] relay done done: stdout
+2021-02-23 11:26:01,551 INFO    MainThread:1551487 [redirect.py:_stop():293] _stop closed: stdout
+2021-02-23 11:26:01,552 INFO    MainThread:1551487 [redirect.py:_stop():299] _stop joined: stdout
+2021-02-23 11:26:01,552 INFO    MainThread:1551487 [redirect.py:_stop():301] _stop rd closed: stdout
+2021-02-23 11:26:01,552 INFO    MainThread:1551487 [redirect.py:uninstall():236] uninstall done
+2021-02-23 11:26:01,552 INFO    MainThread:1551487 [redirect.py:uninstall():232] uninstall start
+2021-02-23 11:26:01,552 INFO    MainThread:1551487 [redirect.py:_stop():287] _stop: stderr
+2021-02-23 11:26:01,552 INFO    stderr    :1551487 [redirect.py:_pipe_relay():129] relay done saw last write: stderr
+2021-02-23 11:26:01,552 INFO    MainThread:1551487 [redirect.py:_stop():293] _stop closed: stderr
+2021-02-23 11:26:01,552 INFO    stderr    :1551487 [redirect.py:_pipe_relay():145] relay done done: stderr
+2021-02-23 11:26:01,552 INFO    MainThread:1551487 [redirect.py:_stop():299] _stop joined: stderr
+2021-02-23 11:26:01,552 INFO    MainThread:1551487 [redirect.py:_stop():301] _stop rd closed: stderr
+2021-02-23 11:26:01,552 INFO    MainThread:1551487 [redirect.py:uninstall():236] uninstall done
+2021-02-23 11:26:01,721 INFO    MainThread:1551487 [wandb_run.py:_wait_for_finish():1504] got exit ret: file_counts {
+  wandb_count: 2
+  other_count: 2
+}
+pusher_stats {
+  uploaded_bytes: 37434
+  total_bytes: 37434
+}
+
+2021-02-23 11:26:08,610 INFO    MainThread:1551487 [wandb_run.py:_wait_for_finish():1504] got exit ret: file_counts {
+  wandb_count: 2
+  other_count: 2
+}
+pusher_stats {
+  uploaded_bytes: 37434
+  total_bytes: 37434
+}
+
+2021-02-23 11:26:10,612 INFO    MainThread:1551487 [wandb_run.py:_wait_for_finish():1504] got exit ret: done: true
+exit_result {
+}
+file_counts {
+  wandb_count: 6
+  other_count: 2
+}
+pusher_stats {
+  uploaded_bytes: 60273
+  total_bytes: 60273
+}
+
+2021-02-23 11:26:11,887 INFO    MainThread:1551487 [wandb_run.py:_show_files():1726] logging synced files
diff --git a/wandb/run-20210223_112549-3ck5aive/run-3ck5aive.wandb b/wandb/run-20210223_112549-3ck5aive/run-3ck5aive.wandb
new file mode 100644
index 0000000..163afcc
Binary files /dev/null and b/wandb/run-20210223_112549-3ck5aive/run-3ck5aive.wandb differ
