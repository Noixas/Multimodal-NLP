21/02/2021 09:55:57 PM : INFO - Data path checked..

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

Running training with the following parameters: 
21/02/2021 09:55:57 PM : INFO - Model save path checked..
21/02/2021 09:55:57 PM : INFO - config JSON path checked..

data_path : ./dataset
model_path : ./model_checkpoints
vis_path : ./vis_checkpoints
model_save_name : meme.pt
no_model_checkpoints : False
remove_checkpoints : False
config : config/uniter-base.json
feature_path : ./dataset/own_features
pretrained_model_file : uniter-base.pt
max_txt_len : 60
max_bb : 100
min_bb : 10
num_bb : 36
optimizer : adam
loss_func : bce_logits
optimize_for : aucroc
scheduler : warmup_cosine
beta1 : 0.9
beta2 : 0.999
batch_size : 16
num_workers : 0
gradient_accumulation : 2
max_grad_norm : 5
pos_wt : 1.0
lr : 3e-05
warmup_steps : 500
weight_decay : 0.001
max_epoch : 30
lr_decay_step : 3
lr_decay_factor : 0.8
patience : 5.0
early_stop_thresh : 0.001
seed : 43
log_every : 2000
fc_dim : 64
dropout : 0.2
filter_text : True
no_normalize_img : True
train_filename : upsampled_augmented_and_confounders_by_2x_train.jsonl
upsample_multiplier : 0
device : cuda
n_classes : 1

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
21/02/2021 09:55:57 PM : INFO - Tensorboard Visualization path checked..
21/02/2021 09:55:57 PM : INFO - Cleaning Visualization path of older tensorboard files...

filter text True
Loaded dataset contains  16788 samples
filter text True
Loaded dataset contains  500 samples
filter text True
Loaded dataset contains  1000 samples
21/02/2021 09:56:09 PM : INFO - Using pretrained UNITER base model ./model_checkpoints/uniter-base.pt
21/02/2021 09:56:09 PM : INFO - Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

MemeUniter
MemeUniter(
  (uniter_model): UniterModel(
    (embeddings): UniterTextEmbeddings(
      (word_embeddings): Embedding(28996, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (img_embeddings): UniterImageEmbeddings(
      (img_linear): Linear(in_features=2048, out_features=768, bias=True)
      (img_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
      (pos_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
      (pos_linear): Linear(in_features=7, out_features=768, bias=True)
      (mask_embedding): Embedding(2, 2048, padding_idx=0)
      (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): UniterEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (linear_1): Linear(in_features=768, out_features=384, bias=True)
  (activation): ReLU()
  (linear_2): Linear(in_features=384, out_features=1, bias=True)
)


====================================================================================================
					 Training Network
====================================================================================================

Beginning training at:  2021-02-21 21:56:13.368726 


Epoch: 1/30,            
train_loss = 0.4905,  train_acc = 0.7866,  train_prec = 0.0593,  train_recall = 0.1941,  train_f1 = 0.0908,  train_aucroc = 0.5709,  train_opt_accuracy = -1.0000,  train_threshold = -1.0000            
eval_loss = 0.9840,  eval_acc = 0.5060,  eval_prec = 0.0000,  eval_recall = nan,  eval_f1 = nan,  eval_aucroc = 0.6006,  eval_opt_accuracy = -1.0000,  eval_threshold = -1.0000                
lr  =  0.00003000
Elapsed Time:  00:05:05.7521/02/2021 10:01:19 PM : INFO - New High Score! Saving model...

21/02/2021 10:01:20 PM : INFO - current patience: 0

Epoch: 2/30,            
train_loss = 0.3935,  train_acc = 0.8356,  train_prec = 0.2332,  train_recall = 0.6127,  train_f1 = 0.3378,  train_aucroc = 0.7700,  train_opt_accuracy = -1.0000,  train_threshold = -1.0000            
eval_loss = 0.9408,  eval_acc = 0.5300,  eval_prec = 0.0567,  eval_recall = 0.8750,  eval_f1 = 0.1065,  eval_aucroc = 0.6727,  eval_opt_accuracy = -1.0000,  eval_threshold = -1.0000                
lr  =  0.00002998
Elapsed Time:  00:10:13.53
21/02/2021 10:06:26 PM : INFO - New High Score! Saving model...
21/02/2021 10:06:28 PM : INFO - current patience: 0
21/02/2021 10:11:34 PM : INFO - New High Score! Saving model...

Epoch: 3/30,            
train_loss = 0.3165,  train_acc = 0.8763,  train_prec = 0.4829,  train_recall = 0.7386,  train_f1 = 0.5840,  train_aucroc = 0.8613,  train_opt_accuracy = -1.0000,  train_threshold = -1.0000            
eval_loss = 1.0867,  eval_acc = 0.5580,  eval_prec = 0.1498,  eval_recall = 0.7708,  eval_f1 = 0.2508,  eval_aucroc = 0.6779,  eval_opt_accuracy = -1.0000,  eval_threshold = -1.0000                
lr  =  0.00002991
Elapsed Time:  00:15:21.40
21/02/2021 10:11:35 PM : INFO - current patience: 0

Epoch: 4/30,            
train_loss = 0.2430,  train_acc = 0.9081,  train_prec = 0.6346,  train_recall = 0.8132,  train_f1 = 0.7129,  train_aucroc = 0.9209,  train_opt_accuracy = -1.0000,  train_threshold = -1.0000            
eval_loss = 1.0375,  eval_acc = 0.6100,  eval_prec = 0.3320,  eval_recall = 0.7321,  eval_f1 = 0.4568,  eval_aucroc = 0.7049,  eval_opt_accuracy = -1.0000,  eval_threshold = -1.0000                
lr  =  0.00002980
Elapsed Time:  00:20:29.90
21/02/2021 10:16:43 PM : INFO - New High Score! Saving model...
21/02/2021 10:16:44 PM : INFO - current patience: 0

Epoch: 5/30,            
train_loss = 0.1825,  train_acc = 0.9320,  train_prec = 0.7549,  train_recall = 0.8501,  train_f1 = 0.7996,  train_aucroc = 0.9563,  train_opt_accuracy = -1.0000,  train_threshold = -1.0000            
eval_loss = 1.3477,  eval_acc = 0.5920,  eval_prec = 0.2348,  eval_recall = 0.7945,  eval_f1 = 0.3625,  eval_aucroc = 0.7124,  eval_opt_accuracy = -1.0000,  eval_threshold = -1.0000                
lr  =  0.00002965
Elapsed Time:  00:25:41.0721/02/2021 10:21:54 PM : INFO - New High Score! Saving model...

21/02/2021 10:21:55 PM : INFO - current patience: 0

Epoch: 6/30,            
train_loss = 0.1378,  train_acc = 0.9490,  train_prec = 0.8291,  train_recall = 0.8801,  train_f1 = 0.8538,  train_aucroc = 0.9755,  train_opt_accuracy = -1.0000,  train_threshold = -1.0000            
eval_loss = 1.3372,  eval_acc = 0.5680,  eval_prec = 0.2186,  eval_recall = 0.7013,  eval_f1 = 0.3333,  eval_aucroc = 0.7193,  eval_opt_accuracy = -1.0000,  eval_threshold = -1.0000                
lr  =  0.00002946
Elapsed Time:  00:30:53.1121/02/2021 10:27:06 PM : INFO - New High Score! Saving model...

21/02/2021 10:27:07 PM : INFO - current patience: 0

Epoch: 7/30,            
train_loss = 0.1134,  train_acc = 0.9607,  train_prec = 0.8632,  train_recall = 0.9137,  train_f1 = 0.8878,  train_aucroc = 0.9827,  train_opt_accuracy = -1.0000,  train_threshold = -1.0000            
eval_loss = 1.9544,  eval_acc = 0.5660,  eval_prec = 0.2105,  eval_recall = 0.7027,  eval_f1 = 0.3240,  eval_aucroc = 0.6870,  eval_opt_accuracy = -1.0000,  eval_threshold = -1.0000                
lr  =  0.00002923
Elapsed Time:  00:36:07.8721/02/2021 10:32:21 PM : INFO - current patience: 1


Epoch: 8/30,            
train_loss = 0.0972,  train_acc = 0.9654,  train_prec = 0.8884,  train_recall = 0.9166,  train_f1 = 0.9023,  train_aucroc = 0.9879,  train_opt_accuracy = -1.0000,  train_threshold = -1.0000            
eval_loss = 1.9025,  eval_acc = 0.5860,  eval_prec = 0.2713,  eval_recall = 0.7128,  eval_f1 = 0.3930,  eval_aucroc = 0.7039,  eval_opt_accuracy = -1.0000,  eval_threshold = -1.0000                
lr  =  0.00002896
Elapsed Time:  00:41:24.8421/02/2021 10:37:38 PM : INFO - current patience: 2


Epoch: 9/30,            
train_loss = 0.0812,  train_acc = 0.9717,  train_prec = 0.9033,  train_recall = 0.9371,  train_f1 = 0.9199,  train_aucroc = 0.9915,  train_opt_accuracy = -1.0000,  train_threshold = -1.0000            
eval_loss = 1.6199,  eval_acc = 0.6220,  eval_prec = 0.3320,  eval_recall = 0.7736,  eval_f1 = 0.4646,  eval_aucroc = 0.7039,  eval_opt_accuracy = -1.0000,  eval_threshold = -1.0000                
lr  =  0.00002865
Elapsed Time:  00:46:45.3121/02/2021 10:42:58 PM : INFO - current patience: 3

21/02/2021 10:48:07 PM : INFO - current patience: 4

Epoch: 10/30,            
train_loss = 0.0728,  train_acc = 0.9741,  train_prec = 0.9126,  train_recall = 0.9419,  train_f1 = 0.9270,  train_aucroc = 0.9928,  train_opt_accuracy = -1.0000,  train_threshold = -1.0000            
eval_loss = 1.9473,  eval_acc = 0.6100,  eval_prec = 0.3279,  eval_recall = 0.7364,  eval_f1 = 0.4538,  eval_aucroc = 0.6871,  eval_opt_accuracy = -1.0000,  eval_threshold = -1.0000                
lr  =  0.00002830
Elapsed Time:  00:51:54.10

Epoch: 11/30,            
train_loss = 0.0722,  train_acc = 0.9746,  train_prec = 0.9179,  train_recall = 0.9393,  train_f1 = 0.9285,  train_aucroc = 0.9929,  train_opt_accuracy = -1.0000,  train_threshold = -1.0000            
eval_loss = 1.9269,  eval_acc = 0.6160,  eval_prec = 0.3765,  eval_recall = 0.7099,  eval_f1 = 0.4921,  eval_aucroc = 0.6495,  eval_opt_accuracy = -1.0000,  eval_threshold = -1.0000                
lr  =  0.00002791
Elapsed Time:  00:57:00.3421/02/2021 10:53:13 PM : INFO - current patience: 5


----------------------------------------------------------------------------------------------------

--------------------------------------------------
Best Validation scores:
--------------------------------------------------

Val accuracy of best model = 56.800
Val AUC-ROC of best model = 71.934
Val precision of best model = 21.862
Val recall of best model = 70.130
Val f1 of best model = 33.333

--------------------------------------------------
		Evaluating on test set
--------------------------------------------------
21/02/2021 10:53:13 PM : INFO - Training terminated early because the Validation aucroc did not improve for  5.0  epochs
21/02/2021 10:53:13 PM : INFO - Using UNITER model ./model_checkpoints/meme.pt
21/02/2021 10:53:14 PM : INFO - Exporting dev_seen predictions...
21/02/2021 10:53:17 PM : INFO - Finished export of dev_seen predictions
21/02/2021 10:53:17 PM : INFO - Optimal threshold on validation dataset: 0.5000 (accuracy=56.80%)
21/02/2021 10:53:17 PM : INFO - Export and testing on test_seen...
21/02/2021 10:53:23 PM : INFO - Finished export of test predictions
