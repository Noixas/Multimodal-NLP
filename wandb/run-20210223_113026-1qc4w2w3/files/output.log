23/02/2021 11:30:27 AM : INFO - Data path checked..
23/02/2021 11:30:27 AM : INFO - Model save path checked..

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

Running training with the following parameters: 

data_path : ./dataset
model_path : ./model_checkpoints
vis_path : ./vis_checkpoints
model_save_name : meme.pt
no_model_checkpoints : False
remove_checkpoints : False
config : config/uniter-base.json
feature_path : ./dataset/own_features
pretrained_model_file : uniter-base.pt
max_txt_len : 60
max_bb : 100
min_bb : 10
num_bb : 36
optimizer : adam
loss_func : bce_logits
optimize_for : aucroc
scheduler : warmup_cosine
beta1 : 0.9
beta2 : 0.999
batch_size : 16
num_workers : 0
gradient_accumulation : 2
max_grad_norm : 5
pos_wt : 1.0
lr : 3e-05
warmup_steps : 500
weight_decay : 0.001
max_epoch : 30
lr_decay_step : 3
lr_decay_factor : 0.8
patience : 5.0
early_stop_thresh : 0.001
seed : 43
log_every : 2000
fc_dim : 64
dropout : 0.2
filter_text : False
no_normalize_img : True
train_filename : train.jsonl
23/02/2021 11:30:27 AM : INFO - config JSON path checked..
23/02/2021 11:30:27 AM : INFO - Tensorboard Visualization path checked..
23/02/2021 11:30:27 AM : INFO - Cleaning Visualization path of older tensorboard files...

upsample_multiplier : 0
note : 
device : cuda
n_classes : 1

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
filter text False
Loaded dataset contains  8500 samples
filter text False
Loaded dataset contains  500 samples
filter text False
Loaded dataset contains  1000 samples
23/02/2021 11:30:34 AM : INFO - Using pretrained UNITER base model ./model_checkpoints/uniter-base.pt
23/02/2021 11:30:34 AM : INFO - Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

MemeUniter
MemeUniter(
  (uniter_model): UniterModel(
    (embeddings): UniterTextEmbeddings(
      (word_embeddings): Embedding(28996, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (img_embeddings): UniterImageEmbeddings(
      (img_linear): Linear(in_features=2048, out_features=768, bias=True)
      (img_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
      (pos_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
      (pos_linear): Linear(in_features=7, out_features=768, bias=True)
      (mask_embedding): Embedding(2, 2048, padding_idx=0)
      (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): UniterEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (linear_1): Linear(in_features=768, out_features=384, bias=True)
  (activation_1): ReLU()
  (linear_2): Linear(in_features=384, out_features=1, bias=True)
)


====================================================================================================
					 Training Network
====================================================================================================

Beginning training at:  2021-02-23 11:30:38.444713 

{'img_feat': tensor([[[1.5612e-02, 4.9868e-01, 0.0000e+00,  ..., 2.4420e+00,
          2.2442e+00, 3.2790e+00],
         [0.0000e+00, 0.0000e+00, 4.0617e-03,  ..., 0.0000e+00,
          3.3174e-01, 4.9340e+00],
         [0.0000e+00, 0.0000e+00, 1.3875e-03,  ..., 0.0000e+00,
          0.0000e+00, 1.0999e+00],
         ...,
         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
          1.6567e+00, 5.4898e+00],
         [0.0000e+00, 5.5716e-01, 1.4479e-03,  ..., 1.8302e-01,
          4.1672e-01, 7.1385e-01],
         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
          1.1837e+00, 0.0000e+00]],

        [[1.6472e+00, 0.0000e+00, 3.1099e-02,  ..., 1.7600e-01,
          4.8686e+00, 1.5760e-02],
         [0.0000e+00, 3.3627e+00, 0.0000e+00,  ..., 0.0000e+00,
          2.1892e-01, 8.4406e-02],
         [2.6999e+00, 9.5984e-02, 3.9958e-03,  ..., 4.4602e-02,
          2.9881e-01, 2.8486e-03],
         ...,
         [0.0000e+00, 6.8325e+00, 7.5323e-01,  ..., 1.0135e+00,
          1.6959e+00, 2.0508e+00],
         [1.3425e-01, 1.7696e-01, 1.9228e-02,  ..., 1.1174e-01,
          7.6409e-01, 3.3090e-02],
         [2.1623e-01, 1.8586e-01, 2.4881e-02,  ..., 6.1740e-02,
          4.6643e-01, 3.9336e-02]],

        [[8.8972e-02, 1.2055e+00, 1.5363e-04,  ..., 1.7583e-01,
          9.7964e-02, 1.8192e+00],
         [0.0000e+00, 1.4218e+01, 3.5234e-01,  ..., 1.0440e-02,
          0.0000e+00, 1.6708e+00],
         [3.0107e+00, 7.8744e+00, 0.0000e+00,  ..., 0.0000e+00,
          1.8368e-01, 6.4524e-01],
         ...,
         [5.4442e+00, 3.6430e+00, 3.6196e-01,  ..., 0.0000e+00,
          1.1308e+00, 1.2094e-01],
         [5.5932e+00, 1.5168e+01, 2.6215e-02,  ..., 3.1191e-03,
          4.2580e+00, 2.0646e-01],
         [0.0000e+00, 3.2467e+00, 2.7613e-01,  ..., 0.0000e+00,
          1.3608e-02, 8.2527e-02]],

        ...,

        [[0.0000e+00, 1.3709e+00, 7.7179e-01,  ..., 8.3330e-01,
          7.4919e-02, 7.9703e-01],
         [0.0000e+00, 1.8989e+00, 7.8347e-01,  ..., 3.6484e-01,
          1.6066e-01, 1.2052e-01],
         [0.0000e+00, 3.9819e+00, 2.9215e+00,  ..., 7.3750e-01,
          6.2846e+00, 2.4662e+00],
         ...,
         [0.0000e+00, 2.4214e+00, 2.0144e+00,  ..., 8.5421e-02,
          1.6197e-01, 3.5298e-01],
         [0.0000e+00, 1.8422e-02, 6.9878e-04,  ..., 0.0000e+00,
          3.7636e-01, 1.0488e-01],
         [0.0000e+00, 2.0764e+00, 1.2571e+00,  ..., 6.6073e-01,
          2.3893e-01, 0.0000e+00]],

        [[0.0000e+00, 5.4505e-02, 3.8238e-02,  ..., 1.1587e+00,
          0.0000e+00, 8.4889e-02],
         [2.9104e-02, 0.0000e+00, 0.0000e+00,  ..., 3.3894e+00,
          7.2706e-01, 0.0000e+00],
         [0.0000e+00, 3.2760e+00, 4.9646e+00,  ..., 0.0000e+00,
          0.0000e+00, 1.4588e-02],
         ...,
         [2.4560e-01, 1.1290e-01, 4.3252e-01,  ..., 4.7387e+00,
          9.8196e-01, 4.7833e-02],
         [0.0000e+00, 1.8327e+00, 1.6785e-02,  ..., 2.6912e-01,
          1.6926e-03, 2.6101e+00],
         [0.0000e+00, 9.4687e+00, 1.5459e+00,  ..., 1.6427e-02,
          1.8645e-01, 3.0250e+00]],

        [[3.8425e-02, 0.0000e+00, 0.0000e+00,  ..., 7.9539e-02,
          6.2630e-03, 4.3382e-01],
         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
          8.4463e-02, 1.2837e+00],
         [2.8494e-02, 5.3430e-02, 0.0000e+00,  ..., 0.0000e+00,
          3.4569e-02, 1.0765e+00],
         ...,
         [1.3478e-02, 1.3127e-02, 0.0000e+00,  ..., 0.0000e+00,
          0.0000e+00, 3.4972e-01],
         [9.1800e-03, 0.0000e+00, 0.0000e+00,  ..., 1.8383e-02,
          8.3792e-02, 5.1610e-01],
         [1.3099e-02, 0.0000e+00, 0.0000e+00,  ..., 3.1031e-02,
          1.0359e-02, 5.4203e-01]]], device='cuda:0'), 'img_pos_feat': tensor([[[0.0063, 0.0016, 0.9969,  ..., 0.9906, 0.2923, 0.2895],
         [0.7885, 0.2125, 0.8660,  ..., 0.0775, 0.0768, 0.0060],
         [0.1408, 0.1973, 0.2735,  ..., 0.1327, 0.2389, 0.0317],
         ...,
         [0.7845, 0.1985, 0.8677,  ..., 0.0832, 0.0779, 0.0065],
         [0.1303, 0.1789, 0.4560,  ..., 0.3258, 0.2517, 0.0820],
         [0.6018, 0.6366, 0.6830,  ..., 0.0812, 0.0923, 0.0075]],

        [[0.2246, 0.1320, 0.4063,  ..., 0.1818, 0.2101, 0.0382],
         [0.8058, 0.4876, 0.9699,  ..., 0.1642, 0.3795, 0.0623],
         [0.1586, 0.1581, 0.4937,  ..., 0.3352, 0.7307, 0.2449],
         ...,
         [0.4438, 0.8658, 0.8978,  ..., 0.4541, 0.1029, 0.0467],
         [0.0875, 0.4952, 0.5042,  ..., 0.4167, 0.4030, 0.1679],
         [0.1480, 0.4222, 0.4887,  ..., 0.3406, 0.4883, 0.1663]],

        [[0.0000, 0.0172, 0.9696,  ..., 0.9696, 0.5269, 0.5109],
         [0.2845, 0.3749, 0.4285,  ..., 0.1440, 0.4603, 0.0663],
         [0.5449, 0.5014, 0.9077,  ..., 0.3629, 0.1576, 0.0572],
         ...,
         [0.5295, 0.5806, 0.8686,  ..., 0.3391, 0.1645, 0.0558],
         [0.4045, 0.4318, 0.9510,  ..., 0.5465, 0.2410, 0.1317],
         [0.6051, 0.0433, 0.9796,  ..., 0.3745, 0.4492, 0.1682]],

        ...,

        [[0.0643, 0.0059, 0.9969,  ..., 0.9326, 0.8637, 0.8054],
         [0.0646, 0.3747, 0.8214,  ..., 0.7568, 0.5939, 0.4495],
         [0.1319, 0.0282, 0.8449,  ..., 0.7130, 0.0575, 0.0410],
         ...,
         [0.0142, 0.1753, 0.5364,  ..., 0.5222, 0.6925, 0.3616],
         [0.6689, 0.4388, 0.7339,  ..., 0.0650, 0.0504, 0.0033],
         [0.0598, 0.4117, 0.7428,  ..., 0.6830, 0.5022, 0.3430]],

        [[0.3134, 0.0503, 0.9558,  ..., 0.6424, 0.9469, 0.6083],
         [0.0716, 0.3025, 0.9961,  ..., 0.9245, 0.2010, 0.1858],
         [0.8823, 0.6197, 0.9625,  ..., 0.0802, 0.1699, 0.0136],
         ...,
         [0.0068, 0.0266, 0.9916,  ..., 0.9848, 0.6060, 0.5968],
         [0.0000, 0.0136, 0.9861,  ..., 0.9861, 0.2952, 0.2911],
         [0.3216, 0.8501, 0.8092,  ..., 0.4876, 0.1349, 0.0658]],

        [[0.0109, 0.0752, 0.9928,  ..., 0.9820, 0.8521, 0.8367],
         [0.2554, 0.2378, 0.7396,  ..., 0.4842, 0.0973, 0.0471],
         [0.0517, 0.4559, 0.8831,  ..., 0.8314, 0.5313, 0.4417],
         ...,
         [0.1427, 0.5988, 0.5307,  ..., 0.3880, 0.3750, 0.1455],
         [0.0678, 0.0732, 0.9719,  ..., 0.9041, 0.7953, 0.7190],
         [0.0330, 0.0806, 0.9814,  ..., 0.9484, 0.8886, 0.8427]]],
       device='cuda:0'), 'input_ids': tensor([[  101,  7979,  1279,  1129,  1176,  1120,  1103,  4640,  1114,  1139,
          1436,  1905,   102,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],
        [  101,  1165,  1128,  1505,  1114,  1240,  2053,  1114,  1240,  5095,
         12967,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],
        [  101,  1191,  1128,  1341,  1103,   172, 16971, 19610,  1104,  2052,
          1110,  1472,  1121,  1103,   172, 16971,  1968,  2260,  1201,  2403,
           117,  1128,   112,  1231,   170,  4477,  8906,   102,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],
        [  101,  1165,  1103,   179, 17540, 19643,  1128,  1177,  1662,  1111,
          1851,  1201,  1115,  1240,  1416,  9225,  1116,   102,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],
        [  101,  1139,  1148,  2496,   136,  2033,  9297,  1104,  1412,  1822,
         25731,  1643, 26179,  1880,  5600,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],
        [  101, 26063, 17869,   117,  1150,  2085,  1103, 24544, 13408,   136,
           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],
        [  101,   176,  3051,   117,   178,   112,   173,  1176,  1106,  5680,
          5985,  1103, 11074,  1149,  1104,  1115,  1749,   117,  1133,   178,
          1400,   170,  1645,  1120,   126,  1577,   112,   189,   170,  1830,
          7641,  1233,  1202,  1122,   136,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],
        [  101,  1103,  1178,  1590,  1150,  3520,  1187,  1123,  2252,  1110,
           119,   119,   119,  1110,   170,  8244,  1103,  1832,  1104,   190,
          1169,  3465,  1205,   119,   119,   119,  1105,  1294,  1143,  1330,
          4355,  1104,  5679,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],
        [  101,  1725,  1103,  9367,  1110,  1142,  1136,  1640,   170,  1143,
          3263,   136,   102,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],
        [  101, 10565,  1108,   180,  4380,   175,  1358,  2935,   102,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],
        [  101,  1165,  7162,  4003,  1412,  5167,  1152,  1132,  1136, 22864,
           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],
        [  101,  3210,  1146, 16408,  2227,   119,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],
        [  101,  1128,  1176,  1380,  1115,   178,  1202,  1136,  2521,  5548,
         22852,  7034,  1106,  1111, 23221,  1240,  1297,   102,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],
        [  101,  1144,  1126, 14676,   119,   119,   119,   119,   119,   119,
         11981,  1149,   189,  4047,  3382,  8240,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],
        [  101,  1274,   112,   189,  1221,  1293,  1106,  2239,  1114,  1602,
          1234,  1133,   178,  3319,   178,   112,  1325,  1321,   170,  2046,
          1120,  1122,   102,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],
        [  101,  1165,  1884,  1233,  5387,  1468,  7086,   170,   180,  2087,
          1665,  1107, 20049,  8752,  1810,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]],
       device='cuda:0'), 'position_ids': tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59],
        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59],
        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59],
        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59],
        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59],
        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59],
        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59],
        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59],
        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59],
        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59],
        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59],
        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59],
        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59],
        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59],
        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59],
        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59]], device='cuda:0'), 'attention_mask': tensor([[1., 1., 1.,  ..., 0., 0., 0.],
        [1., 1., 1.,  ..., 0., 0., 0.],
        [1., 1., 1.,  ..., 0., 0., 0.],
        ...,
        [1., 1., 1.,  ..., 0., 0., 0.],
        [1., 1., 1.,  ..., 0., 0., 0.],
        [1., 1., 1.,  ..., 0., 0., 0.]], device='cuda:0'), 'gather_index': tensor([[  0,   1,   2,  ..., 133, 134, 135],
        [  0,   1,   2,  ..., 133, 134, 135],
        [  0,   1,   2,  ..., 133, 134, 135],
        ...,
        [  0,   1,   2,  ..., 133, 134, 135],
        [  0,   1,   2,  ..., 133, 134, 135],
        [  0,   1,   2,  ..., 133, 134, 135]], device='cuda:0'), 'output_all_encoded_layers': False, 'gender_race_probs': tensor([[[8.7358e-03, 9.9126e-01, 1.8328e-02,  ..., 3.5724e-02,
          1.6068e-02, 1.1800e-01],
         [5.0143e-01, 4.9857e-01, 1.9331e-04,  ..., 4.9987e-01,
          4.1440e-05, 3.4760e-03],
         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
          0.0000e+00, 0.0000e+00],
         ...,
         [9.7209e-01, 2.7907e-02, 9.9715e-01,  ..., 1.7773e-04,
          7.2041e-06, 6.5352e-04],
         [2.5348e-06, 1.0000e+00, 9.3388e-02,  ..., 1.6413e-01,
          9.9280e-02, 4.7144e-01],
         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
          0.0000e+00, 0.0000e+00]],

        [[8.7358e-03, 9.9126e-01, 1.8328e-02,  ..., 3.5724e-02,
          1.6068e-02, 1.1800e-01],
         [5.0143e-01, 4.9857e-01, 1.9331e-04,  ..., 4.9987e-01,
          4.1440e-05, 3.4760e-03],
         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
          0.0000e+00, 0.0000e+00],
         ...,
         [9.7209e-01, 2.7907e-02, 9.9715e-01,  ..., 1.7773e-04,
          7.2041e-06, 6.5352e-04],
         [2.5348e-06, 1.0000e+00, 9.3388e-02,  ..., 1.6413e-01,
          9.9280e-02, 4.7144e-01],
         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
          0.0000e+00, 0.0000e+00]],

        [[8.7358e-03, 9.9126e-01, 1.8328e-02,  ..., 3.5724e-02,
          1.6068e-02, 1.1800e-01],
         [5.0143e-01, 4.9857e-01, 1.9331e-04,  ..., 4.9987e-01,
          4.1440e-05, 3.4760e-03],
         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
          0.0000e+00, 0.0000e+00],
         ...,
         [9.7209e-01, 2.7907e-02, 9.9715e-01,  ..., 1.7773e-04,
          7.2041e-06, 6.5352e-04],
         [2.5348e-06, 1.0000e+00, 9.3388e-02,  ..., 1.6413e-01,
          9.9280e-02, 4.7144e-01],
         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
          0.0000e+00, 0.0000e+00]],

        ...,

        [[8.7358e-03, 9.9126e-01, 1.8328e-02,  ..., 3.5724e-02,
          1.6068e-02, 1.1800e-01],
         [5.0143e-01, 4.9857e-01, 1.9331e-04,  ..., 4.9987e-01,
          4.1440e-05, 3.4760e-03],
         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
          0.0000e+00, 0.0000e+00],
         ...,
         [9.7209e-01, 2.7907e-02, 9.9715e-01,  ..., 1.7773e-04,
          7.2041e-06, 6.5352e-04],
         [2.5348e-06, 1.0000e+00, 9.3388e-02,  ..., 1.6413e-01,
          9.9280e-02, 4.7144e-01],
         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
          0.0000e+00, 0.0000e+00]],

        [[8.7358e-03, 9.9126e-01, 1.8328e-02,  ..., 3.5724e-02,
          1.6068e-02, 1.1800e-01],
         [5.0143e-01, 4.9857e-01, 1.9331e-04,  ..., 4.9987e-01,
          4.1440e-05, 3.4760e-03],
         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
          0.0000e+00, 0.0000e+00],
         ...,
         [9.7209e-01, 2.7907e-02, 9.9715e-01,  ..., 1.7773e-04,
          7.2041e-06, 6.5352e-04],
         [2.5348e-06, 1.0000e+00, 9.3388e-02,  ..., 1.6413e-01,
          9.9280e-02, 4.7144e-01],
         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
          0.0000e+00, 0.0000e+00]],

        [[8.7358e-03, 9.9126e-01, 1.8328e-02,  ..., 3.5724e-02,
          1.6068e-02, 1.1800e-01],
         [5.0143e-01, 4.9857e-01, 1.9331e-04,  ..., 4.9987e-01,
          4.1440e-05, 3.4760e-03],
         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
          0.0000e+00, 0.0000e+00],
         ...,
         [9.7209e-01, 2.7907e-02, 9.9715e-01,  ..., 1.7773e-04,
          7.2041e-06, 6.5352e-04],
         [2.5348e-06, 1.0000e+00, 9.3388e-02,  ..., 1.6413e-01,
          9.9280e-02, 4.7144e-01],
         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
          0.0000e+00, 0.0000e+00]]], device='cuda:0')}
Traceback (most recent call last):
  File "train_uniter.py", line 648, in <module>
    trainer.train_main()
  File "train_uniter.py", line 432, in train_main
    self.train_iter_step()
  File "train_uniter.py", line 470, in train_iter_step
    output_all_encoded_layers=False, gender_race_probs=self.batch['gender_race_probs'])
  File "/home/astro/anaconda3/envs/nlp2-multimodal/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/astro/Documents/UvA/Block 4 - NLP2/Multimodal NLP/Multimodal-NLP/model/meme_uniter.py", line 29, in forward
    out = torch.cat((out, gender_race_probs), 1) # concatenate the uniter output with gender and race probabilities
RuntimeError: Tensors must have same number of dimensions: got 3 and 2
