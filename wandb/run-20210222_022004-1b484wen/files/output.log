22/02/2021 02:20:05 AM : INFO - Data path checked..

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

Running training with the following parameters: 
22/02/2021 02:20:05 AM : INFO - Model save path checked..
22/02/2021 02:20:05 AM : INFO - config JSON path checked..
22/02/2021 02:20:05 AM : INFO - Tensorboard Visualization path checked..
22/02/2021 02:20:05 AM : INFO - Cleaning Visualization path of older tensorboard files...


data_path : ./dataset
model_path : ./model_checkpoints
vis_path : ./vis_checkpoints
model_save_name : meme.pt
no_model_checkpoints : False
remove_checkpoints : False
config : config/uniter-base.json
feature_path : ./dataset/own_features
pretrained_model_file : uniter-base.pt
max_txt_len : 60
max_bb : 100
min_bb : 10
num_bb : 36
optimizer : adam
loss_func : bce_logits
optimize_for : aucroc
scheduler : warmup_cosine
beta1 : 0.9
beta2 : 0.999
batch_size : 16
num_workers : 0
gradient_accumulation : 2
max_grad_norm : 5
pos_wt : 1.0
lr : 3e-05
warmup_steps : 500
weight_decay : 0.001
max_epoch : 30
lr_decay_step : 3
lr_decay_factor : 0.8
patience : 5.0
early_stop_thresh : 0.001
seed : 43
log_every : 2000
fc_dim : 64
dropout : 0.2
filter_text : True
no_normalize_img : True
train_filename : train.jsonl
upsample_multiplier : 3
device : cuda
n_classes : 1

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
filter text True
Confounders upsampled by 3 times. 
 From 1903  samples to 5709
Upsample both parts of confounders, hateful and non hateful - option 2
Saved confounder samples to: 
./dataset/train_upsampled_confounders_3x_times.jsonl
Loaded dataset contains  14209 samples
filter text True
Loaded dataset contains  500 samples
filter text True
Loaded dataset contains  1000 samples
22/02/2021 02:20:16 AM : INFO - Using pretrained UNITER base model ./model_checkpoints/uniter-base.pt
22/02/2021 02:20:16 AM : INFO - Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

MemeUniter
MemeUniter(
  (uniter_model): UniterModel(
    (embeddings): UniterTextEmbeddings(
      (word_embeddings): Embedding(28996, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (img_embeddings): UniterImageEmbeddings(
      (img_linear): Linear(in_features=2048, out_features=768, bias=True)
      (img_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
      (pos_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
      (pos_linear): Linear(in_features=7, out_features=768, bias=True)
      (mask_embedding): Embedding(2, 2048, padding_idx=0)
      (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): UniterEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (linear_1): Linear(in_features=768, out_features=384, bias=True)
  (activation_1): LeakyReLU(negative_slope=0.1)
  (linear_2): Linear(in_features=384, out_features=1, bias=True)
)


====================================================================================================
					 Training Network
====================================================================================================

Beginning training at:  2021-02-22 02:20:20.299779 

22/02/2021 02:24:31 AM : INFO - New High Score! Saving model...

Epoch: 1/30,            
train_loss = 0.6348,  train_acc = 0.6125,  train_prec = 0.4572,  train_recall = 0.5981,  train_f1 = 0.5183,  train_aucroc = 0.6644,  train_opt_accuracy = -1.0000,  train_threshold = -1.0000            
eval_loss = 0.7614,  eval_acc = 0.6160,  eval_prec = 0.4008,  eval_recall = 0.6923,  eval_f1 = 0.5077,  eval_aucroc = 0.7043,  eval_opt_accuracy = -1.0000,  eval_threshold = -1.0000                
lr  =  0.00002670
Elapsed Time:  00:04:11.66
22/02/2021 02:24:33 AM : INFO - current patience: 0

Epoch: 2/30,            
train_loss = 0.3210,  train_acc = 0.8699,  train_prec = 0.8427,  train_recall = 0.8682,  train_f1 = 0.8552,  train_aucroc = 0.9351,  train_opt_accuracy = -1.0000,  train_threshold = -1.0000            
eval_loss = 0.7491,  eval_acc = 0.6620,  eval_prec = 0.4534,  eval_recall = 0.7671,  eval_f1 = 0.5700,  eval_aucroc = 0.7546,  eval_opt_accuracy = -1.0000,  eval_threshold = -1.0000                
lr  =  0.00002998
Elapsed Time:  00:08:27.1022/02/2021 02:28:47 AM : INFO - New High Score! Saving model...

22/02/2021 02:28:48 AM : INFO - current patience: 0

Epoch: 3/30,            
train_loss = 0.1421,  train_acc = 0.9502,  train_prec = 0.9466,  train_recall = 0.9444,  train_f1 = 0.9455,  train_aucroc = 0.9862,  train_opt_accuracy = -1.0000,  train_threshold = -1.0000            
eval_loss = 1.2591,  eval_acc = 0.6480,  eval_prec = 0.3684,  eval_recall = 0.8198,  eval_f1 = 0.5084,  eval_aucroc = 0.7224,  eval_opt_accuracy = -1.0000,  eval_threshold = -1.0000                
lr  =  0.00002992
Elapsed Time:  00:12:40.62
22/02/2021 02:33:00 AM : INFO - current patience: 1
22/02/2021 02:37:13 AM : INFO - current patience: 2

Epoch: 4/30,            
train_loss = 0.0875,  train_acc = 0.9705,  train_prec = 0.9674,  train_recall = 0.9679,  train_f1 = 0.9677,  train_aucroc = 0.9942,  train_opt_accuracy = -1.0000,  train_threshold = -1.0000            
eval_loss = 1.2644,  eval_acc = 0.6280,  eval_prec = 0.3684,  eval_recall = 0.7521,  eval_f1 = 0.4946,  eval_aucroc = 0.7350,  eval_opt_accuracy = -1.0000,  eval_threshold = -1.0000                
lr  =  0.00002982
Elapsed Time:  00:16:53.51

Epoch: 5/30,            
train_loss = 0.0691,  train_acc = 0.9784,  train_prec = 0.9751,  train_recall = 0.9774,  train_f1 = 0.9763,  train_aucroc = 0.9960,  train_opt_accuracy = -1.0000,  train_threshold = -1.0000            
eval_loss = 1.3254,  eval_acc = 0.6700,  eval_prec = 0.4939,  eval_recall = 0.7531,  eval_f1 = 0.5966,  eval_aucroc = 0.7406,  eval_opt_accuracy = -1.0000,  eval_threshold = -1.0000                
lr  =  0.00002968
Elapsed Time:  00:21:05.2822/02/2021 02:41:25 AM : INFO - current patience: 3


Epoch: 6/30,            
train_loss = 0.0613,  train_acc = 0.9800,  train_prec = 0.9767,  train_recall = 0.9794,  train_f1 = 0.9780,  train_aucroc = 0.9972,  train_opt_accuracy = -1.0000,  train_threshold = -1.0000            
eval_loss = 1.4233,  eval_acc = 0.6640,  eval_prec = 0.4372,  eval_recall = 0.7883,  eval_f1 = 0.5625,  eval_aucroc = 0.7547,  eval_opt_accuracy = -1.0000,  eval_threshold = -1.0000                
lr  =  0.00002949
Elapsed Time:  00:25:42.5622/02/2021 02:46:02 AM : INFO - New High Score! Saving model...

22/02/2021 02:46:04 AM : INFO - current patience: 4
22/02/2021 02:50:29 AM : INFO - New High Score! Saving model...

Epoch: 7/30,            
train_loss = 0.0628,  train_acc = 0.9798,  train_prec = 0.9781,  train_recall = 0.9776,  train_f1 = 0.9779,  train_aucroc = 0.9971,  train_opt_accuracy = -1.0000,  train_threshold = -1.0000            
eval_loss = 1.6969,  eval_acc = 0.6900,  eval_prec = 0.4980,  eval_recall = 0.7987,  eval_f1 = 0.6135,  eval_aucroc = 0.7570,  eval_opt_accuracy = -1.0000,  eval_threshold = -1.0000                
lr  =  0.00002927
Elapsed Time:  00:30:08.90
22/02/2021 02:50:30 AM : INFO - current patience: 0
22/02/2021 02:55:02 AM : INFO - current patience: 1

Epoch: 8/30,            
train_loss = 0.0586,  train_acc = 0.9814,  train_prec = 0.9792,  train_recall = 0.9801,  train_f1 = 0.9796,  train_aucroc = 0.9973,  train_opt_accuracy = -1.0000,  train_threshold = -1.0000            
eval_loss = 1.2554,  eval_acc = 0.6940,  eval_prec = 0.5344,  eval_recall = 0.7765,  eval_f1 = 0.6331,  eval_aucroc = 0.7567,  eval_opt_accuracy = -1.0000,  eval_threshold = -1.0000                
lr  =  0.00002900
Elapsed Time:  00:34:42.35

Epoch: 9/30,            
train_loss = 0.0586,  train_acc = 0.9813,  train_prec = 0.9793,  train_recall = 0.9796,  train_f1 = 0.9795,  train_aucroc = 0.9971,  train_opt_accuracy = -1.0000,  train_threshold = -1.0000            
eval_loss = 1.3619,  eval_acc = 0.6760,  eval_prec = 0.5830,  eval_recall = 0.7094,  eval_f1 = 0.6400,  eval_aucroc = 0.7600,  eval_opt_accuracy = -1.0000,  eval_threshold = -1.0000                
lr  =  0.00002869
Elapsed Time:  00:38:58.72
22/02/2021 02:59:19 AM : INFO - New High Score! Saving model...
22/02/2021 02:59:20 AM : INFO - current patience: 0

Epoch: 10/30,            
train_loss = 0.0565,  train_acc = 0.9824,  train_prec = 0.9816,  train_recall = 0.9798,  train_f1 = 0.9807,  train_aucroc = 0.9976,  train_opt_accuracy = -1.0000,  train_threshold = -1.0000            
eval_loss = 1.4695,  eval_acc = 0.6600,  eval_prec = 0.4494,  eval_recall = 0.7655,  eval_f1 = 0.5663,  eval_aucroc = 0.7439,  eval_opt_accuracy = -1.0000,  eval_threshold = -1.0000                
lr  =  0.00002835
Elapsed Time:  00:43:32.1522/02/2021 03:03:52 AM : INFO - current patience: 1


Epoch: 11/30,            
train_loss = 0.0580,  train_acc = 0.9809,  train_prec = 0.9790,  train_recall = 0.9792,  train_f1 = 0.9791,  train_aucroc = 0.9975,  train_opt_accuracy = -1.0000,  train_threshold = -1.0000            
eval_loss = 1.2148,  eval_acc = 0.6780,  eval_prec = 0.5142,  eval_recall = 0.7560,  eval_f1 = 0.6120,  eval_aucroc = 0.7704,  eval_opt_accuracy = -1.0000,  eval_threshold = -1.0000                
lr  =  0.00002796
Elapsed Time:  00:48:01.5322/02/2021 03:08:21 AM : INFO - New High Score! Saving model...

22/02/2021 03:08:23 AM : INFO - current patience: 0

Epoch: 12/30,            
train_loss = 0.0527,  train_acc = 0.9819,  train_prec = 0.9801,  train_recall = 0.9802,  train_f1 = 0.9802,  train_aucroc = 0.9980,  train_opt_accuracy = -1.0000,  train_threshold = -1.0000            
eval_loss = 1.6021,  eval_acc = 0.6600,  eval_prec = 0.4980,  eval_recall = 0.7278,  eval_f1 = 0.5913,  eval_aucroc = 0.7359,  eval_opt_accuracy = -1.0000,  eval_threshold = -1.0000                
lr  =  0.00002754
Elapsed Time:  00:52:38.6122/02/2021 03:12:58 AM : INFO - current patience: 1

22/02/2021 03:17:44 AM : INFO - current patience: 2

Epoch: 13/30,            
train_loss = 0.0553,  train_acc = 0.9824,  train_prec = 0.9805,  train_recall = 0.9809,  train_f1 = 0.9807,  train_aucroc = 0.9975,  train_opt_accuracy = -1.0000,  train_threshold = -1.0000            
eval_loss = 1.3750,  eval_acc = 0.6840,  eval_prec = 0.5870,  eval_recall = 0.7214,  eval_f1 = 0.6473,  eval_aucroc = 0.7427,  eval_opt_accuracy = -1.0000,  eval_threshold = -1.0000                
lr  =  0.00002708
Elapsed Time:  00:57:24.22

Epoch: 14/30,            
train_loss = 0.0532,  train_acc = 0.9825,  train_prec = 0.9816,  train_recall = 0.9800,  train_f1 = 0.9808,  train_aucroc = 0.9974,  train_opt_accuracy = -1.0000,  train_threshold = -1.0000            
eval_loss = 1.4851,  eval_acc = 0.6740,  eval_prec = 0.4980,  eval_recall = 0.7593,  eval_f1 = 0.6015,  eval_aucroc = 0.7474,  eval_opt_accuracy = -1.0000,  eval_threshold = -1.0000                
lr  =  0.00002659
Elapsed Time:  01:02:06.6922/02/2021 03:22:26 AM : INFO - current patience: 3


Epoch: 15/30,            
train_loss = 0.0520,  train_acc = 0.9841,  train_prec = 0.9832,  train_recall = 0.9820,  train_f1 = 0.9826,  train_aucroc = 0.9976,  train_opt_accuracy = -1.0000,  train_threshold = -1.0000            
eval_loss = 1.9948,  eval_acc = 0.6480,  eval_prec = 0.4413,  eval_recall = 0.7415,  eval_f1 = 0.5533,  eval_aucroc = 0.7312,  eval_opt_accuracy = -1.0000,  eval_threshold = -1.0000                
lr  =  0.00002606
Elapsed Time:  01:06:40.9722/02/2021 03:27:01 AM : INFO - current patience: 4


Epoch: 16/30,            
train_loss = 0.0459,  train_acc = 0.9850,  train_prec = 0.9827,  train_recall = 0.9844,  train_f1 = 0.9835,  train_aucroc = 0.9985,  train_opt_accuracy = -1.0000,  train_threshold = -1.0000            
eval_loss = 1.5072,  eval_acc = 0.6540,  eval_prec = 0.4332,  eval_recall = 0.7643,  eval_f1 = 0.5530,  eval_aucroc = 0.7334,  eval_opt_accuracy = -1.0000,  eval_threshold = -1.0000                
lr  =  0.00002551
Elapsed Time:  01:13:12.37
22/02/2021 03:33:32 AM : INFO - current patience: 5
22/02/2021 03:33:32 AM : INFO - Training terminated early because the Validation aucroc did not improve for  5.0  epochs

----------------------------------------------------------------------------------------------------

--------------------------------------------------
Best Validation scores:
--------------------------------------------------

Val accuracy of best model = 67.800
Val AUC-ROC of best model = 77.041
Val precision of best model = 51.417
Val recall of best model = 75.595
Val f1 of best model = 61.205

--------------------------------------------------
		Evaluating on test set
--------------------------------------------------
22/02/2021 03:33:33 AM : INFO - Using UNITER model ./model_checkpoints/meme.pt
22/02/2021 03:33:34 AM : INFO - Exporting dev_seen predictions...
22/02/2021 03:33:37 AM : INFO - Finished export of dev_seen predictions
22/02/2021 03:33:37 AM : INFO - Optimal threshold on validation dataset: 0.5000 (accuracy=67.80%)
22/02/2021 03:33:37 AM : INFO - Export and testing on test_seen...
22/02/2021 03:33:43 AM : INFO - Finished export of test predictions
